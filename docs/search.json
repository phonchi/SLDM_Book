[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistical Learning and Data Mining",
    "section": "",
    "text": "Preface\nThis is the companion book for the course Statistical Learning and Data Mining open in the Department of Applied Mathematics, National Sun Yat-sen University. Statistical learning refers to a set of tools for modeling and understanding complex datasets. It is a recently developed area in statistics and blends with parallel developments in computer science and, in particular, machine learning. We cover how to use machine learning techniques and statistics with the goal of statistical inference: drawing conclusions on the data at hand. The book encompasses many methods such as regression, classification, regression trees, boosting, support vector machines, clustering and dimension reduction.\nThe book is based on several well-known books and resources, including:\n\nAn Introduction to Statistical Learning, Second Edition\nThe Elements of Statistical Learning, Second Edition\n\nIf you would like to review basics about Python, you may refer to\n\nComputer Programming"
  },
  {
    "objectID": "Chapter1_Lab.html#pandas",
    "href": "Chapter1_Lab.html#pandas",
    "title": "1  Introduction",
    "section": "1.1 Pandas",
    "text": "1.1 Pandas\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nimport seaborn as sns #Samuel Norman Seaborn\nfrom sklearn.preprocessing import scale\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn.decomposition import PCA\n\n%matplotlib inline\nplt.style.use('seaborn-white')\nsns.set_context(\"notebook\", font_scale=1.5, rc={\"lines.linewidth\": 2.5})\n\nPython is a general-purpose language with statistics modules. When it comes to building complex analysis pipelines that mix statistics with e.g. image analysis, text mining, or control of a physical experiment, the richness of Python is an invaluable asset\n\n1.1.1 Constructing data\nOne way to think a Series is regarded it as a labeled array. Creating a Series by passing a list of values, letting pandas create a default integer index:\n\ns = pd.Series([1, 3, 5, np.nan, 6, 8])\ns\n\n0    1.0\n1    3.0\n2    5.0\n3    NaN\n4    6.0\n5    8.0\ndtype: float64\n\n\nSince we did not specify an index for the data, a default one consisting of the integers 0 through N - 1 (where N is the length of the data) is created. Often you’ll want to create a Series with an index identifying each data point with a label:\n\ns2 = pd.Series([4, 7, -5, 3], index=[\"d\", \"b\", \"a\", \"c\"])\ns2\n\nd    4\nb    7\na   -5\nc    3\ndtype: int64\n\n\nAnother way to think about a Series is as a fixed-length, ordered dictionary, as it is a mapping of index values to data values.\n\nsdata = {\"Ohio\": 35000, \"Texas\": 71000, \"Oregon\": 16000, \"Utah\": 5000}\ns3 = pd.Series(sdata)\ns3\n\nOhio      35000\nTexas     71000\nOregon    16000\nUtah       5000\ndtype: int64\n\n\nWe will store and manipulate this data in a pandas.DataFrame, from the pandas module. It is the Python equivalent of the spreadsheet table. It is different from a 2D numpy array as it has named columns, can contain a mixture of different data types by column, and has elaborate mechanisms. The DataFrame has both a row and column index.\n\ndates = pd.date_range(\"20220101\", periods=6)\ndf = pd.DataFrame(np.random.randn(6, 4), index=dates, columns=list(\"ABCD\"))\ndf\n\n\n  \n    \n      \n\n\n\n\n\n\nA\nB\nC\nD\n\n\n\n\n2022-01-01\n0.166868\n2.444831\n0.291433\n1.426635\n\n\n2022-01-02\n1.002246\n-0.027256\n-0.383451\n0.266056\n\n\n2022-01-03\n-0.544344\n-1.092930\n0.156811\n-0.065610\n\n\n2022-01-04\n-0.042673\n0.090997\n-0.278790\n1.249868\n\n\n2022-01-05\n0.940585\n1.306884\n1.229377\n0.318209\n\n\n2022-01-06\n0.097427\n1.118011\n0.359106\n0.616620\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nCreating from arrays: A pandas.DataFrame can also be seen as a dictionary of 1D Series that share the same index, eg arrays or lists. If we have 3 numpy arrays:\n\nt = np.arange(10) #start from 0\nsin_t = np.sin(t)\ncos_t = np.cos(t)\ndf2 = pd.DataFrame({'t': t, 'sin': sin_t, 'cos': cos_t})\ndf2\n\n\n  \n    \n      \n\n\n\n\n\n\nt\nsin\ncos\n\n\n\n\n0\n0\n0.000000\n1.000000\n\n\n1\n1\n0.841471\n0.540302\n\n\n2\n2\n0.909297\n-0.416147\n\n\n3\n3\n0.141120\n-0.989992\n\n\n4\n4\n-0.756802\n-0.653644\n\n\n5\n5\n-0.958924\n0.283662\n\n\n6\n6\n-0.279415\n0.960170\n\n\n7\n7\n0.656987\n0.753902\n\n\n8\n8\n0.989358\n-0.145500\n\n\n9\n9\n0.412118\n-0.911130\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nOne of the most common is from a dictionary of equal-length lists or NumPy arrays:\n\ndata = {\"state\": [\"Ohio\", \"Ohio\", \"Ohio\", \"Nevada\", \"Nevada\", \"Nevada\"],\n        \"year\": [2000, 2001, 2002, 2001, 2002, 2003],\n        \"pop\": [1.5, 1.7, 3.6, 2.4, 2.9, 3.2]}\nframe = pd.DataFrame(data)\nframe\n\n\n  \n    \n      \n\n\n\n\n\n\nstate\nyear\npop\n\n\n\n\n0\nOhio\n2000\n1.5\n\n\n1\nOhio\n2001\n1.7\n\n\n2\nOhio\n2002\n3.6\n\n\n3\nNevada\n2001\n2.4\n\n\n4\nNevada\n2002\n2.9\n\n\n5\nNevada\n2003\n3.2\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nThe columns of the resulting DataFrame may have different dtypes.\n\ndf2.dtypes\n\nt        int64\nsin    float64\ncos    float64\ndtype: object\n\n\n\n\n1.1.2 Viewing data\n\ns.array, s.index\n\n(&lt;PandasArray&gt;\n [1.0, 3.0, 5.0, nan, 6.0, 8.0]\n Length: 6, dtype: float64, RangeIndex(start=0, stop=6, step=1))\n\n\n\ndf.head()\n\n\n  \n    \n      \n\n\n\n\n\n\nA\nB\nC\nD\n\n\n\n\n2022-01-01\n0.166868\n2.444831\n0.291433\n1.426635\n\n\n2022-01-02\n1.002246\n-0.027256\n-0.383451\n0.266056\n\n\n2022-01-03\n-0.544344\n-1.092930\n0.156811\n-0.065610\n\n\n2022-01-04\n-0.042673\n0.090997\n-0.278790\n1.249868\n\n\n2022-01-05\n0.940585\n1.306884\n1.229377\n0.318209\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\ndf.tail(3)\n\n\n  \n    \n      \n\n\n\n\n\n\nA\nB\nC\nD\n\n\n\n\n2022-01-04\n-0.042673\n0.090997\n-0.278790\n1.249868\n\n\n2022-01-05\n0.940585\n1.306884\n1.229377\n0.318209\n\n\n2022-01-06\n0.097427\n1.118011\n0.359106\n0.616620\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\ndf.index\n\nDatetimeIndex(['2022-01-01', '2022-01-02', '2022-01-03', '2022-01-04',\n               '2022-01-05', '2022-01-06'],\n              dtype='datetime64[ns]', freq='D')\n\n\n\ndf.columns\n\nIndex(['A', 'B', 'C', 'D'], dtype='object')\n\n\n\ndf.values\n\narray([[ 0.16686843,  2.44483123,  0.29143297,  1.42663542],\n       [ 1.00224626, -0.02725596, -0.3834514 ,  0.26605642],\n       [-0.54434398, -1.0929295 ,  0.15681066, -0.06560956],\n       [-0.04267318,  0.09099718, -0.27878987,  1.24986756],\n       [ 0.9405845 ,  1.30688362,  1.22937671,  0.31820881],\n       [ 0.09742713,  1.11801138,  0.35910632,  0.6166202 ]])\n\n\ndescribe() shows a quick statistic summary of your data:\n\ndf.describe()\n\n\n  \n    \n      \n\n\n\n\n\n\nA\nB\nC\nD\n\n\n\n\ncount\n6.000000\n6.000000\n6.000000\n6.000000\n\n\nmean\n0.270018\n0.640090\n0.229081\n0.635296\n\n\nstd\n0.597864\n1.239381\n0.576149\n0.588628\n\n\nmin\n-0.544344\n-1.092930\n-0.383451\n-0.065610\n\n\n25%\n-0.007648\n0.002307\n-0.169890\n0.279095\n\n\n50%\n0.132148\n0.604504\n0.224122\n0.467415\n\n\n75%\n0.747155\n1.259666\n0.342188\n1.091556\n\n\nmax\n1.002246\n2.444831\n1.229377\n1.426635\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nSorting by an axis:\n\ndf.sort_index(axis=1, ascending=False)\n\n\n  \n    \n      \n\n\n\n\n\n\nD\nC\nB\nA\n\n\n\n\n2022-01-01\n1.426635\n0.291433\n2.444831\n0.166868\n\n\n2022-01-02\n0.266056\n-0.383451\n-0.027256\n1.002246\n\n\n2022-01-03\n-0.065610\n0.156811\n-1.092930\n-0.544344\n\n\n2022-01-04\n1.249868\n-0.278790\n0.090997\n-0.042673\n\n\n2022-01-05\n0.318209\n1.229377\n1.306884\n0.940585\n\n\n2022-01-06\n0.616620\n0.359106\n1.118011\n0.097427\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nSorting by values:\n\ndf.sort_values(by=\"B\")\n\n\n  \n    \n      \n\n\n\n\n\n\nA\nB\nC\nD\n\n\n\n\n2022-01-03\n-0.544344\n-1.092930\n0.156811\n-0.065610\n\n\n2022-01-02\n1.002246\n-0.027256\n-0.383451\n0.266056\n\n\n2022-01-04\n-0.042673\n0.090997\n-0.278790\n1.249868\n\n\n2022-01-06\n0.097427\n1.118011\n0.359106\n0.616620\n\n\n2022-01-05\n0.940585\n1.306884\n1.229377\n0.318209\n\n\n2022-01-01\n0.166868\n2.444831\n0.291433\n1.426635\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\n1.1.3 Selecting data\nSelecting a single column, which yields a Series, equivalent to df.A:\n\ndf[\"A\"]\n\n2022-01-01    0.166868\n2022-01-02    1.002246\n2022-01-03   -0.544344\n2022-01-04   -0.042673\n2022-01-05    0.940585\n2022-01-06    0.097427\nFreq: D, Name: A, dtype: float64\n\n\n\ndf[0:3] #Selecting via [], which slices the rows. df[\"20210101\":\"20210103\"] also works\n\n\n  \n    \n      \n\n\n\n\n\n\nA\nB\nC\nD\n\n\n\n\n2022-01-01\n0.166868\n2.444831\n0.291433\n1.426635\n\n\n2022-01-02\n1.002246\n-0.027256\n-0.383451\n0.266056\n\n\n2022-01-03\n-0.544344\n-1.092930\n0.156811\n-0.065610\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nSelection by labels of columns or rows (loc)\n\ndf.loc[:, [\"A\", \"B\"]]  \n\n\n  \n    \n      \n\n\n\n\n\n\nA\nB\n\n\n\n\n2022-01-01\n0.166868\n2.444831\n\n\n2022-01-02\n1.002246\n-0.027256\n\n\n2022-01-03\n-0.544344\n-1.092930\n\n\n2022-01-04\n-0.042673\n0.090997\n\n\n2022-01-05\n0.940585\n1.306884\n\n\n2022-01-06\n0.097427\n1.118011\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\ndf.loc[\"20220101\":\"20220104\",\"A\":\"C\"]\n\n\n  \n    \n      \n\n\n\n\n\n\nA\nB\nC\n\n\n\n\n2022-01-01\n0.166868\n2.444831\n0.291433\n\n\n2022-01-02\n1.002246\n-0.027256\n-0.383451\n\n\n2022-01-03\n-0.544344\n-1.092930\n0.156811\n\n\n2022-01-04\n-0.042673\n0.090997\n-0.278790\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nSelection by position (Simiar to NumPy and Python)\n\ndf.iloc[3]\n\nA   -0.042673\nB    0.090997\nC   -0.278790\nD    1.249868\nName: 2022-01-04 00:00:00, dtype: float64\n\n\n\ndf.iloc[3:5, 0:2]\n\n\n  \n    \n      \n\n\n\n\n\n\nA\nB\n\n\n\n\n2022-01-04\n-0.042673\n0.090997\n\n\n2022-01-05\n0.940585\n1.306884\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nBoolean indexing\n\ndf[df[\"A\"] &gt; 0]\n\n\n  \n    \n      \n\n\n\n\n\n\nA\nB\nC\nD\n\n\n\n\n2022-01-01\n0.166868\n2.444831\n0.291433\n1.426635\n\n\n2022-01-02\n1.002246\n-0.027256\n-0.383451\n0.266056\n\n\n2022-01-05\n0.940585\n1.306884\n1.229377\n0.318209\n\n\n2022-01-06\n0.097427\n1.118011\n0.359106\n0.616620\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nSetting and adding data\n\ndf2 = df.copy()\ndf2[\"E\"] = [\"one\", \"one\", \"two\", \"three\", \"four\", \"three\"]\ndf2\n\n\n  \n    \n      \n\n\n\n\n\n\nA\nB\nC\nD\nE\n\n\n\n\n2022-01-01\n0.166868\n2.444831\n0.291433\n1.426635\none\n\n\n2022-01-02\n1.002246\n-0.027256\n-0.383451\n0.266056\none\n\n\n2022-01-03\n-0.544344\n-1.092930\n0.156811\n-0.065610\ntwo\n\n\n2022-01-04\n-0.042673\n0.090997\n-0.278790\n1.249868\nthree\n\n\n2022-01-05\n0.940585\n1.306884\n1.229377\n0.318209\nfour\n\n\n2022-01-06\n0.097427\n1.118011\n0.359106\n0.616620\nthree\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nAn important method on pandas objects is reindex, which means to create a new object with the values rearranged to align with the new index.\n\nobj = pd.Series([4.5, 7.2, -5.3, 3.6], index=[\"d\", \"b\", \"a\", \"c\"])\nobj\n\nd    4.5\nb    7.2\na   -5.3\nc    3.6\ndtype: float64\n\n\n\nobj2 = obj.reindex([\"a\", \"b\", \"c\", \"d\", \"e\"]) # Reindexing allows you to change/add/delete the index on a specified axis. This returns a copy of the data.\nobj2  #pandas primarily uses the value np.nan to represent missing data. It is by default not included in computations. Check dropna(), fillna() and isna()\n\na   -5.3\nb    7.2\nc    3.6\nd    4.5\ne    NaN\ndtype: float64\n\n\n\n\n1.1.4 Computation about data\n\ndf.mean()\n\nA    0.270018\nB    0.640090\nC    0.229081\nD    0.635296\ndtype: float64\n\n\n\ndf.apply(np.cumsum)\n\n\n  \n    \n      \n\n\n\n\n\n\nA\nB\nC\nD\n\n\n\n\n2022-01-01\n0.166868\n2.444831\n0.291433\n1.426635\n\n\n2022-01-02\n1.169115\n2.417575\n-0.092018\n1.692692\n\n\n2022-01-03\n0.624771\n1.324646\n0.064792\n1.627082\n\n\n2022-01-04\n0.582098\n1.415643\n-0.213998\n2.876950\n\n\n2022-01-05\n1.522682\n2.722527\n1.015379\n3.195159\n\n\n2022-01-06\n1.620109\n3.840538\n1.374485\n3.811779\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\n1.1.5 Merge and Group data\n\npieces = [df[:2], df[2:4], df[4:]]\npieces, type(pieces[0])\n\n([                   A         B         C         D\n  2022-01-01  0.166868  2.444831  0.291433  1.426635\n  2022-01-02  1.002246 -0.027256 -0.383451  0.266056,\n                     A         B         C         D\n  2022-01-03 -0.544344 -1.092930  0.156811 -0.065610\n  2022-01-04 -0.042673  0.090997 -0.278790  1.249868,\n                     A         B         C         D\n  2022-01-05  0.940585  1.306884  1.229377  0.318209\n  2022-01-06  0.097427  1.118011  0.359106  0.616620],\n pandas.core.frame.DataFrame)\n\n\n\npd.concat(pieces) #Adding a column to a DataFrame is relatively fast. However, adding a row requires a copy, and may be expensive.\n\n\n  \n    \n      \n\n\n\n\n\n\nA\nB\nC\nD\n\n\n\n\n2022-01-01\n0.166868\n2.444831\n0.291433\n1.426635\n\n\n2022-01-02\n1.002246\n-0.027256\n-0.383451\n0.266056\n\n\n2022-01-03\n-0.544344\n-1.092930\n0.156811\n-0.065610\n\n\n2022-01-04\n-0.042673\n0.090997\n-0.278790\n1.249868\n\n\n2022-01-05\n0.940585\n1.306884\n1.229377\n0.318209\n\n\n2022-01-06\n0.097427\n1.118011\n0.359106\n0.616620\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\ndf = pd.DataFrame(\n    {\n        \"A\": [\"foo\", \"bar\", \"foo\", \"bar\", \"foo\", \"bar\", \"foo\", \"foo\"],\n        \"B\": [\"one\", \"one\", \"two\", \"three\", \"two\", \"two\", \"one\", \"three\"],\n        \"C\": np.random.randn(8),\n        \"D\": np.random.randn(8),\n    }\n)\ndf\n\n\n  \n    \n      \n\n\n\n\n\n\nA\nB\nC\nD\n\n\n\n\n0\nfoo\none\n-2.072404\n0.308518\n\n\n1\nbar\none\n-0.786539\n0.875751\n\n\n2\nfoo\ntwo\n0.544077\n1.649226\n\n\n3\nbar\nthree\n-0.276529\n-0.998452\n\n\n4\nfoo\ntwo\n-1.575059\n0.129585\n\n\n5\nbar\ntwo\n1.484244\n0.783970\n\n\n6\nfoo\none\n0.266894\n1.922994\n\n\n7\nfoo\nthree\n-1.590179\n0.378632\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\ndf.groupby(\"A\").sum()\n\n\n  \n    \n      \n\n\n\n\n\n\nC\nD\n\n\nA\n\n\n\n\n\n\nbar\n0.421177\n0.661268\n\n\nfoo\n-4.426670\n4.388955\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\n1.1.6 I/O\n\ndf.to_excel(\"foo.xlsx\", sheet_name=\"Sheet1\")\n\n\ndf.to_csv(\"foo.csv\")\n\nFor more information, refer to https://wesmckinney.com/book/pandas-basics.html and https://pandas.pydata.org/Pandas_Cheat_Sheet.pdf"
  },
  {
    "objectID": "Chapter1_Lab.html#seaborn",
    "href": "Chapter1_Lab.html#seaborn",
    "title": "1  Introduction",
    "section": "1.2 Seaborn",
    "text": "1.2 Seaborn\nStatistical analysis is a process of understanding how variables in a dataset relate to each other and how those relationships depend on other variables. Visualization can be a core component of this process because, when data are visualized properly, the human visual system can see trends and patterns that indicate a relationship.\nThe “tips” dataset https://www.kaggle.com/ranjeetjain3/seaborn-tips-dataset\n\ntips = sns.load_dataset(\"tips\")\nprint(tips.shape)\ntips.head()\n\n(244, 7)\n\n\n\n  \n    \n      \n\n\n\n\n\n\ntotal_bill\ntip\nsex\nsmoker\nday\ntime\nsize\n\n\n\n\n0\n16.99\n1.01\nFemale\nNo\nSun\nDinner\n2\n\n\n1\n10.34\n1.66\nMale\nNo\nSun\nDinner\n3\n\n\n2\n21.01\n3.50\nMale\nNo\nSun\nDinner\n3\n\n\n3\n23.68\n3.31\nMale\nNo\nSun\nDinner\n2\n\n\n4\n24.59\n3.61\nFemale\nNo\nSun\nDinner\n4\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n1.2.1 Scatterplot\nThe scatter plot is a mainstay of statistical visualization. It depicts the joint distribution of two variables using a cloud of points, where each point represents an observation in the dataset.\n\n# Scatterplot\nsns.relplot(x=\"total_bill\", y=\"tip\", data=tips) \n\n\n\n\nWhile the points are plotted in two dimensions, another dimension can be added to the plot by conditioning a third variable. In seaborn, this is referred to as using a “hue/styple/size semantic”, because the color/style/size of the point gains meaning:\n\nsns.relplot(x=\"total_bill\", y=\"tip\", hue=\"smoker\", data=tips) \n\n\n\n\n\nsns.relplot(x=\"total_bill\", y=\"tip\", style=\"smoker\", data=tips) # you can use a different marker style for each class\n\n\n\n\n\nsns.relplot(x=\"total_bill\", y=\"tip\", hue=\"size\", data=tips) #if hue is numeric rather than categorical\n\n\n\n\n\nsns.relplot(x=\"total_bill\", y=\"tip\", size=\"size\", sizes=(15, 200), data=tips) #size rather than colors\n\n\n\n\nNote that we can plot small multiples by using row and col variable\n\nsns.relplot(x=\"total_bill\", y=\"tip\", hue=\"smoker\",\n            col=\"time\", data=tips) #show in different subplot\n\n\n\n\n\n\n1.2.2 Lineplot\nWith some datasets, you may want to understand changes in one variable as a function of time, or a similarly continuous variable. In this situation, a good choice is to draw a line plot to emphasize the continuity.\n\ndf = pd.DataFrame(dict(time=np.arange(500),\n                       value=np.random.randn(500).cumsum()))\ng = sns.relplot(x=\"time\", y=\"value\", kind=\"line\", data=df)\n\n\n\n\nMore complex datasets will have multiple measurements for the same value of the x variable. The default behavior in seaborn is to aggregate the multiple measurements at each x value by plotting the mean and the 95% confidence interval around the mean by bootstraping:\n\nfmri = sns.load_dataset(\"fmri\")\nsns.lineplot(x=\"timepoint\", y=\"signal\", hue=\"event\", style=\"event\", markers=True, data=fmri)\n\n&lt;matplotlib.axes._subplots.AxesSubplot at 0x7fdf78d9b150&gt;\n\n\n\n\n\n\n\n1.2.3 Histplot\nAn early step in any effort to analyze or model data should be to understand how the variables are distributed. Techniques for distribution visualization can provide quick answers to many important questions. What range do the observations cover? What is their central tendency? Are they heavily skewed in one direction? Is there evidence for bimodality? Are there significant outliers? Do the answers to these questions vary across subsets defined by other variables?\n\nsns.displot(x=\"total_bill\", data = tips) #check parameter bins and binwidth\n\n\n\n\n\nsns.histplot(x=\"day\", hue=\"sex\", data=tips) #By default, the different histograms are “layered” on top of each other and, in some cases, they may be difficult to distinguish.\n\n&lt;matplotlib.axes._subplots.AxesSubplot at 0x7fdf789b0cd0&gt;\n\n\n\n\n\n\nsns.histplot(x=\"day\", hue=\"sex\", multiple=\"dodge\", shrink=.8, data=tips)\n\n&lt;matplotlib.axes._subplots.AxesSubplot at 0x7fdf78947910&gt;\n\n\n\n\n\n\n\n1.2.4 Kdeplot\nKernel density estimation (KDE) presents a different solution to the same problem. Rather than using discrete bins, a KDE plot smooths the observations with a Gaussian kernel, producing a continuous density estimate:\n\nsns.kdeplot(x=\"total_bill\", data=tips)\n\n&lt;matplotlib.axes._subplots.AxesSubplot at 0x7fdf7c9ea390&gt;\n\n\n\n\n\n\nsns.kdeplot(x=\"total_bill\", hue=\"time\", multiple=\"stack\", data=tips)\n\n&lt;matplotlib.axes._subplots.AxesSubplot at 0x7fdf78829750&gt;\n\n\n\n\n\n\n\n1.2.5 Joinplot\njointplot() augments a bivariate relatonal or distribution plot with the marginal distributions of the two variables. By default, jointplot() represents the bivariate distribution using scatterplot() and the marginal distributions using histplot():\n\nsns.jointplot(x=\"total_bill\", y=\"tip\", data=tips) \n\n\n\n\n\n\n1.2.6 Pairpolt\nThe pairplot() function offers a similar blend of joint and marginal distributions. Rather than focusing on a single relationship, however, pairplot() uses a “small-multiple” approach to visualize the univariate distribution of all variables in a dataset along with all of their pairwise relationships:\n\nsns.pairplot(tips)\n\n\n\n\n\n\n1.2.7 Boxplot\nAs the size of the dataset grows, categorical scatter plots become limited in the information they can provide about the distribution of values within each category. When this happens, there are several approaches for summarizing the distributional information in ways that facilitate easy comparisons across the category levels.\n\nsns.catplot(x=\"day\", y=\"total_bill\", kind=\"box\", data=tips)\n\n\n\n\n\nsns.catplot(x=\"day\", y=\"total_bill\", hue=\"smoker\", kind=\"box\", data=tips) # When adding a hue semantic, the box for each level of the semantic variable is moved along the categorical axis so they don’t overlap\n\n\n\n\n\n\n1.2.8 Barplot\nRather than showing the distribution within each category, you might want to show an estimate of the central tendency of the values. In seaborn, the barplot() function operates on a full dataset and applies a function to obtain the estimate (taking the mean by default). When there are multiple observations in each category, it also uses bootstrapping to compute a confidence interval around the estimate, which is plotted using error bars:\n\nax = sns.barplot(x=\"day\", y=\"tip\", data=tips)\n\n\n\n\n\nsns.catplot(x=\"day\", y=\"total_bill\", hue=\"sex\", kind=\"bar\", data=tips)\n\n\n\n\n\n\n1.2.9 Countplot\n\nsns.countplot(x=\"smoker\", data=tips) #simply count the number\n\n&lt;matplotlib.axes._subplots.AxesSubplot at 0x7fdf77f6f390&gt;\n\n\n\n\n\n\n\n1.2.10 Pointplot\nThis function also encodes the value of the estimate with height on the other axis, but rather than showing a full bar, it plots the point estimate and confidence interval\n\nsns.pointplot(x=\"day\", y=\"tip\", data=tips, ci=68)\n\n&lt;matplotlib.axes._subplots.AxesSubplot at 0x7fdf77f42ad0&gt;\n\n\n\n\n\n\n\n1.2.11 Regplot/lmplot\nIn the spirit of Tukey, the regression plots in seaborn are primarily intended to add a visual guide that helps to emphasize patterns in a dataset during exploratory data analyses. The goal of seaborn, however, is to make exploring a dataset through visualization quick and easy, as doing so is just as (if not more) important than exploring a dataset through tables of statistics.\n\nsns.regplot(x=\"total_bill\", y=\"tip\", data=tips)\n\n&lt;matplotlib.axes._subplots.AxesSubplot at 0x7fdf77eaee90&gt;\n\n\n\n\n\nWhen the y variable is binary, simple linear regression also “works” but provides implausible predictions. The solution in this case is to fit a logistic regression, such that the regression line shows the estimated probability of y = 1 for a given value of x:\n\ntips[\"big_tip\"] = (tips.tip / tips.total_bill) &gt; .15\nsns.lmplot(x=\"total_bill\", y=\"big_tip\", data=tips,\n           logistic=True, y_jitter=.03)\n\n\n\n\n\nsns.lmplot(x=\"total_bill\", y=\"tip\", hue=\"smoker\", col=\"time\", data=tips)\n\n\n\n\n\n\n1.2.12 Heatmap\n\ncorr = tips.corr()\nsns.heatmap(corr)\n\n&lt;matplotlib.axes._subplots.AxesSubplot at 0x7fdf77af6cd0&gt;\n\n\n\n\n\nCustomized your plot https://seaborn.pydata.org/tutorial/axis_grids.html. For more information, see https://seaborn.pydata.org/tutorial.html"
  },
  {
    "objectID": "Chapter1_Lab.html#lab1-loading-datasets-and-processing",
    "href": "Chapter1_Lab.html#lab1-loading-datasets-and-processing",
    "title": "1  Introduction",
    "section": "1.3 Lab1: Loading Datasets and processing",
    "text": "1.3 Lab1: Loading Datasets and processing\n\n\n\n\nsource: https://stackoverflow.com/questions/2354725/what-exactly-is-llvm\n\nDatasets available on https://www.statlearning.com/resources-second-edition\n\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\nMounted at /content/drive\n\n\n\nWage = pd.read_csv('/content/drive/MyDrive/Lab/Data/Wage.csv')\nWage.head(3)\n\n\n  \n    \n      \n\n\n\n\n\n\nyear\nage\nmaritl\nrace\neducation\nregion\njobclass\nhealth\nhealth_ins\nlogwage\nwage\n\n\n\n\n0\n2006\n18\n1. Never Married\n1. White\n1. &lt; HS Grad\n2. Middle Atlantic\n1. Industrial\n1. &lt;=Good\n2. No\n4.318063\n75.043154\n\n\n1\n2004\n24\n1. Never Married\n1. White\n4. College Grad\n2. Middle Atlantic\n2. Information\n2. &gt;=Very Good\n2. No\n4.255273\n70.476020\n\n\n2\n2003\n45\n2. Married\n1. White\n3. Some College\n2. Middle Atlantic\n1. Industrial\n1. &lt;=Good\n1. Yes\n4.875061\n130.982177\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\nWage.shape, Wage.columns\n\n((3000, 11),\n Index(['year', 'age', 'maritl', 'race', 'education', 'region', 'jobclass',\n        'health', 'health_ins', 'logwage', 'wage'],\n       dtype='object'))\n\n\n\nWage.info(), Wage.describe()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 3000 entries, 0 to 2999\nData columns (total 11 columns):\n #   Column      Non-Null Count  Dtype  \n---  ------      --------------  -----  \n 0   year        3000 non-null   int64  \n 1   age         3000 non-null   int64  \n 2   maritl      3000 non-null   object \n 3   race        3000 non-null   object \n 4   education   3000 non-null   object \n 5   region      3000 non-null   object \n 6   jobclass    3000 non-null   object \n 7   health      3000 non-null   object \n 8   health_ins  3000 non-null   object \n 9   logwage     3000 non-null   float64\n 10  wage        3000 non-null   float64\ndtypes: float64(2), int64(2), object(7)\nmemory usage: 257.9+ KB\n\n\n(None,               year          age      logwage         wage\n count  3000.000000  3000.000000  3000.000000  3000.000000\n mean   2005.791000    42.414667     4.653905   111.703608\n std       2.026167    11.542406     0.351753    41.728595\n min    2003.000000    18.000000     3.000000    20.085537\n 25%    2004.000000    33.750000     4.447158    85.383940\n 50%    2006.000000    42.000000     4.653213   104.921507\n 75%    2008.000000    51.000000     4.857332   128.680488\n max    2009.000000    80.000000     5.763128   318.342430)\n\n\n\nWage[Wage['year'] == 2004]['wage'].mean()\n\n111.15999687022257\n\n\n\ngroupby_year = Wage.groupby('year')\nfor year, value in groupby_year['wage']:\n    print((year, value.mean()))\n\n(2003, 106.1982838253092)\n(2004, 111.15999687022257)\n(2005, 110.03785731274047)\n(2006, 114.24257930246814)\n(2007, 112.89455609045216)\n(2008, 113.55199949510215)\n(2009, 115.97177059231909)\n\n\n\ngroupby_year # groupby_year is a powerful object that exposes many operations on the resulting group of dataframes:\n\n&lt;pandas.core.groupby.generic.DataFrameGroupBy object at 0x7fdf77a27f90&gt;\n\n\nSeaborn combines simple statistical fits with plotting on Pandas dataframes.\n\n# creating plots\n# Scatter plot with polynomial regression line, the regression line is bounded by the data limits. truncate=True.\nplt.figure(figsize=(4,6))\nsns.scatterplot(x=\"age\", y=\"wage\", data=Wage, alpha=0.1)\nsns.regplot(x=\"age\", y=\"wage\", data=Wage, order=4, truncate=True, scatter=False) \n\n&lt;matplotlib.axes._subplots.AxesSubplot at 0x7fdf77a47890&gt;\n\n\n\n\n\n\n# creating plots\n# Scatter plot with polynomial regression line\nplt.figure(figsize=(4,6))\nsns.scatterplot(x=\"year\", y=\"wage\", data=Wage, alpha=0.1)\nsns.regplot(x=\"year\", y=\"wage\", data=Wage, order=1, truncate=True, scatter=False)\n\n&lt;matplotlib.axes._subplots.AxesSubplot at 0x7fdf77a05ed0&gt;\n\n\n\n\n\n\nprint(Wage.education.unique())\noriginalL = list(Wage.education.unique())\norderL = [originalL[0], originalL[3], originalL[2], originalL[1], originalL[4]]\n\n['1. &lt; HS Grad' '4. College Grad' '3. Some College' '2. HS Grad'\n '5. Advanced Degree']\n\n\n\nplt.figure(figsize=(4,6))\nax = sns.boxplot(x=\"education\", y=\"wage\", data=Wage, order=orderL)\nax.set_xticklabels([t.get_text().split()[0][0]  for t in ax.get_xticklabels()])\n\n[Text(0, 0, '1'),\n Text(0, 0, '2'),\n Text(0, 0, '3'),\n Text(0, 0, '4'),\n Text(0, 0, '5')]\n\n\n\n\n\n\nSmarket = pd.read_csv('/content/drive/MyDrive/Lab/Data/Smarket.csv')\nSmarket.head()\n\n\n  \n    \n      \n\n\n\n\n\n\nYear\nLag1\nLag2\nLag3\nLag4\nLag5\nVolume\nToday\nDirection\n\n\n\n\n0\n2001\n0.381\n-0.192\n-2.624\n-1.055\n5.010\n1.1913\n0.959\nUp\n\n\n1\n2001\n0.959\n0.381\n-0.192\n-2.624\n-1.055\n1.2965\n1.032\nUp\n\n\n2\n2001\n1.032\n0.959\n0.381\n-0.192\n-2.624\n1.4112\n-0.623\nDown\n\n\n3\n2001\n-0.623\n1.032\n0.959\n0.381\n-0.192\n1.2760\n0.614\nUp\n\n\n4\n2001\n0.614\n-0.623\n1.032\n0.959\n0.381\n1.2057\n0.213\nUp\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\nplt.figure(figsize=(4,6))\nax =sns.boxplot(x=\"Direction\", y=\"Lag1\", data=Smarket, order=[\"Down\", \"Up\"])\nax.set_ylabel(\"Percentage change in S&P\")\nax.set_xlabel(\"Today's Direction\")\nplt.title(\"Yesterday\")\n\nText(0.5, 1.0, 'Yesterday')\n\n\n\n\n\n\nplt.figure(figsize=(4,6))\nax = sns.boxplot(x=\"Direction\", y=\"Lag2\", data=Smarket, order=[\"Down\", \"Up\"])\nax.set_ylabel(\"Percentage change in S&P\")\nax.set_xlabel(\"Today's Direction\")\nplt.title(\"Two Days Previois\")\n\nText(0.5, 1.0, 'Two Days Previois')\n\n\n\n\n\n\nplt.figure(figsize=(4,6))\nax = sns.boxplot(x=\"Direction\", y=\"Lag3\", data=Smarket, order=[\"Down\", \"Up\"])\nax.set_ylabel(\"Percentage change in S&P\")\nax.set_xlabel(\"Today's Direction\")\nplt.title(\"Three Days Previois\")\n\nText(0.5, 1.0, 'Three Days Previois')\n\n\n\n\n\n\nSmarket = pd.read_csv('/content/drive/MyDrive/Lab/Data/Smarket.csv', index_col=0) #use col0 as index\n\n\nSmarket.head()\n\n\n  \n    \n      \n\n\n\n\n\n\nLag1\nLag2\nLag3\nLag4\nLag5\nVolume\nToday\nDirection\n\n\nYear\n\n\n\n\n\n\n\n\n\n\n\n\n2001\n0.381\n-0.192\n-2.624\n-1.055\n5.010\n1.1913\n0.959\nUp\n\n\n2001\n0.959\n0.381\n-0.192\n-2.624\n-1.055\n1.2965\n1.032\nUp\n\n\n2001\n1.032\n0.959\n0.381\n-0.192\n-2.624\n1.4112\n-0.623\nDown\n\n\n2001\n-0.623\n1.032\n0.959\n0.381\n-0.192\n1.2760\n0.614\nUp\n\n\n2001\n0.614\n-0.623\n1.032\n0.959\n0.381\n1.2057\n0.213\nUp\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\nSmarket.loc[:'2004'][['Lag1','Lag2']]\n\n\n  \n    \n      \n\n\n\n\n\n\nLag1\nLag2\n\n\nYear\n\n\n\n\n\n\n2001\n0.381\n-0.192\n\n\n2001\n0.959\n0.381\n\n\n2001\n1.032\n0.959\n\n\n2001\n-0.623\n1.032\n\n\n2001\n0.614\n-0.623\n\n\n...\n...\n...\n\n\n2004\n0.046\n0.342\n\n\n2004\n-0.431\n0.046\n\n\n2004\n0.715\n-0.431\n\n\n2004\n-0.007\n0.715\n\n\n2004\n0.008\n-0.007\n\n\n\n\n\n998 rows × 2 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\nX_train = Smarket.loc[:'2004'][['Lag1','Lag2']]\ny_train = Smarket.loc[:'2004']['Direction']\n\nX_test = Smarket.loc['2005':][['Lag1','Lag2']]\ny_test = Smarket.loc['2005':]['Direction']\n\n\nqda = QuadraticDiscriminantAnalysis()\npred = qda.fit(X_train, y_train).predict_proba(X_test)\n\n\nqda.classes_\n\narray(['Down', 'Up'], dtype=object)\n\n\n\nplt.figure(figsize=(4,6))\nsns.boxplot(x=y_test, y=pred[:,0]) #predicted probability for decrease\n\n&lt;matplotlib.axes._subplots.AxesSubplot at 0x7fdf78024bd0&gt;\n\n\n\n\n\n\nNCI60 = pd.read_csv('/content/drive/MyDrive/Lab/Data/NCI60_data.csv')\nNCI60.head()\n\n\n  \n    \n      \n\n\n\n\n\n\nUnnamed: 0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n...\n6821\n6822\n6823\n6824\n6825\n6826\n6827\n6828\n6829\n6830\n\n\n\n\n0\nV1\n0.300000\n1.180000\n0.550000\n1.140000\n-0.265000\n-7.000000e-02\n0.350000\n-0.315000\n-0.450000\n...\n-0.990020\n0.000000\n0.030000\n-0.175000\n0.629981\n-0.030000\n0.000000\n0.280000\n-0.340000\n-1.930000\n\n\n1\nV2\n0.679961\n1.289961\n0.169961\n0.379961\n0.464961\n5.799610e-01\n0.699961\n0.724961\n-0.040039\n...\n-0.270058\n-0.300039\n-0.250039\n-0.535039\n0.109941\n-0.860039\n-1.250049\n-0.770039\n-0.390039\n-2.000039\n\n\n2\nV3\n0.940000\n-0.040000\n-0.170000\n-0.040000\n-0.605000\n0.000000e+00\n0.090000\n0.645000\n0.430000\n...\n0.319981\n0.120000\n-0.740000\n-0.595000\n-0.270020\n-0.150000\n0.000000\n-0.120000\n-0.410000\n0.000000\n\n\n3\nV4\n0.280000\n-0.310000\n0.680000\n-0.810000\n0.625000\n-1.387779e-17\n0.170000\n0.245000\n0.020000\n...\n-1.240020\n-0.110000\n-0.160000\n0.095000\n-0.350019\n-0.300000\n-1.150010\n1.090000\n-0.260000\n-1.100000\n\n\n4\nV5\n0.485000\n-0.465000\n0.395000\n0.905000\n0.200000\n-5.000000e-03\n0.085000\n0.110000\n0.235000\n...\n0.554980\n-0.775000\n-0.515000\n-0.320000\n0.634980\n0.605000\n0.000000\n0.745000\n0.425000\n0.145000\n\n\n\n\n\n5 rows × 6831 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\nNCI60 = pd.read_csv('/content/drive/MyDrive/Lab/Data/NCI60_data.csv').drop('Unnamed: 0', axis=1)\nNCI60.columns = np.arange(NCI60.columns.size)\nNCI60.head()\n\n\n  \n    \n      \n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n...\n6820\n6821\n6822\n6823\n6824\n6825\n6826\n6827\n6828\n6829\n\n\n\n\n0\n0.300000\n1.180000\n0.550000\n1.140000\n-0.265000\n-7.000000e-02\n0.350000\n-0.315000\n-0.450000\n-0.654980\n...\n-0.990020\n0.000000\n0.030000\n-0.175000\n0.629981\n-0.030000\n0.000000\n0.280000\n-0.340000\n-1.930000\n\n\n1\n0.679961\n1.289961\n0.169961\n0.379961\n0.464961\n5.799610e-01\n0.699961\n0.724961\n-0.040039\n-0.285019\n...\n-0.270058\n-0.300039\n-0.250039\n-0.535039\n0.109941\n-0.860039\n-1.250049\n-0.770039\n-0.390039\n-2.000039\n\n\n2\n0.940000\n-0.040000\n-0.170000\n-0.040000\n-0.605000\n0.000000e+00\n0.090000\n0.645000\n0.430000\n0.475019\n...\n0.319981\n0.120000\n-0.740000\n-0.595000\n-0.270020\n-0.150000\n0.000000\n-0.120000\n-0.410000\n0.000000\n\n\n3\n0.280000\n-0.310000\n0.680000\n-0.810000\n0.625000\n-1.387779e-17\n0.170000\n0.245000\n0.020000\n0.095019\n...\n-1.240020\n-0.110000\n-0.160000\n0.095000\n-0.350019\n-0.300000\n-1.150010\n1.090000\n-0.260000\n-1.100000\n\n\n4\n0.485000\n-0.465000\n0.395000\n0.905000\n0.200000\n-5.000000e-03\n0.085000\n0.110000\n0.235000\n1.490019\n...\n0.554980\n-0.775000\n-0.515000\n-0.320000\n0.634980\n0.605000\n0.000000\n0.745000\n0.425000\n0.145000\n\n\n\n\n\n5 rows × 6830 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\nX = pd.DataFrame(scale(NCI60))\nX.shape\n\n(64, 6830)\n\n\n\ny = pd.read_csv('/content/drive/MyDrive/Lab/Data/NCI60_labs.csv', usecols=[1], skiprows=1, names=['type'])\ny.shape\n\n(64, 1)\n\n\n\ny.type.value_counts()\n\nRENAL          9\nNSCLC          9\nMELANOMA       8\nBREAST         7\nCOLON          7\nOVARIAN        6\nLEUKEMIA       6\nCNS            5\nPROSTATE       2\nUNKNOWN        1\nK562B-repro    1\nK562A-repro    1\nMCF7A-repro    1\nMCF7D-repro    1\nName: type, dtype: int64\n\n\n\n# Fit the PCA model and transform X to get the principal components\npca2 = PCA()\nNCI60_plot = pd.DataFrame(pca2.fit_transform(X))\n\n\nfig, (ax1, ax2) = plt.subplots(1,2, figsize=(15,6))\n\n# Left plot\nsns.scatterplot(x =0, y=-NCI60_plot[1], data=NCI60_plot, hue=y.type, alpha=0.5, s=50, ax=ax1, legend=False)\nax1.set_xlabel('Z1') \nax1.set_ylabel('Z2')\n   \n\n# Right plot\nsns.scatterplot(x = 0, y= 2, data=NCI60_plot, hue=y.type, alpha=0.5, s=50, ax=ax2)\nax2.legend(loc='center left', bbox_to_anchor=(1, 0.5), ncol=2)\nax2.set_xlabel('Z1')  \nax2.set_ylabel('Z3')\n\nText(0, 0.5, 'Z3')"
  },
  {
    "objectID": "Chapter 2_Lab.html#basic-commands",
    "href": "Chapter 2_Lab.html#basic-commands",
    "title": "2  Statistical Learning",
    "section": "2.1 Basic Commands",
    "text": "2.1 Basic Commands\nRefer to Numpy notebook for more detail\n\n# imports and setup\nimport math\nimport pandas as pd\nimport numpy as np\nfrom scipy.stats.stats import pearsonr\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D # for 3D plots\nimport seaborn as sns\n%matplotlib inline\n\npd.set_option('precision', 2) # number precision for pandas\nplt.style.use('seaborn-white')\nsns.set_context(\"notebook\", font_scale=1.5, rc={\"lines.linewidth\": 2.5})\n\n\n# vector\nx = [1,3,2,5] # do not use this\nx = np.array([1, 6, 2]) # use ndarray instead\ny = np.array([1, 4, 3])\nx.shape, y.shape\n\n((3,), (3,))\n\n\n\n# array operations\nx + y\n\narray([ 2, 10,  5])\n\n\n\n# matrix creation\nx = np.array([[1,2],[3,4]]) # row major, do not use np.matrix\nx, np.isfortran(x)\n\n(array([[1, 2],\n        [3, 4]]), False)\n\n\n\nx = np.asfortranarray(np.array(range(1,5)).reshape(2,2)) #do not do this\nx, np.isfortran(x) \n\n(array([[1, 2],\n        [3, 4]]), True)\n\n\n\nnp.array(range(1,5)).reshape(2,2)\n\narray([[1, 2],\n       [3, 4]])\n\n\n\nnp.sqrt(x) # c.f. https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.sqrtm.html\n\narray([[1.        , 1.41421356],\n       [1.73205081, 2.        ]])\n\n\n\nprint(np.power(x, 2))\nprint(x**2)\n\n[[ 1  4]\n [ 9 16]]\n[[ 1  4]\n [ 9 16]]\n\n\n\n# random normal distribution & correlation\nx = np.random.randn(50)\ny = x + np.random.normal(loc=50, scale=.1, size=50)\npearsonr(x, y)[0]\n\n0.9961062617773434\n\n\n\n# random seed and basic statistical functions\nnp.random.seed(3)\ny = np.random.randn(50)\ny.mean(), y.var(), np.sqrt(y.var()), y.std()\n\n(-0.2939268200794072,\n 0.9969329133079725,\n 0.9984652789696657,\n 0.9984652789696657)"
  },
  {
    "objectID": "Chapter 2_Lab.html#graphics",
    "href": "Chapter 2_Lab.html#graphics",
    "title": "2  Statistical Learning",
    "section": "2.2 Graphics",
    "text": "2.2 Graphics\nHere, we review some of the command we have learned using Seaborn\n\nx = np.random.randn(100)\ny = np.random.randn(100)\n\n# seaborn scatterplot\np = sns.jointplot(x=x, y=y, kind='scatter')\np.set_axis_labels(xlabel='this is x axis', ylabel='this is y axis')\n\n\n\n\nWe will often want to save the output of an Python plot. The command that we use to do this will be the savefig().\n\np.savefig(\"Figure.pdf\") #depends on the file extension\np.savefig(\"Figure.jpg\")\n\n\n# create a sequence of numbers\nx = np.arange(1, 11)\nx\n\narray([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10])\n\n\n\n# linearly spaced numbers\nx = np.linspace(-np.pi, np.pi, num=50)\nx\n\narray([-3.14159265, -3.01336438, -2.88513611, -2.75690784, -2.62867957,\n       -2.5004513 , -2.37222302, -2.24399475, -2.11576648, -1.98753821,\n       -1.85930994, -1.73108167, -1.60285339, -1.47462512, -1.34639685,\n       -1.21816858, -1.08994031, -0.96171204, -0.83348377, -0.70525549,\n       -0.57702722, -0.44879895, -0.32057068, -0.19234241, -0.06411414,\n        0.06411414,  0.19234241,  0.32057068,  0.44879895,  0.57702722,\n        0.70525549,  0.83348377,  0.96171204,  1.08994031,  1.21816858,\n        1.34639685,  1.47462512,  1.60285339,  1.73108167,  1.85930994,\n        1.98753821,  2.11576648,  2.24399475,  2.37222302,  2.5004513 ,\n        2.62867957,  2.75690784,  2.88513611,  3.01336438,  3.14159265])\n\n\nWe will now create some more sophisticated plots. We create three array here\n\nA vector of the x values (the first dimension),\nA vector of the y values (the second dimension), and\nAn 2D array whose elements correspond to the z value (the third dimension) for each pair of (x, y) coordinates.\n\n\nplt.figure(figsize=(6,6))\n\nx = np.linspace(-np.pi, np.pi, num=50)\ny = x\n\n# simulating R outer function\ndef pf(a, b):\n    return math.cos(b) / (1 + a**2)\n\nf = np.empty((len(x), len(y)))\n \nfor i in range(len(x)):\n    for j in range(len(y)):\n        f[i,j] = pf(x[i], y[j])\n\n        \n# contour plot\ncp = plt.contour(x, y, f.T, 45, cmap='viridis')\nplt.clabel(cp, inline=1, fontsize=10);\n\n\n\n\n\n# contour 2\nplt.figure(figsize=(6,6))\n\nfa = (f - f.T)/2\ncp = plt.contour(x, y, fa.T, 15, cmap='viridis')\nplt.clabel(cp, inline=1, fontsize=10);\n\n\n\n\n\n# heatmap\nplt.figure(figsize=(6,6))\ncp = plt.contourf(x, y, fa.T, 15, cmap='viridis') #adiidtional f\nplt.clabel(cp, inline=1, fontsize=10)\nplt.colorbar()\n\n&lt;matplotlib.colorbar.Colorbar at 0x7f7c932f7a50&gt;\n\n\n\n\n\n\n# 3d perspective\n\nfig = plt.figure(figsize=(10,10))\nax = fig.add_subplot(111, projection='3d')\nax.plot_wireframe(x, y, fa.T, cmap='viridis')\n\n&lt;mpl_toolkits.mplot3d.art3d.Line3DCollection at 0x7f7c932129d0&gt;"
  },
  {
    "objectID": "Chapter 2_Lab.html#indexing-data",
    "href": "Chapter 2_Lab.html#indexing-data",
    "title": "2  Statistical Learning",
    "section": "2.3 Indexing Data",
    "text": "2.3 Indexing Data\n\n# matrix creation (R equivalent of matrix(1:16, 4 ,4))\nA = np.arange(16).reshape(4, 4).transpose()\nA\n\narray([[ 0,  4,  8, 12],\n       [ 1,  5,  9, 13],\n       [ 2,  6, 10, 14],\n       [ 3,  7, 11, 15]])\n\n\n\nA[1, 2]\n\n9\n\n\n\n# select a range of rows and columns\nA[0:3, 1:4]\n\narray([[ 4,  8, 12],\n       [ 5,  9, 13],\n       [ 6, 10, 14]])\n\n\n\n# select a range of rows and all columns\nA[0:2,:]\n\narray([[ 0,  4,  8, 12],\n       [ 1,  5,  9, 13]])\n\n\n\n# select all rows and a range of columns\nA[:,0:2]\n\narray([[0, 4],\n       [1, 5],\n       [2, 6],\n       [3, 7]])\n\n\n\nA[0,:]\n\narray([ 0,  4,  8, 12])\n\n\n\n# shape of the matrix\nA.shape\n\n(4, 4)"
  },
  {
    "objectID": "Chapter 2_Lab.html#loading-data",
    "href": "Chapter 2_Lab.html#loading-data",
    "title": "2  Statistical Learning",
    "section": "2.4 Loading Data",
    "text": "2.4 Loading Data\nThe data set also includes a number of missing observations, indicated by a question mark ?. Missing values are a common occurrence in real data sets.\n\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\nMounted at /content/drive\n\n\n\n# read csv data with pandas into dataframe, explicitly setting na_values.\n# pandas read_xxx functions infer datatypes, headers, dates, etc. \n# without explicit declarations\nAuto = pd.read_csv('/content/drive/MyDrive/Lab/Data/Auto_origin.csv', na_values=['?'])\nAuto\n\n\n  \n    \n      \n\n\n\n\n\n\nmpg\ncylinders\ndisplacement\nhorsepower\nweight\nacceleration\nyear\norigin\nname\n\n\n\n\n0\n18.0\n8\n307.0\n130.0\n3504\n12.0\n70\n1\nchevrolet chevelle malibu\n\n\n1\n15.0\n8\n350.0\n165.0\n3693\n11.5\n70\n1\nbuick skylark 320\n\n\n2\n18.0\n8\n318.0\n150.0\n3436\n11.0\n70\n1\nplymouth satellite\n\n\n3\n16.0\n8\n304.0\n150.0\n3433\n12.0\n70\n1\namc rebel sst\n\n\n4\n17.0\n8\n302.0\n140.0\n3449\n10.5\n70\n1\nford torino\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n392\n27.0\n4\n140.0\n86.0\n2790\n15.6\n82\n1\nford mustang gl\n\n\n393\n44.0\n4\n97.0\n52.0\n2130\n24.6\n82\n2\nvw pickup\n\n\n394\n32.0\n4\n135.0\n84.0\n2295\n11.6\n82\n1\ndodge rampage\n\n\n395\n28.0\n4\n120.0\n79.0\n2625\n18.6\n82\n1\nford ranger\n\n\n396\n31.0\n4\n119.0\n82.0\n2720\n19.4\n82\n1\nchevy s-10\n\n\n\n\n\n397 rows × 9 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n# dropping rows (axis-0) where there are NA values (inplace)\nAuto.dropna(axis=0, inplace=True)\nAuto.shape\n\n(392, 9)\n\n\n\nAuto.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 392 entries, 0 to 396\nData columns (total 9 columns):\n #   Column        Non-Null Count  Dtype  \n---  ------        --------------  -----  \n 0   mpg           392 non-null    float64\n 1   cylinders     392 non-null    int64  \n 2   displacement  392 non-null    float64\n 3   horsepower    392 non-null    float64\n 4   weight        392 non-null    int64  \n 5   acceleration  392 non-null    float64\n 6   year          392 non-null    int64  \n 7   origin        392 non-null    int64  \n 8   name          392 non-null    object \ndtypes: float64(4), int64(4), object(1)\nmemory usage: 30.6+ KB\n\n\n\nAuto.describe()\n\n\n  \n    \n      \n\n\n\n\n\n\nmpg\ncylinders\ndisplacement\nhorsepower\nweight\nacceleration\nyear\norigin\n\n\n\n\ncount\n392.00\n392.00\n392.00\n392.00\n392.00\n392.00\n392.00\n392.00\n\n\nmean\n23.45\n5.47\n194.41\n104.47\n2977.58\n15.54\n75.98\n1.58\n\n\nstd\n7.81\n1.71\n104.64\n38.49\n849.40\n2.76\n3.68\n0.81\n\n\nmin\n9.00\n3.00\n68.00\n46.00\n1613.00\n8.00\n70.00\n1.00\n\n\n25%\n17.00\n4.00\n105.00\n75.00\n2225.25\n13.78\n73.00\n1.00\n\n\n50%\n22.75\n4.00\n151.00\n93.50\n2803.50\n15.50\n76.00\n1.00\n\n\n75%\n29.00\n8.00\n275.75\n126.00\n3614.75\n17.02\n79.00\n2.00\n\n\nmax\n46.60\n8.00\n455.00\n230.00\n5140.00\n24.80\n82.00\n3.00\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n# get column names of the dataframe\nAuto.columns\n\nIndex(['mpg', 'cylinders', 'displacement', 'horsepower', 'weight',\n       'acceleration', 'year', 'origin', 'name'],\n      dtype='object')\n\n\n\n# seaborn scatterplot\npl = sns.jointplot(x='cylinders', y='mpg', data=Auto)\n\n\n\n\n\n# changing data type of a column into category\nAuto['cylinders'] = Auto['cylinders'].astype('category')\nAuto.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 392 entries, 0 to 396\nData columns (total 9 columns):\n #   Column        Non-Null Count  Dtype   \n---  ------        --------------  -----   \n 0   mpg           392 non-null    float64 \n 1   cylinders     392 non-null    category\n 2   displacement  392 non-null    float64 \n 3   horsepower    392 non-null    float64 \n 4   weight        392 non-null    int64   \n 5   acceleration  392 non-null    float64 \n 6   year          392 non-null    int64   \n 7   origin        392 non-null    int64   \n 8   name          392 non-null    object  \ndtypes: category(1), float64(4), int64(3), object(1)\nmemory usage: 36.3+ KB\n\n\n\n# seaborn boxplot \nsns.boxplot(x='cylinders', y='mpg', data=Auto)\n\n&lt;matplotlib.axes._subplots.AxesSubplot at 0x7f7c933c7d90&gt;\n\n\n\n\n\n\n# seaborn displot\nsns.displot(x=\"mpg\", data=Auto, bins=15)\n\n\n\n\n\n# seaborn pairplot for selected variables, colored by another\nsns.pairplot(Auto, vars=['mpg', 'displacement', 'horsepower', 'weight', 'acceleration'], hue='cylinders')\n\n\n\n\n\n# summary statistics for all dataframe columns, including non-numerical columns\nAuto.describe(include='all')\n\n\n  \n    \n      \n\n\n\n\n\n\nmpg\ncylinders\ndisplacement\nhorsepower\nweight\nacceleration\nyear\norigin\nname\n\n\n\n\ncount\n392.00\n392.0\n392.00\n392.00\n392.00\n392.00\n392.00\n392.00\n392\n\n\nunique\nNaN\n5.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n301\n\n\ntop\nNaN\n4.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\namc matador\n\n\nfreq\nNaN\n199.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n5\n\n\nmean\n23.45\nNaN\n194.41\n104.47\n2977.58\n15.54\n75.98\n1.58\nNaN\n\n\nstd\n7.81\nNaN\n104.64\n38.49\n849.40\n2.76\n3.68\n0.81\nNaN\n\n\nmin\n9.00\nNaN\n68.00\n46.00\n1613.00\n8.00\n70.00\n1.00\nNaN\n\n\n25%\n17.00\nNaN\n105.00\n75.00\n2225.25\n13.78\n73.00\n1.00\nNaN\n\n\n50%\n22.75\nNaN\n151.00\n93.50\n2803.50\n15.50\n76.00\n1.00\nNaN\n\n\n75%\n29.00\nNaN\n275.75\n126.00\n3614.75\n17.02\n79.00\n2.00\nNaN\n\n\nmax\n46.60\nNaN\n455.00\n230.00\n5140.00\n24.80\n82.00\n3.00\nNaN\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n# summary statistics for a single column and wrapped as dataframe for pretty table display in jupyter\npd.DataFrame(Auto['mpg'].describe())\n\n\n  \n    \n      \n\n\n\n\n\n\nmpg\n\n\n\n\ncount\n392.00\n\n\nmean\n23.45\n\n\nstd\n7.81\n\n\nmin\n9.00\n\n\n25%\n17.00\n\n\n50%\n22.75\n\n\n75%\n29.00\n\n\nmax\n46.60"
  },
  {
    "objectID": "Chapter 2_Lab.html#knn-example",
    "href": "Chapter 2_Lab.html#knn-example",
    "title": "2  Statistical Learning",
    "section": "2.5 KNN example",
    "text": "2.5 KNN example\nThe following code is modified from https://github.com/empathy87/The-Elements-of-Statistical-Learning-Python-Notebooks\n\nGenerates 10 means \\(m_k\\) from a bivariate Gaussian distrubition for each color:\n\n\\(N((1, 0)^T, \\textbf{I})\\) for BLUE\n\\(N((0, 1)^T, \\textbf{I})\\) for ORANGE\n\nFor each color generates 100 observations as following:\n\nFor each observation it picks \\(m_k\\) at random with probability 1/10.\nThen generates a \\(N(m_k,\\textbf{I}/5)\\)\n\n\n\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn.mixture._gaussian_mixture import _compute_precision_cholesky\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import accuracy_score\n\n\n# load training data that was used in the book\ndf = pd.read_csv(\"/content/drive/MyDrive/Lab/Data/mixture.txt\")\nX_train = df[['x1', 'x2']].values\ny_train = df.y.values\n\n# set the known BLUE and ORANGE clusters means\nblue_means = np.array([[-0.25343316, 1.7414788], [0.26669318, 0.3712341],\n                       [2.09646921, 1.2333642], [-0.06127272, -0.2086791],\n                       [2.70354085, 0.5968283], [2.37721198, -1.1864147],\n                       [1.05690759, -0.6838939], [0.57888354, -0.0683458],\n                       [0.62425213, 0.5987384], [1.67335495, -0.2893159]])\norange_means = np.array([[1.19936869, 0.2484086], [-0.30256110, 0.9454190],\n                         [0.05727232, 2.4197271], [1.32932203, 0.8192260],\n                         [-0.07938424, 1.6138017], [3.50792673, 1.0529863],\n                         [1.61392290, 0.6717378], [1.00753570, 1.3683071],\n                         [-0.45462141, 1.0860697], [-1.79801805, 1.9297806]])\nall_means = np.vstack((blue_means, orange_means))\n\ngaussian_mixture_model = GaussianMixture(\n    n_components=20,\n    covariance_type='spherical',\n    means_init=all_means,\n    random_state=1\n).fit(all_means)\n# set known covariances\ngaussian_mixture_model.covariances_ = [1/5]*20\n# it looks like a hack, but GaussianMixture uses precisions_cholesky_\n# for predict_proba method. Because we changed covariances_ we need\n# to recalculate precisions_cholesky_ too.\ngaussian_mixture_model.precisions_cholesky_ = _compute_precision_cholesky(\n    gaussian_mixture_model.covariances_,\n    gaussian_mixture_model.covariance_type)\n\n# sample 10,000 points for testing\nX_test, y_test = gaussian_mixture_model.sample(10000)\n# y_test contains sampled component indices\n# index &lt; 10 means that the class is BLUE (0)\ny_test = 1*(y_test &gt;= 10)\n\n\ndef optimal_bayes_predict(X):\n    components_proba = gaussian_mixture_model.predict_proba(X)\n    # first 10 components are BLUE(0), and others are BROWN(1)\n    blue_proba = np.sum(components_proba[:, :10], axis=1)\n    brown_proba = np.sum(components_proba[:, 10:], axis=1)\n    y_hat = 1*(blue_proba &lt; brown_proba)\n    return y_hat\n\n\nbayes_error_rate = 1 - accuracy_score(y_train, optimal_bayes_predict(X_train))\nprint(f'The optimal Bayes error rate = {bayes_error_rate}')\n\nThe optimal Bayes error rate = 0.15000000000000002\n\n\n\nbayes_error_rate = 1 - accuracy_score(y_test, optimal_bayes_predict(X_test))\nprint(f'The optimal Bayes error rate = {bayes_error_rate}')\n\nThe optimal Bayes error rate = 0.2148\n\n\n\n# define commonly used colors\nGRAY1, GRAY4, PURPLE = '#231F20', '#646369', '#A020F0'\nBLUE, ORANGE, BLUE1 = '#57B5E8', '#E69E00', '#174A7E'\n\n# configure all plots font family and border line widths\nplt.rcParams['font.family'] = 'Arial'\nplt.rcParams['axes.linewidth'] = 0.5\n\n# prepares a plot with a title and circles representing training data\ndef plot_train_data(title):\n    fig, ax = plt.subplots(figsize=(2.8, 2.8), dpi=110)\n    ax.set_aspect(1.3)\n    ax.scatter(X_train[:, 0], X_train[:, 1], s=18, facecolors='none',\n               edgecolors=np.array([BLUE, ORANGE])[y_train])\n    ax.tick_params(\n        bottom=False, left=False, labelleft=False, labelbottom=False)\n    ax.set_xlim(-2.6, 4.2)\n    ax.set_ylim(-2.0, 2.9)\n    fig.subplots_adjust(left=0, right=1, top=1, bottom=0)\n    ax.text(-2.6, 3.2, title, color=GRAY4, fontsize=9)\n    for spine in ax.spines.values():\n        spine.set_color(GRAY1)\n    return fig, ax\n\n# test it\n_, _ = plot_train_data('Training data')\n\nWARNING:matplotlib.font_manager:findfont: Font family ['Arial'] not found. Falling back to DejaVu Sans.\n\n\n\n\n\n\n# given a model prediction function computes X points on n x n grid and the\n# corresponding predicted classes\ndef fill_prediction_grid(n1, n2, predict):\n    x1, x2 = np.linspace(-2.6, 4.2, n1), np.linspace(-2.0, 2.9, n2)\n    X = np.transpose([np.tile(x1, n2), np.repeat(x2, n1)])\n    y = predict(X)\n    return X, y\n\n\n# given a model prediction function computes X0 and X1 n x n meshgrids\n# and the corresponing predicted classes meshgrid\ndef fill_prediction_meshgrid(predict):\n    n1, n2 = 1000, 1000\n    X, y = fill_prediction_grid(n1, n2, predict)\n    return X[:, 0].reshape(n1, n2), X[:, 1].reshape(n1, n2), y.reshape(n1, n2)\n\n\n# given a model prediction function plots train data, model decision\n# bounary and background dots\ndef plot_model(predict, title):\n    fig, ax = plot_train_data(title)\n    # plot background dots\n    X, y = fill_prediction_grid(69, 99, predict)\n    ax.scatter(X[:, 0], X[:, 1], marker='.', lw=0, s=2,\n               c=np.array([BLUE, ORANGE])[y])\n    # plot the decision boundary\n    X0, X1, Y = fill_prediction_meshgrid(predict)\n    ax.contour(X0, X1, Y, [0.5], colors=GRAY1, linewidths=[0.7])\n    return fig, ax\n\n# plot the optimal Bayes decision boundary\n_, _ = plot_model(optimal_bayes_predict, 'Bayes Optimal Classifier')\n\n\n\n\n\n# lets save Bayes meshgrids for optimal decision boundary plotting\nX0_bayes, X1_bayes, Y_bayes = fill_prediction_meshgrid(optimal_bayes_predict)\n\n\n# given a model prediction function plots performance statistics\ndef plot_model_stat(predict, title):\n    fig, ax = plot_model(predict, title)\n    ax.contour(X0_bayes, X1_bayes, Y_bayes, [0.5], colors='purple',\n               linewidths=[0.5], linestyles='dashed')\n    test_error_rate = 1 - accuracy_score(y_test, predict(X_test))\n    train_error_rate = 1 - accuracy_score(y_train, predict(X_train))\n    parms = {'color': GRAY1, 'fontsize': 7,\n             'bbox': {'facecolor': 'white', 'pad': 3, 'edgecolor': 'none'}}\n    ax.text(-2.42, -1.35, f'Training Error: {train_error_rate:.3f}', **parms)\n    ax.text(-2.42, -1.62, f'Test Error:       {test_error_rate:.3f}', **parms)\n    ax.text(-2.42, -1.89, f'Bayes Error:    {bayes_error_rate:.3f}', **parms)\n    return fig, ax\n\n\n# Run GridSearchCV to find the best n_neighbors parameter using the 10-folds\n# CV. It finds 12, but the book uses 15-Nearest Neighbor Classifier because\n# the authors selected the most parsimonious model within one standard error\n# from the best model (one standard error rule). We will apply this rule in\n# other examples, not here.\nk_neighbors_grid_search = GridSearchCV(\n    KNeighborsClassifier(),\n    {'n_neighbors': list(range(1, 50))},\n    cv=10\n).fit(X_train, y_train)\nk_neighbors_grid_search.best_params_\n\n{'n_neighbors': 12}\n\n\n\n# PAGE 14. Use 15-nearest-neighbor averaging of the binary coded response as\n#          the method of fitting. Thus Y-hat is the proportion of ORANGE’s in\n#          the neighborhood, and so assigning class ORANGE to G-hat if\n#          Y-hat&gt;0.5 amounts to a majority vote in the neighborhood.\nneighbors15_classifier = KNeighborsClassifier(\n    n_neighbors=15\n).fit(X_train, y_train)\n_, _ = plot_model_stat(\n    neighbors15_classifier.predict, '15-Nearest Neighbor Classifier')\n\nWARNING:matplotlib.font_manager:findfont: Font family ['Arial'] not found. Falling back to DejaVu Sans.\n\n\n\n\n\n\n# PAGE 16. The classes are coded as a binary variable (BLUE = 0,ORANGE = 1),\n#          and then predicted by 1-nearest-neighbor classiﬁcation.\nneighbors1_classifier = KNeighborsClassifier(\n    n_neighbors=1\n).fit(X_train, y_train)\n_, _ = plot_model_stat(\n    neighbors1_classifier.predict, '1−Nearest Neighbor Classifier')\n\n\n\n\n\nn_neighbors_vals = list(range(1, 30, 2))\nk_neighbors_grid_search = GridSearchCV(\n    KNeighborsClassifier(),\n    {'n_neighbors': n_neighbors_vals},\n    cv=10, scoring='accuracy',\n    return_train_score=True\n).fit(X_train, y_train)\n\ntrain_errors, test_errors = [], []\nfor k in n_neighbors_vals:\n    clf = KNeighborsClassifier(n_neighbors=k).fit(X_train, y_train)\n    train_errors.append(1 - clf.score(X_train, y_train))\n    test_errors.append(1 - clf.score(X_test, y_test))\n\n\n# PAGE 467. k-nearest-neighbors on the two-class mixture data. The upper\n#           panel shows the misclassification errors as a function of\n#           neighborhood size. Standard error bars are included for 10-fold\n#           cross validation. The lower panel shows the decision boundary\n#           for 7-nearest-neighbors, which appears to be optimal for minimizing\n#           test error.\ncv_erros = 1 - np.vstack([\n    k_neighbors_grid_search.cv_results_[f'split{i}_test_score']\n    for i in range(10)]).T\ncv_mean_errors = np.mean(cv_erros, axis=1)\ncv_std_errors = np.std(cv_erros, ddof=1, axis=1)/np.sqrt(10)\nbest_index = np.argmin(cv_mean_errors)\nbest_err, best_std_err = cv_mean_errors[best_index], cv_std_errors[best_index]\n\nfig, ax = plt.subplots(figsize=(2.8, 2.8), dpi=110)\nfig.subplots_adjust(left=0, right=1, top=1, bottom=0)\nax.scatter(n_neighbors_vals, test_errors, c='#0000FF', s=9)\nax.plot(n_neighbors_vals, test_errors, c='#0000FF', linewidth=0.8,\n        label='Test Error')\nax.plot(n_neighbors_vals, cv_mean_errors, c='#00FF00', linewidth=0.8,\n        label='10-fold CV')\nax.scatter(n_neighbors_vals, train_errors, c=ORANGE, s=9)\nax.plot(n_neighbors_vals, train_errors, c=ORANGE, linewidth=0.8,\n        label='Training Error')\n\nax.errorbar(n_neighbors_vals, cv_mean_errors,\n            color='#00FF00', linestyle='None', marker='o', elinewidth=0.8,\n            markersize=3, yerr=cv_std_errors, ecolor='#00FF00', capsize=3)\nax.axhline(y=best_err+best_std_err, c=PURPLE, linewidth=0.8, linestyle='--',\n           label='Bayes Error')\nfor i in ax.get_yticklabels() + ax.get_xticklabels():\n    i.set_fontsize(6)\nax.set_xlabel('Number of Neighbors', color=GRAY4, fontsize=8)\nax.set_ylabel('Misclassification Errors', color=GRAY4, fontsize=8)\n\nneighbors7_classifier = KNeighborsClassifier(\n    n_neighbors=7).fit(X_train, y_train)\nplot_model_stat(neighbors7_classifier.predict, '7-Nearest Neighbors')\n_ = ax.legend(loc='best', prop={'size': 8})\n\nWARNING:matplotlib.font_manager:findfont: Font family ['Arial'] not found. Falling back to DejaVu Sans.\nWARNING:matplotlib.font_manager:findfont: Font family ['Arial'] not found. Falling back to DejaVu Sans."
  },
  {
    "objectID": "Chapter 2_Lab.html#references",
    "href": "Chapter 2_Lab.html#references",
    "title": "2  Statistical Learning",
    "section": "2.6 References",
    "text": "2.6 References\n[1] https://web.stanford.edu/~hastie/ElemStatLearn/\n[2] https://github.com/emredjan/ISL-python\n[3] https://github.com/empathy87/The-Elements-of-Statistical-Learning-Python-Notebooks"
  },
  {
    "objectID": "Chapter_3_Lab.html#simple-linear-regression",
    "href": "Chapter_3_Lab.html#simple-linear-regression",
    "title": "3  Regression",
    "section": "3.1 Simple Linear Regression",
    "text": "3.1 Simple Linear Regression\nThe ISLR2 contains the Boston data set, which records medv (median house value) for \\(506\\) census tracts in Boston. We will seek to predict medv using \\(12\\) predictors such as rmvar (average number of rooms per house), age (average age of houses), and lstat (percent of households with low socioeconomic status).\n\nimport pandas as pd\nimport numpy as np\n\n# Modeling\nfrom sklearn.metrics import mean_squared_error, explained_variance_score, r2_score\nfrom sklearn import linear_model\nfrom sklearn.preprocessing import PolynomialFeatures\n\nimport statsmodels.formula.api as smf\nimport statsmodels.api as sm\nfrom statsmodels.sandbox.regression.predstd import wls_prediction_std\nfrom statsmodels.stats.outliers_influence import OLSInfluence\nfrom statsmodels.graphics.regressionplots import *\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor as vif\n\nfrom scipy import stats\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\nplt.style.use('seaborn-white')\nsns.set_context(\"notebook\", font_scale=1.5, rc={\"lines.linewidth\": 2.5})\n\n\n# temp fix from https://nbviewer.jupyter.org/gist/thatneat/10286720\ndef transform_exog_to_model(fit, exog):\n    transform=True\n    self=fit\n\n    # The following is lifted straight from statsmodels.base.model.Results.predict()\n    if transform and hasattr(self.model, 'formula') and exog is not None:\n        from patsy import dmatrix\n        exog = dmatrix(self.model.data.orig_exog.design_info.builder,\n                       exog)\n\n    if exog is not None:\n        exog = np.asarray(exog)\n        if exog.ndim == 1 and (self.model.exog.ndim == 1 or\n                               self.model.exog.shape[1] == 1):\n            exog = exog[:, None]\n        exog = np.atleast_2d(exog)  # needed in count model shape[1]\n\n    # end lifted code\n    return exog\n\n\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\nMounted at /content/drive\n\n\n\nBoston = pd.read_csv(\"/content/drive/MyDrive/Lab/Data/Boston.csv\", index_col='Unnamed: 0')\nBoston.index = Boston.index - 1 \nBoston.head()\n\n\n  \n    \n      \n\n\n\n\n\n\ncrim\nzn\nindus\nchas\nnox\nrm\nage\ndis\nrad\ntax\nptratio\nlstat\nmedv\n\n\n\n\n0\n0.00632\n18.0\n2.31\n0\n0.538\n6.575\n65.2\n4.0900\n1\n296\n15.3\n4.98\n24.0\n\n\n1\n0.02731\n0.0\n7.07\n0\n0.469\n6.421\n78.9\n4.9671\n2\n242\n17.8\n9.14\n21.6\n\n\n2\n0.02729\n0.0\n7.07\n0\n0.469\n7.185\n61.1\n4.9671\n2\n242\n17.8\n4.03\n34.7\n\n\n3\n0.03237\n0.0\n2.18\n0\n0.458\n6.998\n45.8\n6.0622\n3\n222\n18.7\n2.94\n33.4\n\n\n4\n0.06905\n0.0\n2.18\n0\n0.458\n7.147\n54.2\n6.0622\n3\n222\n18.7\n5.33\n36.2\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\nprint(Boston.shape)\nprint(Boston.info())\n\n(506, 13)\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 506 entries, 0 to 505\nData columns (total 13 columns):\n #   Column   Non-Null Count  Dtype  \n---  ------   --------------  -----  \n 0   crim     506 non-null    float64\n 1   zn       506 non-null    float64\n 2   indus    506 non-null    float64\n 3   chas     506 non-null    int64  \n 4   nox      506 non-null    float64\n 5   rm       506 non-null    float64\n 6   age      506 non-null    float64\n 7   dis      506 non-null    float64\n 8   rad      506 non-null    int64  \n 9   tax      506 non-null    int64  \n 10  ptratio  506 non-null    float64\n 11  lstat    506 non-null    float64\n 12  medv     506 non-null    float64\ndtypes: float64(10), int64(3)\nmemory usage: 55.3 KB\nNone\n\n\nWe will start by using the ols() function to fit a simple linear regression model, with medv as the response and lstat as the predictor.\nThe basic syntax is \\(ols(y \\sim x, data)\\), where y is the response, x is the predictor, and data is the data set in which these two variables are kept.\n\n# est = smf.ols(y ~ x, data)\nest = smf.ols('medv ~ lstat',data = Boston).fit()\nprint(est.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   medv   R-squared:                       0.544\nModel:                            OLS   Adj. R-squared:                  0.543\nMethod:                 Least Squares   F-statistic:                     601.6\nDate:                Sun, 25 Sep 2022   Prob (F-statistic):           5.08e-88\nTime:                        08:44:49   Log-Likelihood:                -1641.5\nNo. Observations:                 506   AIC:                             3287.\nDf Residuals:                     504   BIC:                             3295.\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     34.5538      0.563     61.415      0.000      33.448      35.659\nlstat         -0.9500      0.039    -24.528      0.000      -1.026      -0.874\n==============================================================================\nOmnibus:                      137.043   Durbin-Watson:                   0.892\nProb(Omnibus):                  0.000   Jarque-Bera (JB):              291.373\nSkew:                           1.453   Prob(JB):                     5.36e-64\nKurtosis:                       5.319   Cond. No.                         29.7\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nAnother way is to use scikit-learn like API as follows:\n\nX = Boston[\"lstat\"]\nX = sm.add_constant(X)\ny = Boston[\"medv\"]\nmodel = sm.OLS(y,X).fit()\nprint(model.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   medv   R-squared:                       0.544\nModel:                            OLS   Adj. R-squared:                  0.543\nMethod:                 Least Squares   F-statistic:                     601.6\nDate:                Sun, 25 Sep 2022   Prob (F-statistic):           5.08e-88\nTime:                        08:44:49   Log-Likelihood:                -1641.5\nNo. Observations:                 506   AIC:                             3287.\nDf Residuals:                     504   BIC:                             3295.\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         34.5538      0.563     61.415      0.000      33.448      35.659\nlstat         -0.9500      0.039    -24.528      0.000      -1.026      -0.874\n==============================================================================\nOmnibus:                      137.043   Durbin-Watson:                   0.892\nProb(Omnibus):                  0.000   Jarque-Bera (JB):              291.373\nSkew:                           1.453   Prob(JB):                     5.36e-64\nKurtosis:                       5.319   Cond. No.                         29.7\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n/usr/local/lib/python3.7/dist-packages/statsmodels/tsa/tsatools.py:142: FutureWarning: In a future version of pandas all arguments of concat except for the argument 'objs' will be keyword-only\n  x = pd.concat(x[::order], 1)\n\n\nWhen statsmodel detected as a categorical variable, and thus each of its different values are treated as different entities. An integer column can be forced to be treated as categorical using: model = ols('VIQ ~ C(Gender)', data).fit() By default, statsmodels treats a categorical variable with K possible values as K-1 ‘dummy’ boolean variables (the last level being absorbed into the intercept term). This is almost always a good default choice - however, it is possible to specify different encodings for categorical variables (http://statsmodels.sourceforge.net/devel/contrasts.html).\nIn order to obtain a confidence interval for the coefficient estimates, we can use the conf_int() command.\n\nest.conf_int(alpha=0.05)      # default alpha=0.05 : 95% confidence interval\n\n\n  \n    \n      \n\n\n\n\n\n\n0\n1\n\n\n\n\nIntercept\n33.448457\n35.659225\n\n\nlstat\n-1.026148\n-0.873951\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nThe predict() function can be used to produce the prediction for new instance.\n\nX_new = pd.DataFrame({'lstat':[5,10,15]})\nest.predict(X_new)\n\n0    29.803594\n1    25.053347\n2    20.303101\ndtype: float64\n\n\n\n# prediction interval: _, lower bound, upper bound\ntransformed = transform_exog_to_model(est, X_new)\nwls_prediction_std(est, transformed , weights=[1])[1:]\n\n/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: DeprecationWarning: The DesignInfo.builder attribute is deprecated starting in patsy v0.4.0; distinct builder objects have been eliminated and design_info.builder is now just a long-winded way of writing 'design_info' (i.e. the .builder attribute just returns self)\n  if __name__ == '__main__':\n\n\n(array([17.56567478, 12.82762635,  8.0777421 ]),\n array([42.04151344, 37.27906833, 32.52845905]))\n\n\nThe get_prediction() function can be used to produce confidence intervals and prediction intervals for the prediction of medv for a given value of lstat.\n\npredictions = est.get_prediction(X_new)\npredictions.summary_frame(alpha=0.05)\n\n\n  \n    \n      \n\n\n\n\n\n\nmean\nmean_se\nmean_ci_lower\nmean_ci_upper\nobs_ci_lower\nobs_ci_upper\n\n\n\n\n0\n29.803594\n0.405247\n29.007412\n30.599776\n17.565675\n42.041513\n\n\n1\n25.053347\n0.294814\n24.474132\n25.632563\n12.827626\n37.279068\n\n\n2\n20.303101\n0.290893\n19.731588\n20.874613\n8.077742\n32.528459\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nFor instance, the 95,% confidence interval associated with a lstat value of 10 is \\((24.47, 25.63)\\), and the 95,% prediction interval is \\((12.828, 37.28)\\). As expected, the confidence and prediction intervals are centered around the same point (a predicted value of \\(25.05\\) for medv when lstat equals 10), but the latter are substantially wider.\nWe will now plot medv and lstat along with the least squares regression line using matplotlib or regplot() functions.\n\nsns.scatterplot(x='lstat', y='medv', data=Boston)\n\nX = pd.DataFrame({'lstat':[Boston.lstat.min(), Boston.lstat.max()]})\nY_pred = est.predict(X)\nsns.lineplot(x=X.values[:,0], y=Y_pred.values, color='red')\nplt.xlabel(\"lstat\")\nplt.ylabel(\"medv\")\n\nText(0, 0.5, 'medv')\n\n\n\n\n\n\nsns.regplot(x='lstat',y='medv', data=Boston)\n\n&lt;matplotlib.axes._subplots.AxesSubplot at 0x7f67f36559d0&gt;\n\n\n\n\n\nNext we examine some diagnostic plots, several of which were discussed in Section 3.3.3. Four diagnostic plots are plotted according to the results from ols(). Also check https://www.statsmodels.org/stable/generated/statsmodels.stats.outliers_influence.OLSInfluence.html and https://www.statsmodels.org/stable/generated/statsmodels.regression.linear_model.OLSResults.html#statsmodels.regression.linear_model.OLSResults\n\ninfulence = OLSInfluence(est)\n\n\nols_sm_resid = est.resid # residuals\nols_fitted = est.fittedvalues\nprstd = wls_prediction_std(est)[0]\nols_sm_resid_stud = ols_sm_resid / prstd # studentized residuals or infulence.resid_studentized_internal\n\n\ninfulence = OLSInfluence(est)\nols_sm_resid = est.resid # residuals\nols_fitted = est.fittedvalues\nols_sm_resid_stud = infulence.resid_studentized_internal\nleverage = OLSInfluence(est).hat_matrix_diag\n\n\nf, axes = plt.subplots(2, 2, sharex=False, sharey=False) \nf.set_figheight(15)\nf.set_figwidth(20)\n\nsns.regplot(x='lstat', y='medv', data=Boston, ax=axes[0, 0], scatter_kws={'alpha': 0.5}) # regression plot\naxes[0, 0].set_title(\"reg plot\")\nsns.scatterplot(x=ols_fitted,y=ols_sm_resid, ax=axes[0, 1], alpha=0.5)\naxes[0, 1].set_xlabel(\"fittedvalues\")\naxes[0, 1].set_ylabel(\"residual\")\naxes[0, 1].set_title(\"residual plot\")\n#sns.residplot(x=est.predict(), y='medv', data=df, ax=axes[0, 1], scatter_kws={'alpha': '0.5'}) # residual plot\n\n#plot_leverage_resid2(ols_sm_results, ax=axes[1, 0], color='red') # leverage plot\n\n# custom leverage plot instead of above\n#axes[1, 0].autoscale(enable=True, axis='y', tight=True)\naxes[1, 0].scatter(leverage, ols_sm_resid_stud, alpha=0.5, color='red')\naxes[1, 0].set_xlabel(\"Leverage\")\naxes[1, 0].set_ylabel(\"Studentized residuals\")\n#axes[1, 0].set_ylim(-5, 5)\naxes[1, 0].set_title(\"leverage\")\n# studentized residual plot\naxes[1, 1].scatter(ols_fitted, ols_sm_resid_stud, alpha=0.5, color='magenta')\naxes[1, 1].axhline(0, ls=\":\", c=\".2\")\naxes[1, 1].axhline(-3, ls=\":\", c=\".6\")\naxes[1, 1].axhline(3, ls=\":\", c=\".6\")\naxes[1, 1].set_ylim(-5, 5)\naxes[1, 1].set_xlabel(\"fittedvalues\")\naxes[1, 1].set_ylabel(\"Studentized residuals\")\naxes[1, 1].set_title(\"studentized residual plot\")\n\nx = est.fittedvalues[np.logical_or(ols_sm_resid_stud &gt; 3, ols_sm_resid_stud &lt; -3)]\ny = ols_sm_resid_stud[np.logical_or(ols_sm_resid_stud &gt; 3, ols_sm_resid_stud &lt; -3)]\n\nfor i, x, y in zip(x.index, x, y):\n    axes[1, 1].annotate(i, xy=(x, y));\n\n\n\n\n\n3.1.1 Optional - Other useful plot\nSeaborn also has the functionality of residual plot\n\nsns.residplot(x=\"lstat\", y=\"medv\", data=Boston)\n\n&lt;matplotlib.axes._subplots.AxesSubplot at 0x7f67f07b7890&gt;\n\n\n\n\n\nStatsmodel has more diagonostic plot, like the influence plot where the size of the points is relate to Cook’s distance. https://www.statsmodels.org/stable/examples/notebooks/generated/regression_plots.html\n\nf = sm.graphics.influence_plot(est, criterion=\"cooks\")\nf.set_figheight(10)\nf.set_figwidth(10)\n\n\n\n\nThe plot_regress_exog function is a convenience function that gives a 2x2 plot containing the dependent variable and fitted values with confidence intervals vs. the independent variable chosen, the residuals of the model vs. the chosen independent variable, a partial regression plot, and a CCPR plot. This function can be used for quickly checking modeling assumptions with respect to a single regressor. Check https://www.statsmodels.org/stable/examples/notebooks/generated/regression_plots.html#Component-Component-plus-Residual-(CCPR)-Plots\n\nf = sm.graphics.plot_regress_exog(est, \"lstat\")\nf.set_figheight(10)\nf.set_figwidth(15)\nf.tight_layout(pad=1.0)"
  },
  {
    "objectID": "Chapter_3_Lab.html#multiple-regression",
    "href": "Chapter_3_Lab.html#multiple-regression",
    "title": "3  Regression",
    "section": "3.2 Multiple Regression",
    "text": "3.2 Multiple Regression\nIn order to fit a multiple linear regression model using least squares, we again use the ols() function. The syntax \\(ols(y \\sim x1 + x2 + x3)\\) is used to fit a model with three predictors, x1, x2, and x3. The summary() function now outputs the regression coefficients for all the predictors.\n\n#string_cols = ' + '.join(data.columns[:-1])\nest = smf.ols('medv ~ lstat+age',data = Boston).fit()\nprint(est.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   medv   R-squared:                       0.551\nModel:                            OLS   Adj. R-squared:                  0.549\nMethod:                 Least Squares   F-statistic:                     309.0\nDate:                Sun, 25 Sep 2022   Prob (F-statistic):           2.98e-88\nTime:                        08:44:53   Log-Likelihood:                -1637.5\nNo. Observations:                 506   AIC:                             3281.\nDf Residuals:                     503   BIC:                             3294.\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     33.2228      0.731     45.458      0.000      31.787      34.659\nlstat         -1.0321      0.048    -21.416      0.000      -1.127      -0.937\nage            0.0345      0.012      2.826      0.005       0.011       0.059\n==============================================================================\nOmnibus:                      124.288   Durbin-Watson:                   0.945\nProb(Omnibus):                  0.000   Jarque-Bera (JB):              244.026\nSkew:                           1.362   Prob(JB):                     1.02e-53\nKurtosis:                       5.038   Cond. No.                         201.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nThe Boston data set contains 12 variables, and so it would be cumbersome to have to type all of these in order to perform a regression using all of the predictors. Instead, we can use the code:\n\ncolumns_selected = \"+\".join(Boston.columns.difference([\"medv\"]))\nmy_formula = \"medv ~ \" + columns_selected\nest = smf.ols(my_formula,data = Boston).fit()\nprint(est.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   medv   R-squared:                       0.734\nModel:                            OLS   Adj. R-squared:                  0.728\nMethod:                 Least Squares   F-statistic:                     113.5\nDate:                Sun, 25 Sep 2022   Prob (F-statistic):          2.23e-133\nTime:                        08:44:53   Log-Likelihood:                -1504.9\nNo. Observations:                 506   AIC:                             3036.\nDf Residuals:                     493   BIC:                             3091.\nDf Model:                          12                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     41.6173      4.936      8.431      0.000      31.919      51.316\nage            0.0036      0.013      0.271      0.787      -0.023       0.030\nchas           2.8400      0.870      3.264      0.001       1.131       4.549\ncrim          -0.1214      0.033     -3.678      0.000      -0.186      -0.057\ndis           -1.4908      0.202     -7.394      0.000      -1.887      -1.095\nindus          0.0135      0.062      0.217      0.829      -0.109       0.136\nlstat         -0.5520      0.051    -10.897      0.000      -0.652      -0.452\nnox          -18.7580      3.851     -4.870      0.000     -26.325     -11.191\nptratio       -0.9375      0.132     -7.091      0.000      -1.197      -0.678\nrad            0.2894      0.067      4.325      0.000       0.158       0.421\nrm             3.6581      0.420      8.705      0.000       2.832       4.484\ntax           -0.0127      0.004     -3.337      0.001      -0.020      -0.005\nzn             0.0470      0.014      3.384      0.001       0.020       0.074\n==============================================================================\nOmnibus:                      171.096   Durbin-Watson:                   1.077\nProb(Omnibus):                  0.000   Jarque-Bera (JB):              709.937\nSkew:                           1.477   Prob(JB):                    6.90e-155\nKurtosis:                       7.995   Cond. No.                     1.17e+04\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 1.17e+04. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n\n\nWe can access the individual components of a summary object by name. Hence est.rsquared gives us the \\(R^2\\). The vif() function can be used to compute variance inflation factors. Most VIF’s are low to moderate for this data. Check https://www.statsmodels.org/stable/generated/statsmodels.stats.outliers_influence.variance_inflation_factor.html\n\nest.rsquared\n\n0.7343070437613075\n\n\n\n# don't forget to add constant if the ols model includes intercept\nboston = Boston.drop('medv', axis=1).assign(const=1)\nboston.head()\n\n\n  \n    \n      \n\n\n\n\n\n\ncrim\nzn\nindus\nchas\nnox\nrm\nage\ndis\nrad\ntax\nptratio\nlstat\nconst\n\n\n\n\n0\n0.00632\n18.0\n2.31\n0\n0.538\n6.575\n65.2\n4.0900\n1\n296\n15.3\n4.98\n1\n\n\n1\n0.02731\n0.0\n7.07\n0\n0.469\n6.421\n78.9\n4.9671\n2\n242\n17.8\n9.14\n1\n\n\n2\n0.02729\n0.0\n7.07\n0\n0.469\n7.185\n61.1\n4.9671\n2\n242\n17.8\n4.03\n1\n\n\n3\n0.03237\n0.0\n2.18\n0\n0.458\n6.998\n45.8\n6.0622\n3\n222\n18.7\n2.94\n1\n\n\n4\n0.06905\n0.0\n2.18\n0\n0.458\n7.147\n54.2\n6.0622\n3\n222\n18.7\n5.33\n1\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n# variance inflation factors\n\nfor i, col in enumerate(boston.columns):\n    if col == 'const':\n        pass\n    elif len(col) &gt; 6:\n        print(col, ':', \"{0:.2f}\".format(vif(boston.to_numpy(), i)))\n    else:\n        print(col, '\\t:', \"{0:.2f}\".format(vif(boston.to_numpy(), i)))\n\ncrim    : 1.77\nzn  : 2.30\nindus   : 3.99\nchas    : 1.07\nnox     : 4.37\nrm  : 1.91\nage     : 3.09\ndis     : 3.95\nrad     : 7.45\ntax     : 9.00\nptratio : 1.80\nlstat   : 2.87\n\n\nWhat if we would like to perform a regression using all of the variables but one? For example, in the above regression output, age has a high \\(p\\)-value. So we may wish to run a regression excluding this predictor. The following procedure results in a regression using all predictors except age.\n\ncolumns_selected = \"+\".join(Boston.columns.difference([\"medv\", \"age\"]))\nmy_formula = \"medv ~ \" + columns_selected\nlm_fit1 = smf.ols(formula = my_formula, data=Boston).fit()\nlm_fit1.summary().tables[1]\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n41.5251\n4.920\n8.441\n0.000\n31.859\n51.191\n\n\nchas\n2.8528\n0.868\n3.287\n0.001\n1.148\n4.558\n\n\ncrim\n-0.1214\n0.033\n-3.683\n0.000\n-0.186\n-0.057\n\n\ndis\n-1.5068\n0.193\n-7.825\n0.000\n-1.885\n-1.128\n\n\nindus\n0.0135\n0.062\n0.217\n0.829\n-0.109\n0.135\n\n\nlstat\n-0.5474\n0.048\n-11.483\n0.000\n-0.641\n-0.454\n\n\nnox\n-18.4851\n3.714\n-4.978\n0.000\n-25.782\n-11.188\n\n\nptratio\n-0.9346\n0.132\n-7.099\n0.000\n-1.193\n-0.676\n\n\nrad\n0.2879\n0.067\n4.322\n0.000\n0.157\n0.419\n\n\nrm\n3.6811\n0.411\n8.951\n0.000\n2.873\n4.489\n\n\ntax\n-0.0127\n0.004\n-3.333\n0.001\n-0.020\n-0.005\n\n\nzn\n0.0465\n0.014\n3.379\n0.001\n0.019\n0.074"
  },
  {
    "objectID": "Chapter_3_Lab.html#interaction-term",
    "href": "Chapter_3_Lab.html#interaction-term",
    "title": "3  Regression",
    "section": "3.3 Interaction term",
    "text": "3.3 Interaction term\nIt is easy to include interaction terms in a linear model using the ols() function. The syntax lstat:age tells Python to include an interaction term between lstat and age. The syntax lstat * age simultaneously includes lstat, age, and the interaction term lstat\\(\\times\\)age as predictors; it is a shorthand for lstat + age + lstat:age.\n\nest = smf.ols('medv ~ lstat*age',data = Boston).fit()\nprint(est.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   medv   R-squared:                       0.556\nModel:                            OLS   Adj. R-squared:                  0.553\nMethod:                 Least Squares   F-statistic:                     209.3\nDate:                Sun, 25 Sep 2022   Prob (F-statistic):           4.86e-88\nTime:                        08:44:54   Log-Likelihood:                -1635.0\nNo. Observations:                 506   AIC:                             3278.\nDf Residuals:                     502   BIC:                             3295.\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     36.0885      1.470     24.553      0.000      33.201      38.976\nlstat         -1.3921      0.167     -8.313      0.000      -1.721      -1.063\nage           -0.0007      0.020     -0.036      0.971      -0.040       0.038\nlstat:age      0.0042      0.002      2.244      0.025       0.001       0.008\n==============================================================================\nOmnibus:                      135.601   Durbin-Watson:                   0.965\nProb(Omnibus):                  0.000   Jarque-Bera (JB):              296.955\nSkew:                           1.417   Prob(JB):                     3.29e-65\nKurtosis:                       5.461   Cond. No.                     6.88e+03\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 6.88e+03. This might indicate that there are\nstrong multicollinearity or other numerical problems."
  },
  {
    "objectID": "Chapter_3_Lab.html#non-linear-transformations-of-the-predictors",
    "href": "Chapter_3_Lab.html#non-linear-transformations-of-the-predictors",
    "title": "3  Regression",
    "section": "3.4 Non-linear Transformations of the Predictors",
    "text": "3.4 Non-linear Transformations of the Predictors\nThe ols() function can also accommodate non-linear transformations of the predictors. For instance, given a predictor \\(X\\), we can create a predictor \\(X^2\\) using I(X**2). The function I() is needed since the ** has a special meaning in a formula object. We now perform a regression of medv onto lstat and lstat^2.\n\n#adding power term\nest = smf.ols('medv ~ lstat + I(lstat**2)',data = Boston).fit()\nprint(est.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   medv   R-squared:                       0.641\nModel:                            OLS   Adj. R-squared:                  0.639\nMethod:                 Least Squares   F-statistic:                     448.5\nDate:                Sun, 25 Sep 2022   Prob (F-statistic):          1.56e-112\nTime:                        08:44:54   Log-Likelihood:                -1581.3\nNo. Observations:                 506   AIC:                             3169.\nDf Residuals:                     503   BIC:                             3181.\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n=================================================================================\n                    coef    std err          t      P&gt;|t|      [0.025      0.975]\n---------------------------------------------------------------------------------\nIntercept        42.8620      0.872     49.149      0.000      41.149      44.575\nlstat            -2.3328      0.124    -18.843      0.000      -2.576      -2.090\nI(lstat ** 2)     0.0435      0.004     11.628      0.000       0.036       0.051\n==============================================================================\nOmnibus:                      107.006   Durbin-Watson:                   0.921\nProb(Omnibus):                  0.000   Jarque-Bera (JB):              228.388\nSkew:                           1.128   Prob(JB):                     2.55e-50\nKurtosis:                       5.397   Cond. No.                     1.13e+03\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 1.13e+03. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n\n\nThe near-zero \\(p\\)-value associated with the quadratic term suggests that it leads to an improved model. We use the anova() function to further quantify the extent to which the quadratic fit is superior to the linear fit.\n\nest2 = smf.ols('medv ~ lstat', data = Boston).fit()\nsm.stats.anova_lm(est2, est, typ=1)\n\n\n  \n    \n      \n\n\n\n\n\n\ndf_resid\nssr\ndf_diff\nss_diff\nF\nPr(&gt;F)\n\n\n\n\n0\n504.0\n19472.381418\n0.0\nNaN\nNaN\nNaN\n\n\n1\n503.0\n15347.243158\n1.0\n4125.13826\n135.199822\n7.630116e-28\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nHere Model 0 represents the linear submodel containing only one predictor, lstat, while Model 1 corresponds to the larger quadratic model that has two predictors, lstat and lstat^2. The anova() function performs a hypothesis test comparing the two models. The null hypothesis is that the two models fit the data equally well, and the alternative hypothesis is that the full model is superior. Here the \\(F\\)-statistic is \\(135\\) and the associated \\(p\\)-value is virtually zero. This provides very clear evidence that the model containing the predictors lstat and lstat^2 is far superior to the model that only contains the predictor lstat. This is not surprising, since earlier we saw evidence for non-linearity in the relationship between medv and lstat. If we type\nIn order to create a cubic fit, we can include a predictor of the form I(X**3). However, this approach can start to get cumbersome for higher-order polynomials. A better approach involves using the PolynomialFeatures() function to create the polynomial within ols(). For example, the following command produces a fifth-order polynomial fit:\n\npolynomial_features= PolynomialFeatures(degree=5) # using sklearn\nxp = polynomial_features.fit_transform(Boston.lstat.values.reshape(-1,1))[:,1:] #the intercept should be removed first\n\n\nols_smf = smf.ols(formula='medv ~ xp', data=Boston)\nols_smf_results = ols_smf.fit()\nprint(ols_smf_results.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   medv   R-squared:                       0.682\nModel:                            OLS   Adj. R-squared:                  0.679\nMethod:                 Least Squares   F-statistic:                     214.2\nDate:                Sun, 25 Sep 2022   Prob (F-statistic):          8.73e-122\nTime:                        08:44:54   Log-Likelihood:                -1550.6\nNo. Observations:                 506   AIC:                             3113.\nDf Residuals:                     500   BIC:                             3139.\nDf Model:                           5                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     67.6997      3.604     18.783      0.000      60.618      74.781\nxp[0]        -11.9911      1.526     -7.859      0.000     -14.989      -8.994\nxp[1]          1.2728      0.223      5.703      0.000       0.834       1.711\nxp[2]         -0.0683      0.014     -4.747      0.000      -0.097      -0.040\nxp[3]          0.0017      0.000      4.143      0.000       0.001       0.003\nxp[4]      -1.632e-05   4.42e-06     -3.692      0.000    -2.5e-05   -7.63e-06\n==============================================================================\nOmnibus:                      144.085   Durbin-Watson:                   0.987\nProb(Omnibus):                  0.000   Jarque-Bera (JB):              494.545\nSkew:                           1.292   Prob(JB):                    4.08e-108\nKurtosis:                       7.096   Cond. No.                     1.37e+08\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 1.37e+08. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n\n\nThis suggests that including additional polynomial terms, up to fifth order, leads to an improvement in the model fit! However, further investigation of the data reveals that no polynomial terms beyond fifth order have significant \\(p\\)-values in a regression fit.\nOf course, we are in no way restricted to using polynomial transformations of the predictors. Here we try a log transformation.\n\n# polynomial ols model with intercept\nols_smf = smf.ols(formula='medv ~ np.log(rm)', data=Boston)\n\n# fitted model and summary\nols_smf_results = ols_smf.fit()\nprint(ols_smf_results.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   medv   R-squared:                       0.436\nModel:                            OLS   Adj. R-squared:                  0.435\nMethod:                 Least Squares   F-statistic:                     389.3\nDate:                Sun, 25 Sep 2022   Prob (F-statistic):           1.22e-64\nTime:                        08:44:54   Log-Likelihood:                -1695.4\nNo. Observations:                 506   AIC:                             3395.\nDf Residuals:                     504   BIC:                             3403.\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept    -76.4878      5.028    -15.213      0.000     -86.366     -66.610\nnp.log(rm)    54.0546      2.739     19.732      0.000      48.672      59.437\n==============================================================================\nOmnibus:                      117.102   Durbin-Watson:                   0.681\nProb(Omnibus):                  0.000   Jarque-Bera (JB):              584.336\nSkew:                           0.916   Prob(JB):                    1.30e-127\nKurtosis:                       7.936   Cond. No.                         38.9\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
  },
  {
    "objectID": "Chapter_3_Lab.html#qualitative-predictors",
    "href": "Chapter_3_Lab.html#qualitative-predictors",
    "title": "3  Regression",
    "section": "3.5 Qualitative predictors",
    "text": "3.5 Qualitative predictors\nWe will now examine the Carseats data, which is part of the ISLR2. We will attempt to predict Sales(child car seat sales) in \\(400\\) locations based on a number of predictors.\n\nCarseats = pd.read_csv(\"/content/drive/MyDrive/Lab/Data/Carseats.csv\")\nprint(Carseats.shape)\nCarseats.head()\n\n(400, 11)\n\n\n\n  \n    \n      \n\n\n\n\n\n\nSales\nCompPrice\nIncome\nAdvertising\nPopulation\nPrice\nShelveLoc\nAge\nEducation\nUrban\nUS\n\n\n\n\n0\n9.50\n138\n73\n11\n276\n120\nBad\n42\n17\nYes\nYes\n\n\n1\n11.22\n111\n48\n16\n260\n83\nGood\n65\n10\nYes\nYes\n\n\n2\n10.06\n113\n35\n10\n269\n80\nMedium\n59\n12\nYes\nYes\n\n\n3\n7.40\n117\n100\n4\n466\n97\nMedium\n55\n14\nYes\nYes\n\n\n4\n4.15\n141\n64\n3\n340\n128\nBad\n38\n13\nYes\nNo\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nThe Carseats data includes qualitative predictors such as shelveloc, an indicator of the quality of the shelving location—that is, the space within a store in which the car seat is displayed—at each location. The predictor shelveloc takes on three possible values: Bad, Medium, and Good. Given a qualitative variable such as shelveloc, Python generates dummy variables automatically. Below we fit a multiple regression model that includes some interaction terms. The syntax X1:XP tells Python to include an interaction term between X1 and XP.\n\n# ols model with intercept\ncolumns_selected = \"+\".join(Carseats.columns.difference([\"Sales\"]))\nmy_formula = \"Sales ~ Income:Advertising + Price:Age + \" + columns_selected  \n\n# fitted model and summary\nlm_fit = smf.ols(my_formula, data=Carseats).fit()\nprint(lm_fit.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                  Sales   R-squared:                       0.876\nModel:                            OLS   Adj. R-squared:                  0.872\nMethod:                 Least Squares   F-statistic:                     210.0\nDate:                Sun, 25 Sep 2022   Prob (F-statistic):          6.14e-166\nTime:                        08:44:54   Log-Likelihood:                -564.67\nNo. Observations:                 400   AIC:                             1157.\nDf Residuals:                     386   BIC:                             1213.\nDf Model:                          13                                         \nCovariance Type:            nonrobust                                         \n=======================================================================================\n                          coef    std err          t      P&gt;|t|      [0.025      0.975]\n---------------------------------------------------------------------------------------\nIntercept               6.5756      1.009      6.519      0.000       4.592       8.559\nShelveLoc[T.Good]       4.8487      0.153     31.724      0.000       4.548       5.149\nShelveLoc[T.Medium]     1.9533      0.126     15.531      0.000       1.706       2.201\nUS[T.Yes]              -0.1576      0.149     -1.058      0.291      -0.450       0.135\nUrban[T.Yes]            0.1402      0.112      1.247      0.213      -0.081       0.361\nIncome:Advertising      0.0008      0.000      2.698      0.007       0.000       0.001\nPrice:Age               0.0001      0.000      0.801      0.424      -0.000       0.000\nAdvertising             0.0702      0.023      3.107      0.002       0.026       0.115\nAge                    -0.0579      0.016     -3.633      0.000      -0.089      -0.027\nCompPrice               0.0929      0.004     22.567      0.000       0.085       0.101\nEducation              -0.0209      0.020     -1.063      0.288      -0.059       0.018\nIncome                  0.0109      0.003      4.183      0.000       0.006       0.016\nPopulation              0.0002      0.000      0.433      0.665      -0.001       0.001\nPrice                  -0.1008      0.007    -13.549      0.000      -0.115      -0.086\n==============================================================================\nOmnibus:                        1.281   Durbin-Watson:                   2.047\nProb(Omnibus):                  0.527   Jarque-Bera (JB):                1.147\nSkew:                           0.129   Prob(JB):                        0.564\nKurtosis:                       3.050   Cond. No.                     1.31e+05\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 1.31e+05. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n\n\nPython has created a ShelveLoc[T.Good] dummy variable that takes on a value of 1 if the shelving location is good, and 0 otherwise. It has also created a ShelveLoc[T.Medium] dummy variable that equals 1 if the shelving location is medium, and 0 otherwise. A bad shelving location corresponds to a zero for each of the two dummy variables. The fact that the coefficient for ShelveLoc[T.Good] in the regression output is positive indicates that a good shelving location is associated with high sales (relative to a bad location). And ShelveLoc[T.Medium] has a smaller positive coefficient, indicating that a medium shelving location is associated with higher sales than a bad shelving location but lower sales than a good shelving location.\nAlso check pd.get_dummies."
  },
  {
    "objectID": "Chapter_3_Lab.html#the-sklearn-is-another-popular-way-for-performing-ols-in-python",
    "href": "Chapter_3_Lab.html#the-sklearn-is-another-popular-way-for-performing-ols-in-python",
    "title": "3  Regression",
    "section": "3.6 The sklearn is another popular way for performing OLS in Python",
    "text": "3.6 The sklearn is another popular way for performing OLS in Python\nCheck sklearn https://scikit-learn.org/stable/modules/linear_model.html.\n\n# ols model with intercept\nols_sl = linear_model.LinearRegression(fit_intercept=True) \n\n# fitted ols model (.values.reshape(-1, 1) is required for single predictor?)\nx_train = Boston.lstat.values.reshape(-1, 1)\ny_true =  Boston.medv\nols_sl.fit(x_train, y_true)\n\ny_pred = ols_sl.predict(x_train)\n# summary\nols_sl.intercept_, ols_sl.coef_\n\n(34.5538408793831, array([-0.95004935]))\n\n\n\nresidual = y_true - y_pred\n\n\nax = Boston.plot.scatter(x='lstat', y='medv', figsize=(8, 8))\nax.plot(Boston.lstat, y_pred)\nfor x, yactual, yfitted in zip(Boston.lstat, Boston.medv, y_pred): \n    ax.plot((x, x), (yactual, yfitted), '--', color='C1')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nols_sl_summary = {'R2': r2_score(y_true, y_pred), \n                  'Ex. Var': explained_variance_score(y_true, y_pred), \n                  'MSE': mean_squared_error(y_true, y_pred)}\n\nfor k, v in ols_sl_summary.items():\n    print(k, ':', v)\n\nR2 : 0.5441462975864797\nEx. Var : 0.5441462975864798\nMSE : 38.48296722989415\n\n\n\n# out-of-sample predictions\nols_sl.predict(np.array([5, 10, 15]).reshape(-1, 1))\n\narray([29.80359411, 25.05334734, 20.30310057])\n\n\n\n3.6.1 Optional - Visualizer for sklearn\nSklearn do not come with statistical visulizer like seaborn but you can use yellowbrick\n\n!pip install -U yellowbrick #besure to upgrade your yellowbrick to above 1.0\n\nLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\nRequirement already satisfied: yellowbrick in /usr/local/lib/python3.7/dist-packages (1.5)\nRequirement already satisfied: matplotlib!=3.0.0,&gt;=2.0.2 in /usr/local/lib/python3.7/dist-packages (from yellowbrick) (3.2.2)\nRequirement already satisfied: scikit-learn&gt;=1.0.0 in /usr/local/lib/python3.7/dist-packages (from yellowbrick) (1.0.2)\nRequirement already satisfied: numpy&gt;=1.16.0 in /usr/local/lib/python3.7/dist-packages (from yellowbrick) (1.21.6)\nRequirement already satisfied: cycler&gt;=0.10.0 in /usr/local/lib/python3.7/dist-packages (from yellowbrick) (0.11.0)\nRequirement already satisfied: scipy&gt;=1.0.0 in /usr/local/lib/python3.7/dist-packages (from yellowbrick) (1.7.3)\nRequirement already satisfied: python-dateutil&gt;=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,&gt;=2.0.2-&gt;yellowbrick) (2.8.2)\nRequirement already satisfied: kiwisolver&gt;=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,&gt;=2.0.2-&gt;yellowbrick) (1.4.4)\nRequirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,&gt;=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,&gt;=2.0.2-&gt;yellowbrick) (3.0.9)\nRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver&gt;=1.0.1-&gt;matplotlib!=3.0.0,&gt;=2.0.2-&gt;yellowbrick) (4.1.1)\nRequirement already satisfied: six&gt;=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil&gt;=2.1-&gt;matplotlib!=3.0.0,&gt;=2.0.2-&gt;yellowbrick) (1.15.0)\nRequirement already satisfied: threadpoolctl&gt;=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn&gt;=1.0.0-&gt;yellowbrick) (3.1.0)\nRequirement already satisfied: joblib&gt;=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn&gt;=1.0.0-&gt;yellowbrick) (1.1.0)\n\n\n\nfrom yellowbrick.regressor import PredictionError, ResidualsPlot, CooksDistance\n\n\nmodel = linear_model.LinearRegression(fit_intercept=True)\nvisualizer = PredictionError(model)\nvisualizer.fit(x_train, y_true)  # Fit the training data to the visualizer\nvisualizer.score(x_train, y_true)\nvisualizer.show()\n\n\n\n\n&lt;matplotlib.axes._subplots.AxesSubplot at 0x7f67eda44fd0&gt;\n\n\n\nvisualizer = ResidualsPlot(model, is_fitted=True)\nvisualizer.score(x_train, y_true)  # Evaluate the model on the test data\nvisualizer.show()                 # Finalize and render the figure\n\n\n\n\n&lt;matplotlib.axes._subplots.AxesSubplot at 0x7f67eda1d090&gt;\n\n\nHistogram can be replaced with a Q-Q plot, which is a common way to check that residuals are normally distributed. If the residuals are normally distributed, then their quantiles when plotted against quantiles of normal distribution should form a straight line.\n\nplt.figure(figsize=(12,8)) \nvisualizer = ResidualsPlot(model, hist=False, qqplot=True, is_fitted=True)\nvisualizer.score(x_train, y_true)  # Evaluate the model on the test data\nvisualizer.show() \n\n\n\n\n&lt;matplotlib.axes._subplots.AxesSubplot at 0x7f67ed7a9f10&gt;\n\n\n\nvisualizer = CooksDistance()\nvisualizer.fit(x_train, y_true)\nvisualizer.show()\n\n\n\n\n&lt;matplotlib.axes._subplots.AxesSubplot at 0x7f67ed790a50&gt;"
  },
  {
    "objectID": "Chapter_4_Lab.html#the-stock-market-data",
    "href": "Chapter_4_Lab.html#the-stock-market-data",
    "title": "4  Classification",
    "section": "4.1 The Stock Market Data",
    "text": "4.1 The Stock Market Data\nWe will begin by examining some numerical and graphical summaries of the Smarket data, which is part of the ISLR2 library. This data set consists of percentage returns for the S&P 500 stock index over \\(1,250\\)~days, from the beginning of 2001 until the end of 2005. For each date, we have recorded the percentage returns for each of the five previous trading days, lagone through lagfive. We have also recorded volume (the number of shares traded on the previous day, in billions), Today (the percentage return on the date in question) and direction (whether the market was Up or Down on this date). Our goal is to predict direction (a qualitative response) using the other features.\n\nimport pandas as pd\nimport numpy as np\n\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\n\nfrom sklearn.preprocessing import StandardScaler, scale\nfrom sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay, classification_report,\\\n  roc_curve, auc, RocCurveDisplay\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\nplt.style.use('seaborn-white')\nsns.set_context(\"notebook\", font_scale=1.5, rc={\"lines.linewidth\": 2.5})\n\n\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\nMounted at /content/drive\n\n\n\nSmarket = pd.read_csv('/content/drive/MyDrive/Lab/Data/Smarket.csv')\nprint(Smarket.shape)\nSmarket.head()\n\n(1250, 9)\n\n\n\n  \n    \n      \n\n\n\n\n\n\nYear\nLag1\nLag2\nLag3\nLag4\nLag5\nVolume\nToday\nDirection\n\n\n\n\n0\n2001\n0.381\n-0.192\n-2.624\n-1.055\n5.010\n1.1913\n0.959\nUp\n\n\n1\n2001\n0.959\n0.381\n-0.192\n-2.624\n-1.055\n1.2965\n1.032\nUp\n\n\n2\n2001\n1.032\n0.959\n0.381\n-0.192\n-2.624\n1.4112\n-0.623\nDown\n\n\n3\n2001\n-0.623\n1.032\n0.959\n0.381\n-0.192\n1.2760\n0.614\nUp\n\n\n4\n2001\n0.614\n-0.623\n1.032\n0.959\n0.381\n1.2057\n0.213\nUp\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\nSmarket.columns\n\nIndex(['Year', 'Lag1', 'Lag2', 'Lag3', 'Lag4', 'Lag5', 'Volume', 'Today',\n       'Direction'],\n      dtype='object')\n\n\n\nSmarket.describe(include='all')\n\n\n  \n    \n      \n\n\n\n\n\n\nYear\nLag1\nLag2\nLag3\nLag4\nLag5\nVolume\nToday\nDirection\n\n\n\n\ncount\n1250.000000\n1250.000000\n1250.000000\n1250.000000\n1250.000000\n1250.00000\n1250.000000\n1250.000000\n1250\n\n\nunique\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n2\n\n\ntop\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nUp\n\n\nfreq\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n648\n\n\nmean\n2003.016000\n0.003834\n0.003919\n0.001716\n0.001636\n0.00561\n1.478305\n0.003138\nNaN\n\n\nstd\n1.409018\n1.136299\n1.136280\n1.138703\n1.138774\n1.14755\n0.360357\n1.136334\nNaN\n\n\nmin\n2001.000000\n-4.922000\n-4.922000\n-4.922000\n-4.922000\n-4.92200\n0.356070\n-4.922000\nNaN\n\n\n25%\n2002.000000\n-0.639500\n-0.639500\n-0.640000\n-0.640000\n-0.64000\n1.257400\n-0.639500\nNaN\n\n\n50%\n2003.000000\n0.039000\n0.039000\n0.038500\n0.038500\n0.03850\n1.422950\n0.038500\nNaN\n\n\n75%\n2004.000000\n0.596750\n0.596750\n0.596750\n0.596750\n0.59700\n1.641675\n0.596750\nNaN\n\n\nmax\n2005.000000\n5.733000\n5.733000\n5.733000\n5.733000\n5.73300\n3.152470\n5.733000\nNaN\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\nSmarket.info(memory_usage='deep')\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1250 entries, 0 to 1249\nData columns (total 9 columns):\n #   Column     Non-Null Count  Dtype  \n---  ------     --------------  -----  \n 0   Year       1250 non-null   int64  \n 1   Lag1       1250 non-null   float64\n 2   Lag2       1250 non-null   float64\n 3   Lag3       1250 non-null   float64\n 4   Lag4       1250 non-null   float64\n 5   Lag5       1250 non-null   float64\n 6   Volume     1250 non-null   float64\n 7   Today      1250 non-null   float64\n 8   Direction  1250 non-null   object \ndtypes: float64(7), int64(1), object(1)\nmemory usage: 151.4 KB\n\n\n\nsns.pairplot(data=Smarket, hue='Direction', diag_kind='hist')\n\n\n\n\nThe corr() function produces a matrix that contains all of the pairwise correlations among the predictors in a data set.\n\n#produces a correlation matrix for all the numrical columns \ncorr = Smarket.corr()\ncorr\n\n\n  \n    \n      \n\n\n\n\n\n\nYear\nLag1\nLag2\nLag3\nLag4\nLag5\nVolume\nToday\n\n\n\n\nYear\n1.000000\n0.029700\n0.030596\n0.033195\n0.035689\n0.029788\n0.539006\n0.030095\n\n\nLag1\n0.029700\n1.000000\n-0.026294\n-0.010803\n-0.002986\n-0.005675\n0.040910\n-0.026155\n\n\nLag2\n0.030596\n-0.026294\n1.000000\n-0.025897\n-0.010854\n-0.003558\n-0.043383\n-0.010250\n\n\nLag3\n0.033195\n-0.010803\n-0.025897\n1.000000\n-0.024051\n-0.018808\n-0.041824\n-0.002448\n\n\nLag4\n0.035689\n-0.002986\n-0.010854\n-0.024051\n1.000000\n-0.027084\n-0.048414\n-0.006900\n\n\nLag5\n0.029788\n-0.005675\n-0.003558\n-0.018808\n-0.027084\n1.000000\n-0.022002\n-0.034860\n\n\nVolume\n0.539006\n0.040910\n-0.043383\n-0.041824\n-0.048414\n-0.022002\n1.000000\n0.014592\n\n\nToday\n0.030095\n-0.026155\n-0.010250\n-0.002448\n-0.006900\n-0.034860\n0.014592\n1.000000\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n# the best way to visualize corerelations matrices is heatmap\nplt.figure(figsize = (8,8))\nsns.heatmap(corr)\n\n&lt;matplotlib.axes._subplots.AxesSubplot at 0x7fe61ba50910&gt;\n\n\n\n\n\nAs one would expect, the correlations between the lag variables and today’s returns are close to zero. In other words, there appears to be little correlation between today’s returns and previous days’ returns. The only substantial correlation is between Year and volume. By plotting the data, which is ordered chronologically, we see that volume is increasing over time. In other words, the average number of shares traded daily increased from 2001 to 2005.\n\nprint(corr.iloc[:,-2:])\n\n          Volume     Today\nYear    0.539006  0.030095\nLag1    0.040910 -0.026155\nLag2   -0.043383 -0.010250\nLag3   -0.041824 -0.002448\nLag4   -0.048414 -0.006900\nLag5   -0.022002 -0.034860\nVolume  1.000000  0.014592\nToday   0.014592  1.000000\n\n\n\n# we choose the x axis as index, chossing year will give a discrete plot\nplt.figure(figsize = (12,6))\nsns.scatterplot(x=Smarket.index, y=\"Volume\", data=Smarket)\n\n&lt;matplotlib.axes._subplots.AxesSubplot at 0x7fe61af38a50&gt;"
  },
  {
    "objectID": "Chapter_4_Lab.html#logistic-regression",
    "href": "Chapter_4_Lab.html#logistic-regression",
    "title": "4  Classification",
    "section": "4.2 Logistic Regression",
    "text": "4.2 Logistic Regression\nNext, we will fit a logistic regression model in order to predict direction using lagone through lagfive and volume. The glm() function can be used to fit many types of generalized linear models , including logistic regression. The syntax of the glm() function is similar to that of lm(), except that we must pass in the argument families.Binomial in order to tell Python to run a logistic regression rather than some other type of generalized linear model.\n\npd.concat([pd.get_dummies(Smarket.Direction), Smarket.Direction], axis=1)\n\n\n  \n    \n      \n\n\n\n\n\n\nDown\nUp\nDirection\n\n\n\n\n0\n0\n1\nUp\n\n\n1\n0\n1\nUp\n\n\n2\n1\n0\nDown\n\n\n3\n0\n1\nUp\n\n\n4\n0\n1\nUp\n\n\n...\n...\n...\n...\n\n\n1245\n0\n1\nUp\n\n\n1246\n1\n0\nDown\n\n\n1247\n0\n1\nUp\n\n\n1248\n1\n0\nDown\n\n\n1249\n1\n0\nDown\n\n\n\n\n\n1250 rows × 3 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nWe choose the encoding to treat Up as 1 and Down as 0\n\nSmarket['Direction2'] = pd.get_dummies(Smarket.Direction, drop_first=True)\nSmarket.head()\n\n\n  \n    \n      \n\n\n\n\n\n\nYear\nLag1\nLag2\nLag3\nLag4\nLag5\nVolume\nToday\nDirection\nDirection2\n\n\n\n\n0\n2001\n0.381\n-0.192\n-2.624\n-1.055\n5.010\n1.1913\n0.959\nUp\n1\n\n\n1\n2001\n0.959\n0.381\n-0.192\n-2.624\n-1.055\n1.2965\n1.032\nUp\n1\n\n\n2\n2001\n1.032\n0.959\n0.381\n-0.192\n-2.624\n1.4112\n-0.623\nDown\n0\n\n\n3\n2001\n-0.623\n1.032\n0.959\n0.381\n-0.192\n1.2760\n0.614\nUp\n1\n\n\n4\n2001\n0.614\n-0.623\n1.032\n0.959\n0.381\n1.2057\n0.213\nUp\n1\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\nformula = 'Direction2 ~ Lag1+Lag2+Lag3+Lag4+Lag5+Volume'\nmodel = smf.glm(formula=formula, data=Smarket, family=sm.families.Binomial())\nresult = model.fit()\nprint(result.summary())\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:             Direction2   No. Observations:                 1250\nModel:                            GLM   Df Residuals:                     1243\nModel Family:                Binomial   Df Model:                            6\nLink Function:                  logit   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -863.79\nDate:                Sun, 30 Oct 2022   Deviance:                       1727.6\nTime:                        02:24:34   Pearson chi2:                 1.25e+03\nNo. Iterations:                     4                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     -0.1260      0.241     -0.523      0.601      -0.598       0.346\nLag1          -0.0731      0.050     -1.457      0.145      -0.171       0.025\nLag2          -0.0423      0.050     -0.845      0.398      -0.140       0.056\nLag3           0.0111      0.050      0.222      0.824      -0.087       0.109\nLag4           0.0094      0.050      0.187      0.851      -0.089       0.107\nLag5           0.0103      0.050      0.208      0.835      -0.087       0.107\nVolume         0.1354      0.158      0.855      0.392      -0.175       0.446\n==============================================================================\n\n\nThe smallest \\(p\\)-value here is associated with lagone. The negative coefficient for this predictor suggests that if the market had a positive return yesterday, then it is less likely to go up today. However, at a value of \\(0.15\\), the \\(p\\)-value is still relatively large, and so there is no clear evidence of a real association between lagone and direction.\nWe use the params function in order to access just the coefficients for this fitted model. https://www.statsmodels.org/dev/generated/statsmodels.genmod.generalized_linear_model.GLMResults.html.\n\nprint(\"Coeffieients\")\nprint(result.params)\nprint(\"p-Values\")\nprint(result.pvalues)\nprint(\"Dependent variables\")\nprint(result.model.endog_names)\n\nCoeffieients\nIntercept   -0.126000\nLag1        -0.073074\nLag2        -0.042301\nLag3         0.011085\nLag4         0.009359\nLag5         0.010313\nVolume       0.135441\ndtype: float64\np-Values\nIntercept    0.600700\nLag1         0.145232\nLag2         0.398352\nLag3         0.824334\nLag4         0.851445\nLag5         0.834998\nVolume       0.392404\ndtype: float64\nDependent variables\nDirection2\n\n\nThe predict() function can be used to predict the probability that the market will go up, given values of the predictors. If no data set is supplied to the predict() function, then the probabilities are computed for the training data that was used to fit the logistic regression model. Here we have printed only the first ten probabilities. We know that these values correspond to the probability of the market going up, rather than down, because we have set the dummy variable with a 1 for Up.\n\npredictions = result.predict()\nprint(predictions[:10])\n\n[0.50708413 0.48146788 0.48113883 0.51522236 0.51078116 0.50695646\n 0.49265087 0.50922916 0.51761353 0.48883778]\n\n\nIn order to make a prediction as to whether the market will go up or down on a particular day, we must convert these predicted probabilities into class labels, Up or Down. The following command create a vector of class predictions based on whether the predicted probability of a market increase is greater than or less than \\(0.5\\).\n\npredictions_nominal = [ \"Up\" if x &gt; 0.5 else \"Down\" for x in predictions]\n\nGiven these predictions, the confusion_matrix() function can be used to produce a confusion matrix in order to determine how many observations were correctly or incorrectly classified.\n\ncm = confusion_matrix(Smarket.Direction, predictions_nominal)\ncm\n\narray([[145, 457],\n       [141, 507]])\n\n\n\nConfusionMatrixDisplay(cm, display_labels=[0,1]).plot()\n\n&lt;sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7fe619662690&gt;\n\n\n\n\n\n\nprint(classification_report(Smarket.Direction, predictions_nominal, digits=3))\n\n              precision    recall  f1-score   support\n\n        Down      0.507     0.241     0.327       602\n          Up      0.526     0.782     0.629       648\n\n    accuracy                          0.522      1250\n   macro avg      0.516     0.512     0.478      1250\nweighted avg      0.517     0.522     0.483      1250\n\n\n\nThe diagonal elements of the confusion matrix indicate correct predictions, while the off-diagonals represent incorrect predictions. Hence our model correctly predicted that the market would go up on \\(507\\) days and that it would go down on \\(145\\) days, for a total of \\(507+145 = 652\\) correct predictions. The classification_report() function can be used to compute the fraction of days for which the prediction was correct. In this case, logistic regression correctly predicted the movement of the market \\(52.2\\),% of the time.\nAt first glance, it appears that the logistic regression model is working a little better than random guessing. However, this result is misleading because we trained and tested the model on the same set of \\(1,250\\) observations. In other words, \\(100\\%-52.2\\%=47.8\\%\\), is the training error rate. As we have seen previously, the training error rate is often overly optimistic—it tends to underestimate the test error rate. In order to better assess the accuracy of the logistic regression model in this setting, we can fit the model using part of the data, and then examine how well it predicts the held out data. This will yield a more realistic error rate, in the sense that in practice we will be interested in our model’s performance not on the data that we used to fit the model, but rather on days in the future for which the market’s movements are unknown.\nTo implement this strategy, we will first create a vector corresponding to the observations from 2001 through 2004. We will then use this vector to create a held out data set of observations from 2005.\n\ntrain_filter = Smarket.Year &lt; 2005\n# train_filter.value_counts()\nX_test = Smarket.loc[~train_filter,]\ny_test = Smarket.loc[~train_filter, 'Direction2']\ny_test.shape\n\n(252,)\n\n\nThe data contains \\(1{,}250\\) elements, corresponding to the observations in our data set. The output above indicates that there are 252 such observations in testset.\nWe now fit a logistic regression model using only the subset of the observations that correspond to dates before 2005. We then obtain predicted probabilities of the stock market going up for each of the days in our test set—that is, for the days in 2005.\n\nmodel = smf.glm(formula=formula, data=Smarket, subset=train_filter, family=sm.families.Binomial())\nresult = model.fit()\n\nNotice that we have trained and tested our model on two completely separate data sets: training was performed using only the dates before 2005, and testing was performed using only the dates in 2005. Finally, we compute the predictions for 2005 and compare them to the actual movements of the market over that time period.\n\npredictions = result.predict(X_test)\npredictions_nominal = [ \"Up\" if x &gt; 0.5 else \"Down\" for x in predictions]\nprint(classification_report(Smarket.loc[~train_filter, 'Direction'], predictions_nominal, digits=3))\n\n              precision    recall  f1-score   support\n\n        Down      0.443     0.694     0.540       111\n          Up      0.564     0.312     0.402       141\n\n    accuracy                          0.480       252\n   macro avg      0.503     0.503     0.471       252\nweighted avg      0.511     0.480     0.463       252\n\n\n\nThe results are rather disappointing: the test error rate is \\(52\\),%, which is worse than random guessing! Of course this result is not all that surprising, given that one would not generally expect to be able to use previous days’ returns to predict future market performance. (After all, if it were possible to do so, then the authors of this book would be out striking it rich rather than writing a statistics textbook.)\nWe recall that the logistic regression model had very underwhelming \\(p\\)-values associated with all of the predictors, and that the smallest \\(p\\)-value, though not very small, corresponded to lagone. Perhaps by removing the variables that appear not to be helpful in predicting direction, we can obtain a more effective model. After all, using predictors that have no relationship with the response tends to cause a deterioration in the test error rate (since such predictors cause an increase in variance without a corresponding decrease in bias), and so removing such predictors may in turn yield an improvement.\nBelow we have refit the logistic regression using just lagone and lagtwo, which seemed to have the highest predictive power in the original logistic regression model.\n\nlogreg_stats = smf.glm(formula = 'Direction2 ~ Lag1+Lag2', data=Smarket, subset=train_filter, \n                       family=sm.families.Binomial()).fit()\nprint(logreg_stats.summary())\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:             Direction2   No. Observations:                  998\nModel:                            GLM   Df Residuals:                      995\nModel Family:                Binomial   Df Model:                            2\nLink Function:                  logit   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -690.70\nDate:                Sun, 30 Oct 2022   Deviance:                       1381.4\nTime:                        02:25:08   Pearson chi2:                     998.\nNo. Iterations:                     4                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      0.0322      0.063      0.508      0.611      -0.092       0.156\nLag1          -0.0556      0.052     -1.076      0.282      -0.157       0.046\nLag2          -0.0445      0.052     -0.861      0.389      -0.146       0.057\n==============================================================================\n\n\n\nX_test = Smarket.loc[~train_filter, ['Lag1', 'Lag2']]\ny_test_pred = logreg_stats.predict(X_test)\ny_test_pred_class = [1 if prob &gt; 0.5 else 0 for prob in y_test_pred]\nconf_mat = confusion_matrix(y_test, y_test_pred_class)\nprint(conf_mat)\n\n[[ 35  76]\n [ 35 106]]\n\n\n\nlogreg_stats.p\n\n\nTP = conf_mat[1,1]\nTN = conf_mat[0,0]\nFP = conf_mat[0,1]\nFN = conf_mat[1,0]\n\n# print(\"Accuracy: \", conf_mat.diagonal().sum()/conf_mat.sum() )\nprint(\"Accuracy: \", (TP+TN) / (TP+TN+FP+FN) )\nprint(\"Sensitivity: \",  TP / (FN + TP) )\nprint(\"Precision: \",  TP / (FP + TP) )\nprint(\"False Positive Rate: \",  FP / (FP + TN) )\n\nAccuracy:  0.5595238095238095\nSensitivity:  0.75177304964539\nPrecision:  0.5824175824175825\nFalse Positive Rate:  0.6846846846846847\n\n\nYou can find many useful metrics in the sklearn module https://scikit-learn.org/stable/modules/model_evaluation.html#classification-metrics.\n\nfpr, tpr, thresholds = roc_curve(y_test, y_test_pred_class)\nprint(fpr, tpr)\nroc_auc = auc(fpr, tpr)\nRocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc, estimator_name='example estimator').plot()\n\n[0.         0.68468468 1.        ] [0.         0.75177305 1.        ]\n\n\n&lt;sklearn.metrics._plot.roc_curve.RocCurveDisplay at 0x7fe618521450&gt;\n\n\n\n\n\nNow the results appear to be a little better: \\(56\\%\\) of the daily movements have been correctly predicted. It is worth noting that in this case, a much simpler strategy of predicting that the market will increase every day will also be correct \\(56\\%\\) of the time! Hence, in terms of overall error rate, the logistic regression method is no better than the naive approach. However, the confusion matrix shows that on days when logistic regression predicts an increase in the market, it has a \\(58\\%\\) accuracy rate. This suggests a possible trading strategy of buying on days when the model predicts an increasing market, and avoiding trades on days when a decrease is predicted. Of course one would need to investigate more carefully whether this small improvement was real or just due to random chance.\nSuppose that we want to predict the returns associated with particular values of lagone and lagtwo. In particular, we want to predict direction on a day when lagone and lagtwo equal 1.2 and~1.1, respectively, and on a day when they equal 1.5 and $-$0.8. We do this using the predict() function.\n\nUp_Prob = logreg_stats.predict(pd.DataFrame([[1.2, 1.1], [1.5, -0.8]], \n                                  columns=[\"Lag1\", \"Lag2\"]))   # 0 is the 'Up' , 1 is the 'Down'\nprint(\"Up probabilities for the 2 samples are %s and %s\" % (Up_Prob[0], \n                                                         Up_Prob[1]))\n\nUp probabilities for the 2 samples are 0.47914623911039295 and 0.4960938729355764"
  },
  {
    "objectID": "Chapter_4_Lab.html#linear-discriminant-analysis",
    "href": "Chapter_4_Lab.html#linear-discriminant-analysis",
    "title": "4  Classification",
    "section": "4.3 Linear Discriminant Analysis",
    "text": "4.3 Linear Discriminant Analysis\nNow we will perform LDA on the Smarket data. In Python, we fit an LDA model using the LDA() function, which is part of the sklearn library. We fit the model using only the observations before 2005.\n\nX_train = Smarket.loc[train_filter, ['Lag1','Lag2']]\ny_train = Smarket.loc[train_filter, 'Direction2']\n\nX_test = Smarket.loc[~train_filter, ['Lag1','Lag2']]\ny_test = Smarket.loc[~train_filter, 'Direction2']\n\nlda = LDA()\nmodel = lda.fit(X_train, y_train)\n\n\nprint('Prior Probs are - ',lda.priors_)\nprint('Class Means are  - ',lda.means_)\nprint('Coeff are - ', lda.scalings_) # coefficients of ld\n\nPrior Probs are -  [0.49198397 0.50801603]\nClass Means are  -  [[ 0.04279022  0.03389409]\n [-0.03954635 -0.03132544]]\nCoeff are -  [[-0.64201904]\n [-0.51352928]]\n\n\n\n# for plotting lda\nld_sc = X_train.iloc[:, 0] * lda.scalings_[0] + X_train.iloc[:, 1] * lda.scalings_[1]\n\nld = pd.DataFrame({'groups': y_train, 'ld': ld_sc})\n\ng = sns.FacetGrid(ld, col='groups')\ng.map(plt.hist, 'ld')\nax1, ax2 = g.axes[0]\nax1.set_xlabel(\"Down\")\nax2.set_xlabel(\"Up\")\n\nText(0.5, 15.439999999999998, 'Up')\n\n\n\n\n\nThe LDA output indicates that \\(\\hat\\pi_1=0.492\\) and \\(\\hat\\pi_2=0.508\\); in other words, \\(49.2\\),% of the training observations correspond to days during which the market went down. It also provides the group means; these are the average of each predictor within each class, and are used by LDA as estimates of \\(\\mu_k\\). These suggest that there is a tendency for the previous 2~days’ returns to be negative on days when the market increases, and a tendency for the previous days’ returns to be positive on days when the market declines.\nThe coefficients of linear discriminants output provides the linear combination of lagone and lagtwo that are used to form the LDA decision rule. In other words, these are the multipliers of the elements of \\(X=x\\) in (4.24). If \\(-0.642\\times `lagone` - 0.514 \\times `lagtwo`\\) is large, then the LDA classifier will predict a market increase, and if it is small, then the LDA classifier will predict a market decline.\nThe above plot produces plots of the linear discriminants, obtained by computing \\(-0.642\\times `lagone` - 0.514 \\times `lagtwo`\\) for each of the training observations. The Up and Down observations are displayed separately.\nThe predict() function returns LDA’s predictions about the movement of the market. The predict_proba returns a matrix whose \\(k\\)th column contains the posterior probability that the corresponding observation belongs to the \\(k\\)th class, computed from (4.15).\n\npred = lda.predict(X_test)\npost = lda.predict_proba(X_test)\n\nThe first element, pred, contains LDA’s predictions about the movement of the market. The second element, post, is a matrix whose \\(k\\)th column contains the posterior probability that the corresponding observation belongs to the \\(k\\)th class, computed from (4.15).\nAs we observed in Section 4.5, the LDA and logistic regression predictions are almost identical.\n\ncm = confusion_matrix(y_test,pred)\nprint(cm)\n\n[[ 35  76]\n [ 35 106]]\n\n\nApplying a \\(50\\),% threshold to the posterior probabilities allows us to recreate the predictions contained in pred.\n\nnp.unique(post[:,0]&gt;0.5, return_counts=True)\n\n(array([False,  True]), array([182,  70]))\n\n\nNotice that the posterior probability output by the model corresponds to the probability that the market will increase:\n\npost[:20, 0], pred[:20]\n\n(array([0.49017925, 0.4792185 , 0.46681848, 0.47400107, 0.49278766,\n        0.49385615, 0.49510156, 0.4872861 , 0.49070135, 0.48440262,\n        0.49069628, 0.51199885, 0.48951523, 0.47067612, 0.47445929,\n        0.47995834, 0.49357753, 0.50308938, 0.49788061, 0.48863309]),\n array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1],\n       dtype=uint8))\n\n\nIf we wanted to use a posterior probability threshold other than \\(50\\),% in order to make predictions, then we could easily do so. For instance, suppose that we wish to predict a market decrease only if we are very certain that the market will indeed decrease on that day—say, if the posterior probability is at least \\(90\\),%.\n\nnp.unique(post[:,0]&gt;0.9, return_counts=True)\n\n(array([False]), array([252]))\n\n\nNo days in 2005 meet that threshold! In fact, the greatest posterior probability of decrease in all of 2005 was \\(52.02\\),%."
  },
  {
    "objectID": "Chapter_4_Lab.html#quadratic-discriminant-analysis",
    "href": "Chapter_4_Lab.html#quadratic-discriminant-analysis",
    "title": "4  Classification",
    "section": "4.4 Quadratic Discriminant Analysis",
    "text": "4.4 Quadratic Discriminant Analysis\nWe will now fit a QDA model to the Smarket data.\n\nqda = QDA()\nqda.fit(X_train,y_train)\n\nQuadraticDiscriminantAnalysis()\n\n\n\nprint('Mean for class 0 is - ',qda.means_[0])\nprint('Mean for class 1 is - ',qda.means_[1])\nprint('Prior probalbilities - ',qda.priors_)\n\nMean for class 0 is -  [0.04279022 0.03389409]\nMean for class 1 is -  [-0.03954635 -0.03132544]\nPrior probalbilities -  [0.49198397 0.50801603]\n\n\nThe output contains the group means. But it does not contain the coefficients of the linear discriminants, because the QDA classifier involves a quadratic, rather than a linear, function of the predictors. The predict() function works in exactly the same fashion as for LDA.\n\npred = qda.predict(X_test)\ncm = confusion_matrix(y_test,pred)\nprint(cm)\n\n[[ 30  81]\n [ 20 121]]\n\n\n\nprint('Accuracy using QDA is ',accuracy_score(y_test,pred))\n\nAccuracy using QDA is  0.5992063492063492\n\n\nInterestingly, the QDA predictions are accurate almost \\(60\\),% of the time, even though the 2005 data was not used to fit the model. This level of accuracy is quite impressive for stock market data, which is known to be quite hard to model accurately. This suggests that the quadratic form assumed by QDA may capture the true relationship more accurately than the linear forms assumed by LDA and logistic regression.\nHowever, we recommend evaluating this method’s performance on a larger test set before betting that this approach will consistently beat the market!"
  },
  {
    "objectID": "Chapter_4_Lab.html#naive-bayes",
    "href": "Chapter_4_Lab.html#naive-bayes",
    "title": "4  Classification",
    "section": "4.5 Naive Bayes",
    "text": "4.5 Naive Bayes\nNext, we fit a naive Bayes model to the Smarket data. By default, this implementation of the naive Bayes classifier models each quantitative feature using a Gaussian distribution. However, a kernel density method can also be used to estimate the distributions.\nMeaning of var_smoothing.\n\ngnb = GaussianNB(var_smoothing=0)\ny_pred = gnb.fit(X_train, y_train)\n\n\ngnb.class_prior_\n\narray([0.49198397, 0.50801603])\n\n\n\ngnb.theta_\n\narray([[ 0.04279022,  0.03389409],\n       [-0.03954635, -0.03132544]])\n\n\n\nnp.sqrt(gnb.sigma_)\n\n/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:103: FutureWarning: Attribute `sigma_` was deprecated in 1.0 and will be removed in1.2. Use `var_` instead.\n  warnings.warn(msg, category=FutureWarning)\n\n\narray([[1.22619505, 1.23792871],\n       [1.23045262, 1.21956089]])\n\n\nThe output contains the estimated mean and standard deviation for each variable in each class. For example, the mean for lagone is \\(0.0428\\) for Direction=Down, and the standard deviation is \\(1.226\\). We can easily verify this:\n\nprint(np.mean(X_train.loc[y_train==0]['Lag1']))\nprint(np.std(X_train.loc[y_train==0]['Lag1']))\n\n0.04279022403258655\n1.226195046492573\n\n\nNotice that R calulates the standard deviation with N - 1 as the denominator, and in Python as Numpy use N. https://stackoverflow.com/questions/20708455/different-results-for-standard-deviation-using-numpy-and-r.\nThe predict() function is straightforward.\n\npred = gnb.predict(X_test)\ncm = confusion_matrix(y_test,pred)\nprint(cm)\n\n[[ 29  82]\n [ 20 121]]\n\n\n\nprint('Accuracy using gnb is ',accuracy_score(y_test,pred))\n\nAccuracy using gnb is  0.5952380952380952\n\n\nNaive Bayes performs very well on this data, with accurate predictions over \\(59.5\\%\\) of the time. This is slightly worse than QDA, but much better than LDA.\nThe predict_prob() function can generate estimates of the probability that each observation belongs to a particular class.\n\npreds = gnb.predict_proba(Smarket.loc[Smarket.Year==2005,['Lag1', 'Lag2']])\npreds[:5]\n\narray([[0.4873288 , 0.5126712 ],\n       [0.47623584, 0.52376416],\n       [0.46529531, 0.53470469],\n       [0.47484469, 0.52515531],\n       [0.49020587, 0.50979413]])"
  },
  {
    "objectID": "Chapter_4_Lab.html#k-nearest-neighbors",
    "href": "Chapter_4_Lab.html#k-nearest-neighbors",
    "title": "4  Classification",
    "section": "4.6 \\(K\\)-Nearest Neighbors",
    "text": "4.6 \\(K\\)-Nearest Neighbors\nWe will now perform KNN using the knn() function.\n\nknn = KNeighborsClassifier(n_neighbors=1)\npred = knn.fit(X_train, y_train).predict(X_test)\n\n\nprint(confusion_matrix(y_test, pred))\nprint(classification_report(y_test, pred, digits=3))\n\n[[43 68]\n [58 83]]\n              precision    recall  f1-score   support\n\n           0      0.426     0.387     0.406       111\n           1      0.550     0.589     0.568       141\n\n    accuracy                          0.500       252\n   macro avg      0.488     0.488     0.487       252\nweighted avg      0.495     0.500     0.497       252\n\n\n\nThe results using \\(K=1\\) are not very good, since only \\(50\\),% of the observations are correctly predicted. Of course, it may be that \\(K=1\\) results in an overly flexible fit to the data. Below, we repeat the analysis using \\(K=3\\).\n\nknn = KNeighborsClassifier(n_neighbors=3)\npred = knn.fit(X_train, y_train).predict(X_test)\nprint(confusion_matrix(y_test, pred))\nprint(classification_report(y_test, pred, digits=3))\n\n[[48 63]\n [55 86]]\n              precision    recall  f1-score   support\n\n           0      0.466     0.432     0.449       111\n           1      0.577     0.610     0.593       141\n\n    accuracy                          0.532       252\n   macro avg      0.522     0.521     0.521       252\nweighted avg      0.528     0.532     0.529       252\n\n\n\nThe results have improved slightly. But increasing \\(K\\) further turns out to provide no further improvements. It appears that for this data, QDA provides the best results of the methods that we have examined so far.\nKNN does not perform well on the Smarket data but it does often provide impressive results. As an example we will apply the KNN approach to the Insurance data set, which is part of the ISLR2 library. This data set includes \\(85\\) predictors that measure demographic characteristics for 5,822 individuals. The response variable is Purchase, which indicates whether or not a given individual purchases a caravan insurance policy. In this data set, only \\(6\\),% of people purchased caravan insurance.\n\nCaravan = pd.read_csv('/content/drive/MyDrive/Lab/Data/Caravan.csv')\nprint(Caravan.shape)\nprint(Caravan.Purchase.value_counts())\nprint(Caravan.Purchase.value_counts()[1] / len(Caravan))\n# Only around 6% of the people purchased the insurance\nCaravan.tail()\n\n(5822, 86)\nNo     5474\nYes     348\nName: Purchase, dtype: int64\n0.05977327378907592\n\n\n\n  \n    \n      \n\n\n\n\n\n\nMOSTYPE\nMAANTHUI\nMGEMOMV\nMGEMLEEF\nMOSHOOFD\nMGODRK\nMGODPR\nMGODOV\nMGODGE\nMRELGE\n...\nAPERSONG\nAGEZONG\nAWAOREG\nABRAND\nAZEILPL\nAPLEZIER\nAFIETS\nAINBOED\nABYSTAND\nPurchase\n\n\n\n\n5817\n36\n1\n1\n2\n8\n0\n6\n1\n2\n1\n...\n0\n0\n0\n1\n0\n0\n0\n0\n0\nNo\n\n\n5818\n35\n1\n4\n4\n8\n1\n4\n1\n4\n6\n...\n0\n0\n0\n1\n0\n0\n0\n0\n0\nNo\n\n\n5819\n33\n1\n3\n4\n8\n0\n6\n0\n3\n5\n...\n0\n0\n0\n1\n0\n0\n0\n0\n0\nYes\n\n\n5820\n34\n1\n3\n2\n8\n0\n7\n0\n2\n7\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\nNo\n\n\n5821\n33\n1\n3\n3\n8\n0\n6\n1\n2\n7\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\nNo\n\n\n\n\n\n5 rows × 86 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nBecause the KNN classifier predicts the class of a given test observation by identifying the observations that are nearest to it, the scale of the variables matters. Variables that are on a large scale will have a much larger effect on the distance between the observations, and hence on the KNN classifier, than variables that are on a small scale. For instance, imagine a data set that contains two variables, salary and age (measured in dollars and years, respectively). As far as KNN is concerned, a difference of \\(1,000\\) in salary is enormous compared to a difference of \\(50\\) years in age. Consequently, salary will drive the KNN classification results, and age will have almost no effect. This is contrary to our intuition that a salary difference of \\(1,000\\) is quite small compared to an age difference of \\(50\\) years.\nFurthermore, the importance of scale to the KNN classifier leads to another issue: if we measured salary in Japanese yen, or if we measured age in minutes, then we’d get quite different classification results from what we get if these two variables are measured in dollars and years.\nA good way to handle this problem is to the data so that all variables are given a mean of zero and a standard deviation of one. Then all variables will be on a comparable scale. The scale() function does just this. In standardizing the data, we exclude column Purchase variable.\n\n# From a sample of means if the observations it can be seen that the variables falls in different ranges\n# We want all the variables to have a same range. \nCaravan.mean()[:5]\n\n/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n  This is separate from the ipykernel package so we can avoid doing imports until\n\n\nMOSTYPE     24.253349\nMAANTHUI     1.110615\nMGEMOMV      2.678805\nMGEMLEEF     2.991240\nMOSHOOFD     5.773617\ndtype: float64\n\n\n\ny = Caravan.Purchase\nX = Caravan.drop('Purchase', axis=1).astype('float64')\nX_scaled = scale(X)\nX_scaled.std(axis=0)\n\narray([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n\n\nNow every column has a standard deviation of one and a mean of zero.\nWe now split the observations into a test set, containing the first 1,000 observations, and a training set, containing the remaining observations. We fit a KNN model on the training data using \\(K=1\\), and evaluate its performance on the test data.\n\nX_train = X_scaled[1000:,:]\ny_train = y[1000:]\n\nX_test = X_scaled[:1000,:]\ny_test = y[:1000]\n\n\nknn = KNeighborsClassifier(n_neighbors=1)   # K=1\nknn.fit(X_train, y_train)\n\ny_pred = knn.predict(X_test)   # returns class label \nprint(\"Test set error rate: \", (y_test != y_pred).mean() )    # manual calculatin\n\nTest set error rate:  0.118\n\n\nThe KNN error rate on the 1,000 test observations is just under 12%. At first glance, this may appear to be fairly good. However, since only 6% of customers purchased insurance, we could get the error rate down to 6% by always predicting \\({\\tt No}\\) regardless of the values of the predictors!\nSuppose that there is some non-trivial cost to trying to sell insurance to a given individual. For instance, perhaps a salesperson must visit each potential customer. If the company tries to sell insurance to a random selection of customers, then the success rate will be only 6%, which may be far too low given the costs involved.\nInstead, the company would like to try to sell insurance only to customers who are likely to buy it. So the overall error rate is not of interest. Instead, the fraction of individuals that are correctly predicted to buy insurance is of interest.\nIt turns out that KNN with \\(K = 1\\) does far better than random guessing among the customers that are predicted to buy insurance.\n\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\ncm[1,1]/(cm[0,1]+cm[1,1])\n\n[[873  68]\n [ 50   9]]\n\n\n0.11688311688311688\n\n\nAmong \\(77\\) such customers, \\(9\\), or \\(11.7\\),%, actually do purchase insurance. This is double the rate that one would obtain from random guessing.\nUsing \\(K=3\\), the success rate increases to \\(20\\),%, and with \\(K=5\\) the rate is \\(26.7\\),%. This is over four times the rate that results from random guessing. It appears that KNN is finding some real patterns in a difficult data set!\n\nknn = KNeighborsClassifier(n_neighbors=3)   # K=1\nknn.fit(X_train, y_train)\n\ny_pred = knn.predict(X_test)   # returns class label \ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\nprint(cm[1,1]/(cm[0,1]+cm[1,1]))\n\nknn = KNeighborsClassifier(n_neighbors=5)   # K=1\nknn.fit(X_train, y_train)\n\ny_pred = knn.predict(X_test)   # returns class label \ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\ncm[1,1]/(cm[0,1]+cm[1,1])\n\n[[921  20]\n [ 54   5]]\n0.2\n[[930  11]\n [ 55   4]]\n\n\n0.26666666666666666\n\n\nHowever, while this strategy is cost-effective, it is worth noting that only 15 customers are predicted to purchase insurance using KNN with \\(K=5\\). In practice, the insurance company may wish to expend resources on convincing more than just 15 potential customers to buy insurance.\nAs a comparison, we can also fit a logistic regression model to the data. If we use \\(0.5\\) as the predicted probability cut-off for the classifier, then we have a problem: only seven of the test observations are predicted to purchase insurance. Even worse, we are wrong about all of these! However, we are not required to use a cut-off of \\(0.5\\). If we instead predict a purchase any time the predicted probability of purchase exceeds \\(0.25\\), we get much better results: we predict that 33 people will purchase insurance, and we are correct for about \\(33\\),% of these people. This is over five times better than random guessing!\n\nlog_reg = LogisticRegression(solver='newton-cg', C=1e9)\nlog_reg.fit(X_train, y_train)\n\ny_pred = log_reg.predict(X_test)  # returns class labels based on threshold=0.5\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\nTP = cm[1,1]\nTN = cm[0,0]\nFP = cm[0,1]\nFN = cm[1,0]\nprint(\"Accuracy: \", (TP+TN) / (TP+TN+FP+FN) )\nprint(\"Sensitivity(Recall): \",  TP / (FN + TP) )\nprint(\"Precision: \",  TP / (FP + TP) )\nprint(\"False Positive Rate: \",  FP / (FP + TN) )\n\n[[934   7]\n [ 59   0]]\nAccuracy:  0.934\nSensitivity(Recall):  0.0\nPrecision:  0.0\nFalse Positive Rate:  0.007438894792773645\n\n\n\ny_pred_prob = log_reg.predict_proba(X_test)   # returns prob of class labels\ny_pred_sensitized = ['Yes' if prob&gt;0.25 else 'No' for prob in y_pred_prob[:,1]]\n\n\ncm = confusion_matrix(y_test, y_pred_sensitized)\nprint(cm)\n\nTP = cm[1,1]\nTN = cm[0,0]\nFP = cm[0,1]\nFN = cm[1,0]\nprint(\"Accuracy: \", (TP+TN) / (TP+TN+FP+FN) )\nprint(\"Sensitivity(Recall): \",  TP / (FN + TP) )\nprint(\"Precision: \",  TP / (FP + TP) )\nprint(\"False Positive Rate: \",  FP / (FP + TN) )\n\n[[919  22]\n [ 48  11]]\nAccuracy:  0.93\nSensitivity(Recall):  0.1864406779661017\nPrecision:  0.3333333333333333\nFalse Positive Rate:  0.023379383634431455"
  },
  {
    "objectID": "Chapter_4_Lab.html#poisson-regression",
    "href": "Chapter_4_Lab.html#poisson-regression",
    "title": "4  Classification",
    "section": "4.7 Poisson Regression",
    "text": "4.7 Poisson Regression\nFinally, we fit a Poisson regression model to the Bikeshare data set, which measures the number of bike rentals (bikers) per hour in Washington, DC. The data can be found in the ISLR2 library.\n\nBikeshare = pd.read_csv('/content/drive/MyDrive/Lab/Data/Bikeshare.csv', index_col=0)\nBikeshare.index = Bikeshare.index - 1 \nprint(Bikeshare.shape)\nprint(Bikeshare.columns)\n# Only around 6% of the people purchased the insurance\nBikeshare.tail()\n\n(8645, 15)\nIndex(['season', 'mnth', 'day', 'hr', 'holiday', 'weekday', 'workingday',\n       'weathersit', 'temp', 'atemp', 'hum', 'windspeed', 'casual',\n       'registered', 'bikers'],\n      dtype='object')\n\n\n\n  \n    \n      \n\n\n\n\n\n\nseason\nmnth\nday\nhr\nholiday\nweekday\nworkingday\nweathersit\ntemp\natemp\nhum\nwindspeed\ncasual\nregistered\nbikers\n\n\n\n\n8640\n1\nDec\n365\n19\n0\n6\n0\nclear\n0.42\n0.4242\n0.54\n0.2239\n19\n73\n92\n\n\n8641\n1\nDec\n365\n20\n0\n6\n0\nclear\n0.42\n0.4242\n0.54\n0.2239\n8\n63\n71\n\n\n8642\n1\nDec\n365\n21\n0\n6\n0\nclear\n0.40\n0.4091\n0.58\n0.1940\n2\n50\n52\n\n\n8643\n1\nDec\n365\n22\n0\n6\n0\nclear\n0.38\n0.3939\n0.62\n0.1343\n2\n36\n38\n\n\n8644\n1\nDec\n365\n23\n0\n6\n0\nclear\n0.36\n0.3788\n0.66\n0.0000\n4\n27\n31\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nWe begin by fitting a least squares linear regression model to the data.\n\nBikeshare.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 8645 entries, 0 to 8644\nData columns (total 15 columns):\n #   Column      Non-Null Count  Dtype  \n---  ------      --------------  -----  \n 0   season      8645 non-null   int64  \n 1   mnth        8645 non-null   object \n 2   day         8645 non-null   int64  \n 3   hr          8645 non-null   int64  \n 4   holiday     8645 non-null   int64  \n 5   weekday     8645 non-null   int64  \n 6   workingday  8645 non-null   int64  \n 7   weathersit  8645 non-null   object \n 8   temp        8645 non-null   float64\n 9   atemp       8645 non-null   float64\n 10  hum         8645 non-null   float64\n 11  windspeed   8645 non-null   float64\n 12  casual      8645 non-null   int64  \n 13  registered  8645 non-null   int64  \n 14  bikers      8645 non-null   int64  \ndtypes: float64(4), int64(9), object(2)\nmemory usage: 1.1+ MB\n\n\n\nBikeshare['hr'] = Bikeshare['hr'].astype('category')\nBikeshare['mnth'] = Bikeshare['mnth'].astype('category')\nBikeshare.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 8645 entries, 0 to 8644\nData columns (total 15 columns):\n #   Column      Non-Null Count  Dtype   \n---  ------      --------------  -----   \n 0   season      8645 non-null   int64   \n 1   mnth        8645 non-null   category\n 2   day         8645 non-null   int64   \n 3   hr          8645 non-null   category\n 4   holiday     8645 non-null   int64   \n 5   weekday     8645 non-null   int64   \n 6   workingday  8645 non-null   int64   \n 7   weathersit  8645 non-null   object  \n 8   temp        8645 non-null   float64 \n 9   atemp       8645 non-null   float64 \n 10  hum         8645 non-null   float64 \n 11  windspeed   8645 non-null   float64 \n 12  casual      8645 non-null   int64   \n 13  registered  8645 non-null   int64   \n 14  bikers      8645 non-null   int64   \ndtypes: category(2), float64(4), int64(8), object(1)\nmemory usage: 963.5+ KB\n\n\n\nprint(Bikeshare.mnth)\nlevels = Bikeshare.mnth.unique()\nlevels\n\n0       Jan\n1       Jan\n2       Jan\n3       Jan\n4       Jan\n       ... \n8640    Dec\n8641    Dec\n8642    Dec\n8643    Dec\n8644    Dec\nName: mnth, Length: 8645, dtype: category\nCategories (12, object): ['April', 'Aug', 'Dec', 'Feb', ..., 'May', 'Nov', 'Oct', 'Sept']\n\n\n['Jan', 'Feb', 'March', 'April', 'May', ..., 'Aug', 'Sept', 'Oct', 'Nov', 'Dec']\nLength: 12\nCategories (12, object): ['April', 'Aug', 'Dec', 'Feb', ..., 'May', 'Nov', 'Oct', 'Sept']\n\n\n\nBikeshare.mnth.cat.reorder_categories(levels, inplace=True)\nBikeshare.mnth\n\n/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: FutureWarning: The `inplace` parameter in pandas.Categorical.reorder_categories is deprecated and will be removed in a future version. Reordering categories will always return a new Categorical object.\n  \"\"\"Entry point for launching an IPython kernel.\n\n\n0       Jan\n1       Jan\n2       Jan\n3       Jan\n4       Jan\n       ... \n8640    Dec\n8641    Dec\n8642    Dec\n8643    Dec\n8644    Dec\nName: mnth, Length: 8645, dtype: category\nCategories (12, object): ['April', 'Aug', 'Dec', 'Feb', ..., 'May', 'Nov', 'Oct', 'Sept']\n\n\nHere, we use Jan and 0 as baseline\n\n# Note double quote is needed fot the reference string!\nest = smf.ols('bikers ~ C(mnth, Treatment(\"Jan\")) + C(hr, Treatment(0)) + workingday + temp + weathersit',data = Bikeshare).fit()\nprint(est.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                 bikers   R-squared:                       0.675\nModel:                            OLS   Adj. R-squared:                  0.673\nMethod:                 Least Squares   F-statistic:                     457.3\nDate:                Sun, 25 Sep 2022   Prob (F-statistic):               0.00\nTime:                        08:38:33   Log-Likelihood:                -49743.\nNo. Observations:                8645   AIC:                         9.957e+04\nDf Residuals:                    8605   BIC:                         9.985e+04\nDf Model:                          39                                         \nCovariance Type:            nonrobust                                         \n======================================================================================================\n                                         coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------------------------------\nIntercept                            -68.6317      5.307    -12.932      0.000     -79.035     -58.229\nC(mnth, Treatment(\"Jan\"))[T.April]    41.4249      4.972      8.331      0.000      31.678      51.172\nC(mnth, Treatment(\"Jan\"))[T.Aug]      53.2430      6.640      8.019      0.000      40.227      66.259\nC(mnth, Treatment(\"Jan\"))[T.Dec]      46.4577      4.271     10.878      0.000      38.086      54.829\nC(mnth, Treatment(\"Jan\"))[T.Feb]       6.8452      4.287      1.597      0.110      -1.559      15.250\nC(mnth, Treatment(\"Jan\"))[T.July]     45.3245      7.081      6.401      0.000      31.444      59.205\nC(mnth, Treatment(\"Jan\"))[T.June]     67.8187      6.544     10.364      0.000      54.992      80.646\nC(mnth, Treatment(\"Jan\"))[T.March]    16.5514      4.301      3.848      0.000       8.120      24.983\nC(mnth, Treatment(\"Jan\"))[T.May]      72.5571      5.641     12.862      0.000      61.499      83.615\nC(mnth, Treatment(\"Jan\"))[T.Nov]      60.3100      4.610     13.083      0.000      51.273      69.347\nC(mnth, Treatment(\"Jan\"))[T.Oct]      75.8343      4.950     15.319      0.000      66.130      85.538\nC(mnth, Treatment(\"Jan\"))[T.Sept]     66.6783      5.925     11.254      0.000      55.064      78.293\nC(hr, Treatment(0))[T.1]             -14.5793      5.699     -2.558      0.011     -25.750      -3.408\nC(hr, Treatment(0))[T.2]             -21.5791      5.733     -3.764      0.000     -32.817     -10.341\nC(hr, Treatment(0))[T.3]             -31.1408      5.778     -5.389      0.000     -42.468     -19.814\nC(hr, Treatment(0))[T.4]             -36.9075      5.802     -6.361      0.000     -48.281     -25.534\nC(hr, Treatment(0))[T.5]             -24.1355      5.737     -4.207      0.000     -35.381     -12.890\nC(hr, Treatment(0))[T.6]              20.5997      5.704      3.612      0.000       9.419      31.781\nC(hr, Treatment(0))[T.7]             120.0931      5.693     21.095      0.000     108.934     131.253\nC(hr, Treatment(0))[T.8]             223.6619      5.690     39.310      0.000     212.509     234.815\nC(hr, Treatment(0))[T.9]             120.5819      5.693     21.182      0.000     109.423     131.741\nC(hr, Treatment(0))[T.10]             83.8013      5.705     14.689      0.000      72.618      94.985\nC(hr, Treatment(0))[T.11]            105.4234      5.722     18.424      0.000      94.207     116.640\nC(hr, Treatment(0))[T.12]            137.2837      5.740     23.916      0.000     126.032     148.536\nC(hr, Treatment(0))[T.13]            136.0359      5.760     23.617      0.000     124.745     147.327\nC(hr, Treatment(0))[T.14]            126.6361      5.776     21.923      0.000     115.313     137.959\nC(hr, Treatment(0))[T.15]            132.0865      5.780     22.852      0.000     120.756     143.417\nC(hr, Treatment(0))[T.16]            178.5206      5.772     30.927      0.000     167.206     189.836\nC(hr, Treatment(0))[T.17]            296.2670      5.749     51.537      0.000     284.998     307.536\nC(hr, Treatment(0))[T.18]            269.4409      5.736     46.976      0.000     258.198     280.684\nC(hr, Treatment(0))[T.19]            186.2558      5.714     32.596      0.000     175.055     197.457\nC(hr, Treatment(0))[T.20]            125.5492      5.704     22.012      0.000     114.369     136.730\nC(hr, Treatment(0))[T.21]             87.5537      5.693     15.378      0.000      76.393      98.714\nC(hr, Treatment(0))[T.22]             59.1226      5.689     10.392      0.000      47.970      70.275\nC(hr, Treatment(0))[T.23]             26.8376      5.688      4.719      0.000      15.688      37.987\nweathersit[T.cloudy/misty]           -12.8903      1.964     -6.562      0.000     -16.741      -9.040\nweathersit[T.heavy rain/snow]       -109.7446     76.667     -1.431      0.152    -260.031      40.542\nweathersit[T.light rain/snow]        -66.4944      2.965    -22.425      0.000     -72.307     -60.682\nworkingday                             1.2696      1.784      0.711      0.477      -2.228       4.768\ntemp                                 157.2094     10.261     15.321      0.000     137.095     177.324\n==============================================================================\nOmnibus:                      288.526   Durbin-Watson:                   0.519\nProb(Omnibus):                  0.000   Jarque-Bera (JB):              518.512\nSkew:                           0.273   Prob(JB):                    2.55e-113\nKurtosis:                       4.068   Cond. No.                         131.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nIn ols, the first level of hr (0) and mnth (Jan) are treated as the baseline values by specifying the reference and explicity cated to categorical variable. Therefore, no coefficient estimates are provided for them: implicitly, their coefficient estimates are zero, and all other levels are measured relative to these baselines. For example, the Feb coefficient of \\(6.845\\) signifies that, holding all other variables constant, there are on average about 7 more riders in February than in January. Similarly there are about 16.5 more riders in March than in January.\nhttps://github.com/statsmodels/statsmodels/blob/main/examples/python/contrasts.py.\nThe results seen in Section 4.6.1 used a slightly different coding of the variables hr and mnth, as follows:\n\nfrom patsy.contrasts import Sum\ncontrast1 = Sum().code_without_intercept(list(range(24)))\n#print(contrast.matrix)\ncontrast2 = Sum().code_without_intercept(list(range(12)))\n\n\nest2 = smf.ols('bikers ~ C(mnth, contrast2) + C(hr, contrast1) + workingday + temp + weathersit',data = Bikeshare).fit()\nprint(est2.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                 bikers   R-squared:                       0.675\nModel:                            OLS   Adj. R-squared:                  0.673\nMethod:                 Least Squares   F-statistic:                     457.3\nDate:                Sun, 25 Sep 2022   Prob (F-statistic):               0.00\nTime:                        08:38:34   Log-Likelihood:                -49743.\nNo. Observations:                8645   AIC:                         9.957e+04\nDf Residuals:                    8605   BIC:                         9.985e+04\nDf Model:                          39                                         \nCovariance Type:            nonrobust                                         \n=================================================================================================\n                                    coef    std err          t      P&gt;|t|      [0.025      0.975]\n-------------------------------------------------------------------------------------------------\nIntercept                        73.5974      5.132     14.340      0.000      63.537      83.658\nC(mnth, contrast2)[S.0]          -4.6622      2.741     -1.701      0.089     -10.034       0.710\nC(mnth, contrast2)[S.1]           7.1560      3.535      2.024      0.043       0.227      14.085\nC(mnth, contrast2)[S.2]           0.3706      3.156      0.117      0.907      -5.816       6.558\nC(mnth, contrast2)[S.3]         -39.2419      3.539    -11.088      0.000     -46.179     -32.304\nC(mnth, contrast2)[S.4]         -46.0871      4.085    -11.281      0.000     -54.096     -38.079\nC(mnth, contrast2)[S.5]          -0.7626      3.908     -0.195      0.845      -8.424       6.899\nC(mnth, contrast2)[S.6]          21.7317      3.465      6.272      0.000      14.939      28.524\nC(mnth, contrast2)[S.7]         -29.5357      3.155     -9.361      0.000     -35.721     -23.351\nC(mnth, contrast2)[S.8]          26.4700      2.851      9.285      0.000      20.882      32.058\nC(mnth, contrast2)[S.9]          14.2229      2.860      4.972      0.000       8.616      19.830\nC(mnth, contrast2)[S.10]         29.7472      2.700     11.019      0.000      24.455      35.039\nC(hr, contrast1)[S.0]           -96.1420      3.955    -24.307      0.000    -103.895     -88.389\nC(hr, contrast1)[S.1]          -110.7213      3.966    -27.916      0.000    -118.496    -102.947\nC(hr, contrast1)[S.2]          -117.7212      4.016    -29.310      0.000    -125.594    -109.848\nC(hr, contrast1)[S.3]          -127.2828      4.081    -31.191      0.000    -135.282    -119.283\nC(hr, contrast1)[S.4]          -133.0495      4.117    -32.319      0.000    -141.119    -124.980\nC(hr, contrast1)[S.5]          -120.2775      4.037    -29.794      0.000    -128.191    -112.364\nC(hr, contrast1)[S.6]           -75.5424      3.992    -18.925      0.000     -83.367     -67.718\nC(hr, contrast1)[S.7]            23.9511      3.969      6.035      0.000      16.172      31.730\nC(hr, contrast1)[S.8]           127.5199      3.950     32.284      0.000     119.777     135.263\nC(hr, contrast1)[S.9]            24.4399      3.936      6.209      0.000      16.724      32.155\nC(hr, contrast1)[S.10]          -12.3407      3.936     -3.135      0.002     -20.056      -4.625\nC(hr, contrast1)[S.11]            9.2814      3.945      2.353      0.019       1.549      17.014\nC(hr, contrast1)[S.12]           41.1417      3.957     10.397      0.000      33.385      48.899\nC(hr, contrast1)[S.13]           39.8939      3.975     10.036      0.000      32.102      47.686\nC(hr, contrast1)[S.14]           30.4940      3.991      7.641      0.000      22.671      38.317\nC(hr, contrast1)[S.15]           35.9445      3.995      8.998      0.000      28.114      43.775\nC(hr, contrast1)[S.16]           82.3786      3.988     20.655      0.000      74.561      90.197\nC(hr, contrast1)[S.17]          200.1249      3.964     50.488      0.000     192.355     207.895\nC(hr, contrast1)[S.18]          173.2989      3.956     43.806      0.000     165.544     181.054\nC(hr, contrast1)[S.19]           90.1138      3.940     22.872      0.000      82.390      97.837\nC(hr, contrast1)[S.20]           29.4071      3.936      7.471      0.000      21.691      37.123\nC(hr, contrast1)[S.21]           -8.5883      3.933     -2.184      0.029     -16.298      -0.878\nC(hr, contrast1)[S.22]          -37.0194      3.934     -9.409      0.000     -44.732     -29.307\nweathersit[T.cloudy/misty]      -12.8903      1.964     -6.562      0.000     -16.741      -9.040\nweathersit[T.heavy rain/snow]  -109.7446     76.667     -1.431      0.152    -260.031      40.542\nweathersit[T.light rain/snow]   -66.4944      2.965    -22.425      0.000     -72.307     -60.682\nworkingday                        1.2696      1.784      0.711      0.477      -2.228       4.768\ntemp                            157.2094     10.261     15.321      0.000     137.095     177.324\n==============================================================================\nOmnibus:                      288.526   Durbin-Watson:                   0.519\nProb(Omnibus):                  0.000   Jarque-Bera (JB):              518.512\nSkew:                           0.273   Prob(JB):                    2.55e-113\nKurtosis:                       4.068   Cond. No.                         127.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nWhat is the difference between the two codings? In est2, a coefficient estimate is reported for all but the last level of hr and mnth. Importantly, in est2, the coefficient estimate for the last level of mnth is not zero: instead, it equals the negative of the sum of the coefficient estimates for all of the other levels. Similarly, in est2, the coefficient estimate for the last level of hr is the negative of the sum of the coefficient estimates for all of the other levels. This means that the coefficients of hr and mnth in est2 will always sum to zero, and can be interpreted as the difference from the mean level. For example, the coefficient for January of \\(-46.087\\) indicates that, holding all other variables constant, there are typically 46 fewer riders in January relative to the yearly average.\nIt is important to realize that the choice of coding really does not matter, provided that we interpret the model output correctly in light of the coding used. For example, we see that the predictions from the linear model are the same regardless of coding:\n\nnp.sum((est.predict() - est2.predict())**2)\n\n2.244830988197902e-20\n\n\nThe sum of squared differences is zero. We can also see this using the np.allclose() function:\n\nnp.allclose(est.predict(), est2.predict())\n\nTrue\n\n\nTo reproduce the left-hand side of Figure 4.13, we must first obtain the coefficient estimates associated with mnth. The coefficients for January through November can be obtained directly from the est2 object. The coefficient for December must be explicitly computed as the negative sum of all the other months.\n\ny1 = list(est2.params[1:12])\ny1.append(-np.sum(est2.params[1:12]))\n\n\nax = sns.lineplot(x=range(1,13), y=y1, marker=\"o\")\nax.set_xlabel(\"Month\")\nax.set_ylabel(\"Coefficien\")\nax.set_xticks(range(1,13))\nax.set_xticklabels([\"J\", \"F\", \"M\", \"A\", \"M\", \"J\", \"J\", \"A\", \"S\", \"O\", \"N\", \"D\"])\n\n[Text(0, 0, 'J'),\n Text(0, 0, 'F'),\n Text(0, 0, 'M'),\n Text(0, 0, 'A'),\n Text(0, 0, 'M'),\n Text(0, 0, 'J'),\n Text(0, 0, 'J'),\n Text(0, 0, 'A'),\n Text(0, 0, 'S'),\n Text(0, 0, 'O'),\n Text(0, 0, 'N'),\n Text(0, 0, 'D')]\n\n\n\n\n\nReproducing the right-hand side of Figure 4.13 follows a similar process.\n\ny2 = list(est2.params[12:35])\ny2.append(-np.sum(est2.params[12:35]))\n\n\nax = sns.lineplot(x=range(1,25), y=y2, marker=\"o\")\nax.set_xlabel(\"Hour\")\nax.set_ylabel(\"Coefficien\")\n\nText(0, 0.5, 'Coefficien')\n\n\n\n\n\nNow, we consider instead fitting a Poisson regression model to the Bikeshare data. Very little changes, except that we now use the function glm() with the argument families.Poisson to specify that we wish to fit a Poisson regression model:\n\nformula = 'bikers ~ C(mnth, contrast2) + C(hr, contrast1) + workingday + temp + weathersit'\nmodel = smf.glm(formula=formula, data=Bikeshare, family=sm.families.Poisson())\nresult = model.fit()\nprint(result.summary())\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:                 bikers   No. Observations:                 8645\nModel:                            GLM   Df Residuals:                     8605\nModel Family:                 Poisson   Df Model:                           39\nLink Function:                    log   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:            -1.4054e+05\nDate:                Sun, 25 Sep 2022   Deviance:                   2.2804e+05\nTime:                        08:38:40   Pearson chi2:                 2.20e+05\nNo. Iterations:                     7                                         \nCovariance Type:            nonrobust                                         \n=================================================================================================\n                                    coef    std err          z      P&gt;|z|      [0.025      0.975]\n-------------------------------------------------------------------------------------------------\nIntercept                         4.1182      0.006    683.963      0.000       4.106       4.130\nC(mnth, contrast2)[S.0]           0.0215      0.003      6.888      0.000       0.015       0.028\nC(mnth, contrast2)[S.1]           0.1512      0.004     41.281      0.000       0.144       0.158\nC(mnth, contrast2)[S.2]           0.0167      0.004      4.390      0.000       0.009       0.024\nC(mnth, contrast2)[S.3]          -0.4441      0.005    -91.379      0.000      -0.454      -0.435\nC(mnth, contrast2)[S.4]          -0.6702      0.006   -113.445      0.000      -0.682      -0.659\nC(mnth, contrast2)[S.5]           0.1036      0.004     25.121      0.000       0.096       0.112\nC(mnth, contrast2)[S.6]           0.2232      0.004     62.818      0.000       0.216       0.230\nC(mnth, contrast2)[S.7]          -0.2937      0.004    -70.886      0.000      -0.302      -0.286\nC(mnth, contrast2)[S.8]           0.2405      0.003     82.462      0.000       0.235       0.246\nC(mnth, contrast2)[S.9]           0.1503      0.003     47.248      0.000       0.144       0.156\nC(mnth, contrast2)[S.10]          0.2676      0.003     96.091      0.000       0.262       0.273\nC(hr, contrast1)[S.0]            -0.7544      0.008    -95.744      0.000      -0.770      -0.739\nC(hr, contrast1)[S.1]            -1.2260      0.010   -123.173      0.000      -1.245      -1.206\nC(hr, contrast1)[S.2]            -1.5631      0.012   -131.702      0.000      -1.586      -1.540\nC(hr, contrast1)[S.3]            -2.1983      0.016   -133.846      0.000      -2.230      -2.166\nC(hr, contrast1)[S.4]            -2.8305      0.023   -125.586      0.000      -2.875      -2.786\nC(hr, contrast1)[S.5]            -1.8147      0.013   -134.775      0.000      -1.841      -1.788\nC(hr, contrast1)[S.6]            -0.4299      0.007    -62.341      0.000      -0.443      -0.416\nC(hr, contrast1)[S.7]             0.5752      0.004    130.544      0.000       0.567       0.584\nC(hr, contrast1)[S.8]             1.0769      0.004    302.220      0.000       1.070       1.084\nC(hr, contrast1)[S.9]             0.5818      0.004    135.727      0.000       0.573       0.590\nC(hr, contrast1)[S.10]            0.3369      0.005     71.372      0.000       0.328       0.346\nC(hr, contrast1)[S.11]            0.4941      0.004    112.494      0.000       0.486       0.503\nC(hr, contrast1)[S.12]            0.6796      0.004    167.040      0.000       0.672       0.688\nC(hr, contrast1)[S.13]            0.6736      0.004    164.722      0.000       0.666       0.682\nC(hr, contrast1)[S.14]            0.6249      0.004    149.570      0.000       0.617       0.633\nC(hr, contrast1)[S.15]            0.6538      0.004    158.205      0.000       0.646       0.662\nC(hr, contrast1)[S.16]            0.8743      0.004    231.040      0.000       0.867       0.882\nC(hr, contrast1)[S.17]            1.2946      0.003    397.848      0.000       1.288       1.301\nC(hr, contrast1)[S.18]            1.2123      0.003    365.084      0.000       1.206       1.219\nC(hr, contrast1)[S.19]            0.9140      0.004    247.065      0.000       0.907       0.921\nC(hr, contrast1)[S.20]            0.6162      0.004    147.045      0.000       0.608       0.624\nC(hr, contrast1)[S.21]            0.3642      0.005     78.173      0.000       0.355       0.373\nC(hr, contrast1)[S.22]            0.1175      0.005     22.488      0.000       0.107       0.128\nweathersit[T.cloudy/misty]       -0.0752      0.002    -34.528      0.000      -0.080      -0.071\nweathersit[T.heavy rain/snow]    -0.9263      0.167     -5.554      0.000      -1.253      -0.599\nweathersit[T.light rain/snow]    -0.5758      0.004   -141.905      0.000      -0.584      -0.568\nworkingday                        0.0147      0.002      7.502      0.000       0.011       0.018\ntemp                              0.7853      0.011     68.434      0.000       0.763       0.808\n=================================================================================================\n\n\nWe can plot the coefficients associated with mnth and hr, in order to reproduce Figure 4.15:\n\ny3 = list(result.params[1:12])\ny3.append(-np.sum(result.params[1:12]))\ny4 = list(result.params[12:35])\ny4.append(-np.sum(result.params[12:35]))\n\n\nax = sns.lineplot(x=range(1,13), y=y3, marker=\"o\")\nax.set_xlabel(\"Month\")\nax.set_ylabel(\"Coefficien\")\nax.set_xticks(range(1,13))\nax.set_xticklabels([\"J\", \"F\", \"M\", \"A\", \"M\", \"J\", \"J\", \"A\", \"S\", \"O\", \"N\", \"D\"])\n\n[Text(0, 0, 'J'),\n Text(0, 0, 'F'),\n Text(0, 0, 'M'),\n Text(0, 0, 'A'),\n Text(0, 0, 'M'),\n Text(0, 0, 'J'),\n Text(0, 0, 'J'),\n Text(0, 0, 'A'),\n Text(0, 0, 'S'),\n Text(0, 0, 'O'),\n Text(0, 0, 'N'),\n Text(0, 0, 'D')]\n\n\n\n\n\n\nax = sns.lineplot(x=range(1,25), y=y4, marker=\"o\")\nax.set_xlabel(\"Hour\")\nax.set_ylabel(\"Coefficien\")\n\nText(0, 0.5, 'Coefficien')\n\n\n\n\n\nWe can once again use the predict() function to obtain the fitted values (predictions) from this Poisson regression model.\n\nlen(result.predict())\n\n8645\n\n\n\nlinear_p = est2.predict()\npoisson_p = result.predict()\nax = sns.lineplot(x=linear_p, y=poisson_p)\nax.set_xlabel(\"Linear prediction\")\nax.set_ylabel(\"Poisson prediction\")\n\nText(0, 0.5, 'Poisson prediction')\n\n\n\n\n\nThe predictions from the Poisson regression model are correlated with those from the linear model; however, the former are non-negative. As a result the Poisson regression predictions tend to be larger than those from the linear model for either very low or very high levels of ridership.\nIn this section, we used the glm() function with the argument families.Poisson in order to perform Poisson regression. Earlier in this lab we used the glm() function with families.Binomial to perform logistic regression. Other choices for the families argument can be used to fit other types of GLMs. For instance, families.Gamma fits a gamma regression model."
  },
  {
    "objectID": "Chapter_4_Lab.html#logistic-regression-in-sklearn",
    "href": "Chapter_4_Lab.html#logistic-regression-in-sklearn",
    "title": "4  Classification",
    "section": "4.8 Logistic regression in Sklearn",
    "text": "4.8 Logistic regression in Sklearn\n\nSmarket['Direction2'] = pd.get_dummies(Smarket.Direction, drop_first=True)\nSmarket.head()\n\n\n  \n    \n      \n\n\n\n\n\n\nYear\nLag1\nLag2\nLag3\nLag4\nLag5\nVolume\nToday\nDirection\nDirection2\n\n\n\n\n0\n2001\n0.381\n-0.192\n-2.624\n-1.055\n5.010\n1.1913\n0.959\nUp\n1\n\n\n1\n2001\n0.959\n0.381\n-0.192\n-2.624\n-1.055\n1.2965\n1.032\nUp\n1\n\n\n2\n2001\n1.032\n0.959\n0.381\n-0.192\n-2.624\n1.4112\n-0.623\nDown\n0\n\n\n3\n2001\n-0.623\n1.032\n0.959\n0.381\n-0.192\n1.2760\n0.614\nUp\n1\n\n\n4\n2001\n0.614\n-0.623\n1.032\n0.959\n0.381\n1.2057\n0.213\nUp\n1\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nC is a regularization term where a higher C indicates less penalty on the magnitude of the coefficients and. We set C to be arbitrarily high such that there is effectively no regulariation.\n\nX = Smarket.loc[:, 'Lag1':'Volume']\ny = Smarket.Direction2\nlogreg = LogisticRegression(solver='newton-cg', C=1e42)  #large C for no regularization\nlogreg.fit(X, y)\n\nLogisticRegression(C=1e+42, solver='newton-cg')\n\n\n\nprint(\"Intercept:\", logreg.intercept_)\nlist(zip(X.columns, logreg.coef_[0]) )         # coefficients\n\nIntercept: [-0.1259998]\n\n\n[('Lag1', -0.07307368882753848),\n ('Lag2', -0.042301339556733895),\n ('Lag3', 0.011085169923358404),\n ('Lag4', 0.009358899763340657),\n ('Lag5', 0.010313090846412886),\n ('Volume', 0.13544036505211507)]\n\n\n\nlist( zip(X.index, logreg.predict_proba(X=X)[:10, 1]) )\n\n[(0, 0.5070841630730085),\n (1, 0.4814679278688445),\n (2, 0.48113885449678334),\n (3, 0.5152223784682999),\n (4, 0.5107812050659893),\n (5, 0.5069564651153047),\n (6, 0.49265092350090467),\n (7, 0.5092291586044021),\n (8, 0.5176135774773843),\n (9, 0.4888378035325076)]\n\n\n\ny_pred_class = logreg.predict(X=X)\ny_pred_class_prob = logreg.predict_proba(X=X)\ny_pred_class.sum()\n\n964\n\n\n\n(y == y_pred_class).sum()    \n\n652\n\n\n\nprint(\"Accuracy: \", accuracy_score(y, y_pred_class))   # Accuracy\nprint(\"Accuracy (manual calculation): %0.7f \" % (y == y_pred_class).mean() )\n\nAccuracy:  0.5216\nAccuracy (manual calculation): 0.5216000 \n\n\n\nlogreg.predict([[1.2,1.1,1,1,1,1],\n                 [1.5,-0.8,1,1,1,1]])\n\n/usr/local/lib/python3.7/dist-packages/sklearn/base.py:451: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n  \"X does not have valid feature names, but\"\n\n\narray([0, 0], dtype=uint8)\n\n\nYou can find many useful metrics in the sklearn module https://scikit-learn.org/stable/modules/model_evaluation.html#classification-metrics.\n\nfpr, tpr, thresholds = roc_curve(y, y_pred_class)\nprint(fpr, tpr)\nroc_auc = auc(fpr, tpr)\nRocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc, estimator_name='example estimator').plot()\n\n[0.         0.75913621 1.        ] [0.         0.78240741 1.        ]\n\n\n&lt;sklearn.metrics._plot.roc_curve.RocCurveDisplay at 0x7fe6187c9a50&gt;\n\n\n\n\n\n\nfpr, tpr, thresholds = roc_curve(y, y_pred_class_prob[:,1])\nroc_auc = auc(fpr, tpr)\nRocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc, estimator_name='example estimator').plot()\n\n&lt;sklearn.metrics._plot.roc_curve.RocCurveDisplay at 0x7fe61859a5d0&gt;"
  },
  {
    "objectID": "Chapter_4_Lab.html#optional",
    "href": "Chapter_4_Lab.html#optional",
    "title": "4  Classification",
    "section": "4.9 Optional",
    "text": "4.9 Optional\n\n4.9.1 Poisson regression with sklearn and column transform\nhttps://scikit-learn.org/stable/auto_examples/linear_model/plot_poisson_regression_non_normal_loss.html\n\n\n4.9.2 Yellowbric’s plot\nhttps://www.scikit-yb.org/en/latest/api/classifier/index.html#"
  },
  {
    "objectID": "Chapter_5_Lab.html#the-validation-set-approach",
    "href": "Chapter_5_Lab.html#the-validation-set-approach",
    "title": "5  Resampling methods",
    "section": "5.1 The Validation Set Approach",
    "text": "5.1 The Validation Set Approach\n\nimport pandas as pd\nimport numpy as np\n\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split, LeaveOneOut, cross_val_score, KFold\nfrom sklearn import metrics\nfrom sklearn.utils import resample\n\nimport statsmodels.formula.api as smf\nimport statsmodels.api as sm\n\nfrom functools import partial\nfrom tqdm import tqdm\ntqdm = partial(tqdm, position=0, leave=True)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\nplt.style.use('seaborn-white')\nsns.set_context(\"notebook\", font_scale=1.5, rc={\"lines.linewidth\": 2.5})\n\nIn this lab, we explore the resampling techniques covered in this chapter. Some of the commands in this lab may take a while to run on your computer.\nWe explore the use of the validation set approach in order to estimate the test error rates that result from fitting various linear models on the Auto data set.\nBefore we begin, we use the random_state in order to set a for Python’s random number generator, so that the reader of this book will obtain precisely the same results as those shown below. It is generally a good idea to set a random seed when performing an analysis such as cross-validation that contains an element of randomness, so that the results obtained can be reproduced precisely at a later time.\nWe begin by using the train_test_split() function to split the set of observations into two halves, by selecting a random subset of \\(196\\) observations out of the original \\(392\\) observations. We refer to these observations as the training set.\n\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\nMounted at /content/drive\n\n\n\n# Load data\nAuto = pd.read_csv(\"/content/drive/MyDrive/Lab/Data/Auto.csv\", index_col=0)\nprint(Auto.shape)\nAuto.info()\n\n(392, 9)\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 392 entries, 1 to 397\nData columns (total 9 columns):\n #   Column        Non-Null Count  Dtype  \n---  ------        --------------  -----  \n 0   mpg           392 non-null    float64\n 1   cylinders     392 non-null    int64  \n 2   displacement  392 non-null    float64\n 3   horsepower    392 non-null    int64  \n 4   weight        392 non-null    int64  \n 5   acceleration  392 non-null    float64\n 6   year          392 non-null    int64  \n 7   origin        392 non-null    int64  \n 8   name          392 non-null    object \ndtypes: float64(3), int64(5), object(1)\nmemory usage: 30.6+ KB\n\n\n\nAuto.head()\n\n\n  \n    \n      \n\n\n\n\n\n\nmpg\ncylinders\ndisplacement\nhorsepower\nweight\nacceleration\nyear\norigin\nname\n\n\n\n\n1\n18.0\n8\n307.0\n130\n3504\n12.0\n70\n1\nchevrolet chevelle malibu\n\n\n2\n15.0\n8\n350.0\n165\n3693\n11.5\n70\n1\nbuick skylark 320\n\n\n3\n18.0\n8\n318.0\n150\n3436\n11.0\n70\n1\nplymouth satellite\n\n\n4\n16.0\n8\n304.0\n150\n3433\n12.0\n70\n1\namc rebel sst\n\n\n5\n17.0\n8\n302.0\n140\n3449\n10.5\n70\n1\nford torino\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nWe need to organize them into NumPy array first.\n\nX = Auto.horsepower.values.reshape(-1,1)\ny = Auto.mpg.values\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=0)\n\nWe now use the predict() function to estimate the response for all \\(392\\) observations, and we use the mean_squared_error() function to calculate the MSE of the \\(196\\) observations in the validation set.\n\nlr_model = LinearRegression()\nlr_model.fit(X_train, y_train)\ny_pred = lr_model.predict(X_test)\n\n\nprint (\"The MAE is: {:.5}\".format( metrics.mean_absolute_error(y_test, y_pred) )  )  # (True, Predict) \nprint (\"The MSE is: {:.5}\".format( metrics.mean_squared_error(y_test, y_pred) )  )\nprint (\"The RMSE is: {:.5}\".format( np.sqrt( metrics.mean_squared_error(y_test, y_pred)))) \n\nThe MAE is: 3.8385\nThe MSE is: 23.617\nThe RMSE is: 4.8597\n\n\nTherefore, the estimated test MSE for the linear regression fit is \\(23.6\\). We can use the poly() function to estimate the test error for the quadratic and cubic regressions.\n\n# quadratic polynomial\n# include bias = False, will not return the constant value, that is X**0, as that is added automatically by lin regression\npoly = PolynomialFeatures(2,include_bias=False)\npoly.fit(X_train)\nX_train_quad = poly.transform(X_train)\nX_test_quad = poly.transform(X_test)\n\nlr_model.fit(X_train_quad,y_train)\ny_pred = lr_model.predict(X_test_quad)\n\nprint (\"The MAE is: {:.5}\".format( metrics.mean_absolute_error(y_test, y_pred) )  )  # (True, Predict) \nprint (\"The MSE is: {:.5}\".format( metrics.mean_squared_error(y_test, y_pred) )  )\nprint (\"The RMSE is: {:.5}\".format( np.sqrt( metrics.mean_squared_error(y_test, y_pred)))) \n\nThe MAE is: 3.2235\nThe MSE is: 18.763\nThe RMSE is: 4.3316\n\n\n\n#cubic polynomial\npoly = PolynomialFeatures(3,include_bias=False)\npoly.fit(X_train)\nX_train_cubic = poly.transform(X_train)\nX_test_cubic = poly.transform(X_test)\n\nlr_model.fit(X_train_cubic,y_train)\ny_pred = lr_model.predict(X_test_cubic)\n\nprint (\"The MAE is: {:.5}\".format( metrics.mean_absolute_error(y_test, y_pred) )  )  # (True, Predict) \nprint (\"The MSE is: {:.5}\".format( metrics.mean_squared_error(y_test, y_pred) )  )\nprint (\"The RMSE is: {:.5}\".format( np.sqrt( metrics.mean_squared_error(y_test, y_pred)))) \n\nThe MAE is: 3.219\nThe MSE is: 18.797\nThe RMSE is: 4.3355\n\n\nThese error rates are \\(18.76\\) and \\(18.80\\), respectively. If we choose a different training set instead, then we will obtain somewhat different errors on the validation set.\n\n# with different random state\n# earlier it was random state 0, now its 2\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=2)\n\nlr_model.fit(X_train,y_train)\ny_pred = lr_model.predict(X_test)\nprint('Mean squared error is ',metrics.mean_squared_error(y_test,y_pred))\n\n# quadratic polynomial\npoly = PolynomialFeatures(2,include_bias=False)\npoly.fit(X_train)\nX_train_quad = poly.transform(X_train)\nX_test_quad = poly.transform(X_test)\n\nlr_model.fit(X_train_quad,y_train)\ny_pred = lr_model.predict(X_test_quad)\n\nprint('Mean squared error for quadratic is ',metrics.mean_squared_error(y_test,y_pred))\n\n#cubic polynomial\npoly = PolynomialFeatures(3,include_bias = False)\npoly.fit(X_train)\nX_train_cubic = poly.transform(X_train)\nX_test_cubic = poly.transform(X_test)\n\nlr_model.fit(X_train_cubic,y_train)\ny_pred = lr_model.predict(X_test_cubic)\n\nprint('Mean squared error for cubic is ',metrics.mean_squared_error(y_test,y_pred))\n\nMean squared error is  23.442643969985735\nMean squared error for quadratic is  18.550198801910312\nMean squared error for cubic is  18.59522229455435\n\n\nUsing this split of the observations into a training set and a validation set, we find that the validation set error rates for the models with linear, quadratic, and cubic terms are \\(23.44\\), \\(18.55\\), and \\(18.60\\), respectively.\nThese results are consistent with our previous findings: a model that predicts mpg using a quadratic function of horsepower performs better than a model that involves only a linear function of horsepower, and there is little evidence in favor of a model that uses a cubic function of horsepower."
  },
  {
    "objectID": "Chapter_5_Lab.html#leave-one-out-cross-validation",
    "href": "Chapter_5_Lab.html#leave-one-out-cross-validation",
    "title": "5  Resampling methods",
    "section": "5.2 Leave-One-Out Cross-Validation",
    "text": "5.2 Leave-One-Out Cross-Validation\nThe LOOCV estimate can be automatically computed using cross_val_score or LeaveOneOut.\n\nloocv = LeaveOneOut()\nloocv.get_n_splits(X)\n\n392\n\n\n\nloocv_mse = []\nlm = LinearRegression()\n\nfor train_index, test_index in loocv.split(X):\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    \n    lm1_fit = lm.fit(X_train, y_train)\n    lm1_predict = lm1_fit.predict(X_test)\n    \n    loocv_mse.append(metrics.mean_squared_error(y_test, lm1_predict))\n    \nnp.array(loocv_mse).mean()\n\n24.231513517929226\n\n\n\nloocv_mse2 = []\n\nfor train_index, test_index in loocv.split(X):  \n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n\n    lm2 = sm.OLS(y_train, sm.add_constant(X_train))\n        \n    lm2_fit = lm2.fit()\n    lm2_predict = lm2_fit.predict(sm.add_constant(X_test, has_constant='add'))\n\n    loocv_mse2.append(metrics.mean_squared_error(y_test, lm2_predict))\n    \nnp.array(loocv_mse2).mean()\n\n24.23151351792922\n\n\nThe value correspond to the LOOCV statistic given in (5.1). Our cross-validation estimate for the test error is approximately \\(24.23\\).\nWe can repeat this procedure for increasingly complex polynomial fits.\n\ncv_error = []\nlm = LinearRegression()\n\nfor i in tqdm(range(1,11)):\n    loocv_mse = []\n    for train_index, test_index in loocv.split(X):\n        X_train, X_test = X[train_index], X[test_index]\n        y_train, y_test = y[train_index], y[test_index]\n        \n        poly = PolynomialFeatures(i,include_bias=False)\n        poly.fit(X_train)\n        X_train_2 = poly.transform(X_train)\n        X_test_2 = poly.transform(X_test)\n        \n        lm1_fit = lm.fit(X_train_2, y_train)\n        lm1_predict = lm1_fit.predict(X_test_2)\n        \n        loocv_mse.append(metrics.mean_squared_error(y_test, lm1_predict))\n    cv_error.append(np.array(loocv_mse).mean())\n\n100%|██████████| 10/10 [00:02&lt;00:00,  3.38it/s]\n\n\n\ncv_error\n\n[24.231513517929226,\n 19.24821312448967,\n 19.33498406402931,\n 19.42443031024277,\n 19.03321248615882,\n 18.97863406819667,\n 19.129480449254846,\n 19.224150660848743,\n 19.133322843461364,\n 18.93976572079586]\n\n\nWe can instead use cross_val_score for calculating the validation error. Check https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html for more details.\n\nerror_list = []\nfor power in tqdm(range(1,11)):\n    poly = PolynomialFeatures(power,include_bias=False)\n    X2 = poly.fit_transform(X)  \n    lr = LinearRegression()\n    # for LOOCV, the number of folds be will n = size of data\n    error_list.append(-1*cross_val_score(lr,X2,y, cv = len(X), n_jobs=-1, scoring = 'neg_mean_squared_error').mean())\n\n100%|██████████| 10/10 [00:05&lt;00:00,  1.78it/s]\n\n\n\npd.DataFrame({\"DEGREE\":np.arange(1,11),\"MEAN SQUARED ERROR\":error_list})\n\n\n  \n    \n      \n\n\n\n\n\n\nDEGREE\nMEAN SQUARED ERROR\n\n\n\n\n0\n1\n24.231514\n\n\n1\n2\n19.248213\n\n\n2\n3\n19.334984\n\n\n3\n4\n19.424430\n\n\n4\n5\n19.033212\n\n\n5\n6\n18.978634\n\n\n6\n7\n19.129480\n\n\n7\n8\n19.224151\n\n\n8\n9\n19.133323\n\n\n9\n10\n18.939766\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nAs in Figure 5.4, we see a sharp drop in the estimated test MSE between the linear and quadratic fits, but then no clear improvement from using higher-order polynomials."
  },
  {
    "objectID": "Chapter_5_Lab.html#k-fold-cross-validation",
    "href": "Chapter_5_Lab.html#k-fold-cross-validation",
    "title": "5  Resampling methods",
    "section": "5.3 \\(k\\)-Fold Cross-Validation",
    "text": "5.3 \\(k\\)-Fold Cross-Validation\nThe cross_val_score function can also be used to implement \\(k\\)-fold CV. Below we use \\(k=10\\), a common choice for \\(k\\), on the Auto data set. We once again set a random seed and initialize a vector in which we will store the CV errors corresponding to the polynomial fits of orders one to ten.\n\ncv_error = []\nlm = LinearRegression()\n\nkf = KFold(n_splits=10, shuffle=True, random_state=1)\nfor i in tqdm(range(1,11)):\n    kfold_mse = []\n    for train_index, test_index in kf.split(X):\n        X_train, X_test = X[train_index], X[test_index]\n        y_train, y_test = y[train_index], y[test_index]\n        \n        poly = PolynomialFeatures(i,include_bias=False)\n        poly.fit(X_train)\n        X_train_2 = poly.transform(X_train)\n        X_test_2 = poly.transform(X_test)\n        \n        lm1_fit = lm.fit(X_train_2, y_train)\n        lm1_predict = lm1_fit.predict(X_test_2)\n        \n        kfold_mse.append(metrics.mean_squared_error(y_test, lm1_predict))\n    cv_error.append(np.array(kfold_mse).mean())\n\n100%|██████████| 10/10 [00:00&lt;00:00, 67.33it/s]\n\n\n\ncv_error\n\n[24.097675731883058,\n 19.17888986488955,\n 19.21385952370859,\n 19.21280701624055,\n 18.757991658614383,\n 18.6424339955243,\n 18.810193380203554,\n 18.975900958024067,\n 18.937570150409766,\n 18.79101740940429]\n\n\n\nlr = LinearRegression()\nerror_list = []\nkf_10 = KFold(n_splits=10, shuffle=True, random_state=1)\nfor power in tqdm(range(1,11)):\n    poly = PolynomialFeatures(power,include_bias=False)\n    X2 = poly.fit_transform(X) \n    error_list.append(-1*cross_val_score(lr,X2,y, cv = kf_10, n_jobs=-1, scoring = 'neg_mean_squared_error').mean()) \n    #use k fold instead of stratefied k fold\n\nprint('K FOLD CV')    \npd.DataFrame({\"DEGREE\":np.arange(1,11),\"MEAN SQUARED ERROR\":error_list})\n\n100%|██████████| 10/10 [00:00&lt;00:00, 23.77it/s]\n\n\nK FOLD CV\n\n\n\n\n\n\n  \n    \n      \n\n\n\n\n\n\nDEGREE\nMEAN SQUARED ERROR\n\n\n\n\n0\n1\n24.097676\n\n\n1\n2\n19.178890\n\n\n2\n3\n19.213860\n\n\n3\n4\n19.212807\n\n\n4\n5\n18.757992\n\n\n5\n6\n18.642434\n\n\n6\n7\n18.810193\n\n\n7\n8\n18.975901\n\n\n8\n9\n18.937570\n\n\n9\n10\n18.791017\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nSlightly differnt may due to https://stackoverflow.com/questions/60432894/cross-val-score-and-stratifiedkfold-give-different-result.\nNotice that the computation time is shorter than that of LOOCV. (In principle, the computation time for LOOCV for a least squares linear model should be faster than for \\(k\\)-fold CV, due to the availability of the formula (5.2) for LOOCV; however, unfortunately the function does not make use of this formula.) We still see little evidence that using cubic or higher-order polynomial terms leads to lower test error than simply using a quadratic fit."
  },
  {
    "objectID": "Chapter_5_Lab.html#the-bootstrap",
    "href": "Chapter_5_Lab.html#the-bootstrap",
    "title": "5  Resampling methods",
    "section": "5.4 The Bootstrap",
    "text": "5.4 The Bootstrap\nWe illustrate the use of the bootstrap in the simple example of Section 5.2, as well as on an example involving estimating the accuracy of the linear regression model on the Auto data set.\n\n5.4.1 Estimating the Accuracy of a Statistic of Interest\nOne of the great advantages of the bootstrap approach is that it can be applied in almost all situations. No complicated mathematical calculations are required.\nThe Portfolio data set in the ISLR2 package is simulated data of \\(100\\) pairs of returns, generated in the fashion described in Section 5.2. To illustrate the use of the bootstrap on this data, we must first create a function, alpha.fn(), which takes as input the \\((X,Y)\\) data as well as a vector indicating which observations should be used to estimate \\(\\alpha\\). The function then outputs the estimate for \\(\\alpha\\) based on the selected observations.\n\nPortfolio = pd.read_csv('/content/drive/MyDrive/Lab/Data/Portfolio.csv')\n\n\nprint(Portfolio.shape)\nPortfolio.head()\n\n(100, 2)\n\n\n\n  \n    \n      \n\n\n\n\n\n\nX\nY\n\n\n\n\n0\n-0.895251\n-0.234924\n\n\n1\n-1.562454\n-0.885176\n\n\n2\n-0.417090\n0.271888\n\n\n3\n1.044356\n-0.734198\n\n\n4\n-0.315568\n0.841983\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n# we first define a function equivalent to func alpha defined in the Lab, i would recommend you to go through the function in lab\n# This function takes two arguements, data and indeces, this indices are used to calculate the estimate for alpha for this bootstrap\n\n\ndef alpha_fn(data, start_index, end_index):\n    X = data['X'][start_index:end_index]\n    Y = data['Y'][start_index:end_index]\n    return ((np.var(Y) - np.cov(X, Y)[0][1]) / (np.var(X) + np.var(Y) - 2 * np.cov(X, Y)[0][1]))\n\nThis function returns, or outputs, an estimate for \\(\\alpha\\) based on applying (5.7) to the observations indexed by the argument index. For instance, the following command tells Python to estimate \\(\\alpha\\) using all \\(100\\) observations.\n\nalpha_fn(Portfolio, 0, 100)\n\n0.5766511516104116\n\n\nThe next command uses the get_indices() function to randomly select \\(100\\) observations from the range \\(1\\) to \\(100\\), with replacement. This is equivalent to constructing a new bootstrap data set and recomputing \\(\\hat{\\alpha}\\) based on the new data set.\n\nportfolio_bs = resample(Portfolio, replace=True, n_samples=100, random_state=0)\n\nalpha_fn(portfolio_bs, 0, 100)\n\n0.560336658007497\n\n\nWe can implement a bootstrap analysis by performing this command many times, recording all of the corresponding estimates for \\(\\alpha\\), and computing the resulting standard deviation. Note you should not set a fix random_state for resampling, otherwise the variance will be 0. In stead you may follow https://scikit-learn.org/stable/common_pitfalls.html#controlling-randomness for controlling randomness.\n\nrng = np.random.RandomState(0)\nbs_alpha = []\n\nfor i in range(0, 1000):\n    bs_alpha.append(\n        alpha_fn(resample(Portfolio, replace=True, n_samples=100, random_state=rng), 0, 100)\n    )\n\nbs_alpha = np.array(bs_alpha)\n\nprint('Bootstrapped alpha:', bs_alpha.mean())\nprint('SE:',  bs_alpha.std())\n\nBootstrapped alpha: 0.5805913095922189\nSE: 0.08980409161302538\n\n\nThe final output shows that using the original data, \\(\\hat{\\alpha}=0.581\\), and that the bootstrap estimate for \\({\\rm SE}(\\hat{\\alpha})\\) is \\(0.0898\\).\n\n\n5.4.2 Estimating the Accuracy of a Linear Regression Model\nThe bootstrap approach can be used to assess the variability of the coefficient estimates and predictions from a statistical learning method. Here we use the bootstrap approach in order to assess the variability of the estimates for \\(\\beta_0\\) and \\(\\beta_1\\), the intercept and slope terms for the linear regression model that uses horsepower to predict mpg in the Auto data set. We will compare the estimates obtained using the bootstrap to those obtained using the formulas for \\({\\rm SE}(\\hat{\\beta}_0)\\) and \\({\\rm SE}(\\hat{\\beta}_1)\\) described in Section 3.1.2.\nWe first create a simple function, boot.fn(), which takes in the Auto data set as well as a set of indices for the observations, and returns the intercept and slope estimates for the linear regression model. We then apply this function to the full set of \\(392\\) observations in order to compute the estimates of \\(\\beta_0\\) and \\(\\beta_1\\) on the entire data set using the usual linear regression coefficient estimate formulas from Chapter 3.\n\n# auto data used earlier in the notebook\nAuto = pd.read_csv(\"/content/drive/MyDrive/Lab/Data/Auto.csv\", index_col=0)\nAuto.head()\n\n\n  \n    \n      \n\n\n\n\n\n\nmpg\ncylinders\ndisplacement\nhorsepower\nweight\nacceleration\nyear\norigin\nname\n\n\n\n\n1\n18.0\n8\n307.0\n130\n3504\n12.0\n70\n1\nchevrolet chevelle malibu\n\n\n2\n15.0\n8\n350.0\n165\n3693\n11.5\n70\n1\nbuick skylark 320\n\n\n3\n18.0\n8\n318.0\n150\n3436\n11.0\n70\n1\nplymouth satellite\n\n\n4\n16.0\n8\n304.0\n150\n3433\n12.0\n70\n1\namc rebel sst\n\n\n5\n17.0\n8\n302.0\n140\n3449\n10.5\n70\n1\nford torino\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\ndef boot_fn(data, start_index, end_index):\n    m = LinearRegression(fit_intercept=True).fit(\n        data['horsepower'][start_index:end_index].values.reshape(-1, 1),\n        data['mpg'][start_index:end_index]\n    )\n    \n    return m.intercept_, m.coef_\n    \nboot_fn(Auto, 0, 392)\n\n(39.93586102117047, array([-0.15784473]))\n\n\n\nboot_fn(resample(Auto, replace=True, n_samples=392, random_state=0), 0, 392)\n\n(40.480438868243674, array([-0.16156162]))\n\n\nNext, we use the boot() function to compute the standard errors of 1,000 bootstrap estimates for the intercept and slope terms.\n\nrng = np.random.RandomState(0)\nbs_boot = {'t1': [], 't2': []}\n\nfor i in range(0, 1000):\n    est = boot_fn(resample(Auto, replace=True, n_samples=392, random_state=rng), 0, 392)\n    bs_boot['t1'].append(\n        est[0]\n    )\n    bs_boot['t2'].append(\n        est[1][0]\n    )\n\nt1_es = np.array(bs_boot['t1']).mean()\nt1_se = np.array(bs_boot['t1']).std()\nt2_es = np.array(bs_boot['t2']).mean()\nt2_se = np.array(bs_boot['t2']).std()\n\nprint('t1 bs estimate & se:', t1_es, t1_se)\nprint('t2 bs estimate & se:', t2_es, t2_se)\n\nt1 bs estimate & se: 39.996156672438836 0.8528864796025836\nt2 bs estimate & se: -0.158407507124534 0.0072655098695998096\n\n\nThis indicates that the bootstrap estimate for \\({\\rm SE}(\\hat{\\beta}_0)\\) is \\(0.85\\), and that the bootstrap estimate for \\({\\rm SE}(\\hat{\\beta}_1)\\) is \\(0.0073\\). As discussed in Section 3.1.2, standard formulas can be used to compute the standard errors for the regression coefficients in a linear model. These can be obtained using the summary() function.\n\n# for lets see what the model predicts\nols = smf.ols('mpg ~ horsepower', data=Auto).fit()\nols.summary().tables[1]\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n39.9359\n0.717\n55.660\n0.000\n38.525\n41.347\n\n\nhorsepower\n-0.1578\n0.006\n-24.489\n0.000\n-0.171\n-0.145\n\n\n\n\n\nThe standard error estimates for \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) obtained using the formulas from Section 3.1.2 are \\(0.717\\) for the intercept and \\(0.006\\) for the slope. Interestingly, these are somewhat different from the estimates obtained using the bootstrap. Does this indicate a problem with the bootstrap? In fact, it suggests the opposite. Recall that the standard formulas given in Equation 3.8 on page 66 rely on certain assumptions. For example, they depend on the unknown parameter \\(\\sigma^2\\), the noise variance. We then estimate \\(\\sigma^2\\) using the RSS. Now although the formulas for the standard errors do not rely on the linear model being correct, the estimate for \\(\\sigma^2\\) does.\nWe see in Figure 3.8 on page 91 that there is a non-linear relationship in the data, and so the residuals from a linear fit will be inflated, and so will \\(\\hat{\\sigma}^2\\). Secondly, the standard formulas assume (somewhat unrealistically) that the \\(x_i\\) are fixed, and all the variability comes from the variation in the errors \\(\\epsilon_i\\). The bootstrap approach does not rely on any of these assumptions, and so it is likely giving a more accurate estimate of the standard errors of \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) than is the summary() function.\nBelow we compute the bootstrap standard error estimates and the standard linear regression estimates that result from fitting the quadratic model to the data. Since this model provides a good fit to the data (Figure 3.8), there is now a better correspondence between the bootstrap estimates and the standard estimates of \\({\\rm SE}(\\hat{\\beta}_0)\\), \\({\\rm SE}(\\hat{\\beta}_1)\\) and \\({\\rm SE}(\\hat{\\beta}_2)\\).\n\ndef boot_fn2(data, start_index, end_index):\n    m = LinearRegression(fit_intercept=True).fit(\n        data[['horsepower', 'horsepower_2']][start_index:end_index],\n        data['mpg'][start_index:end_index]\n    )\n    \n    return m.intercept_, m.coef_\n\n\nAuto['horsepower_2'] = np.power(Auto.horsepower, 2)\n\n\nrng = np.random.RandomState(0)\n\nbs_boot2 = {'t1': [], 't2': [], 't3': []}\n\nfor i in range(0, 1000):\n    est = boot_fn2(resample(Auto, replace=True, n_samples=392, random_state=rng), 0, 392)\n    bs_boot2['t1'].append(\n        est[0]\n    )\n    bs_boot2['t2'].append(\n        est[1][0]\n    )\n    bs_boot2['t3'].append(\n        est[1][1]\n    )\n\n\nt1_es = np.array(bs_boot2['t1']).mean()\nt1_se = np.array(bs_boot2['t1']).std()\nt2_es = np.array(bs_boot2['t2']).mean()\nt2_se = np.array(bs_boot2['t2']).std()\nt3_es = np.array(bs_boot2['t3']).mean()\nt3_se = np.array(bs_boot2['t3']).std()\n\nprint('t1 bs estimate & se:', t1_es, t1_se)\nprint('t2 bs estimate & se:', t2_es, t2_se)\nprint('t3 bs estimate & se:', t3_es, t3_se)\n\n\n# for lets see what the model predicts\nols2 = smf.ols('mpg ~ horsepower + horsepower_2', data=Auto).fit()\nols2.summary().tables[1]\n\nt1 bs estimate & se: 56.95462099069789 2.0847008918695753\nt2 bs estimate & se: -0.46692993431680474 0.03307843231128892\nt3 bs estimate & se: 0.0012328331475612475 0.00011942814562882365\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n56.9001\n1.800\n31.604\n0.000\n53.360\n60.440\n\n\nhorsepower\n-0.4662\n0.031\n-14.978\n0.000\n-0.527\n-0.405\n\n\nhorsepower_2\n0.0012\n0.000\n10.080\n0.000\n0.001\n0.001"
  },
  {
    "objectID": "Chapter_5_Lab.html#cross-validation-in-sklearn",
    "href": "Chapter_5_Lab.html#cross-validation-in-sklearn",
    "title": "5  Resampling methods",
    "section": "5.5 Cross validation in sklearn",
    "text": "5.5 Cross validation in sklearn\nhttps://scikit-learn.org/stable/modules/cross_validation.html.\n\ncv_error2 = []\n\n\nfor i in tqdm(range(1,11)):\n    loocv_mse2 = []\n    for train_index, test_index in loocv.split(X):\n        X_train, X_test = X[train_index], X[test_index]\n        y_train, y_test = y[train_index], y[test_index]\n        poly = PolynomialFeatures(i)\n        poly.fit(X_train) \n            \n        X_train_2 = poly.transform(X_train)\n        X_test_2 = poly.transform(X_test)\n\n        lm2 = sm.OLS(y_train, X_train_2)\n            \n        lm2_fit = lm2.fit()\n        lm2_predict = lm2_fit.predict(X_test_2)\n            \n        loocv_mse2.append(metrics.mean_squared_error(y_test, lm2_predict))\n    cv_error2.append(np.array(loocv_mse2).mean())\n\n100%|██████████| 10/10 [00:03&lt;00:00,  3.00it/s]\n\n\n\ncv_error2\n\n[24.23151351792922,\n 19.24821312448954,\n 19.334984064022773,\n 19.424430290207603,\n 19.03320647959501,\n 19.00693693541756,\n 18.995137417415865,\n 23.214193821020206,\n 38.92432074370519,\n 75.92002510349722]"
  },
  {
    "objectID": "Chapter_6_Lab.html#subset-selection-methods",
    "href": "Chapter_6_Lab.html#subset-selection-methods",
    "title": "6  Linear Models and Regularization Methods",
    "section": "6.1 Subset Selection Methods",
    "text": "6.1 Subset Selection Methods\nSome of the commands in this lab may take a while to run on your computer.\n\n6.1.1 Best Subset Selection\nHere we apply the best subset selection approach to the Hitters data. We wish to predict a baseball player’s Salary on the basis of various statistics associated with performance in the previous year.\n\n!pip install mlxtend --upgrade --no-deps -qq\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nimport itertools\n\nimport statsmodels.formula.api as smf\nfrom mlxtend.feature_selection import ExhaustiveFeatureSelector as EFS\nfrom mlxtend.feature_selection import SequentialFeatureSelector as SFS\n#import glmnet as gln\n\n#from sklearn.feature_selection import SequentialFeatureSelector \nfrom sklearn.model_selection import train_test_split,KFold, cross_val_score\nfrom sklearn.preprocessing import scale, StandardScaler \nfrom sklearn.linear_model import LinearRegression, Ridge, RidgeCV, Lasso, LassoCV\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.cross_decomposition import PLSRegression, PLSSVD\n\n%matplotlib inline\nplt.style.use('seaborn-white')\nsns.set_context(\"notebook\", font_scale=1.5, rc={\"lines.linewidth\": 2.5})\n\nFirst of all, we note that the Salary variable is missing for some of the players. The isnull() function can be used to identify the missing observations. It returns a vector of the same length as the input vector, with a True value for any elements that are missing, and a False value for non-missing elements. The sum() function can then be used to count all of the missing elements.\n\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\nDrive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n\n\n\nHitter = pd.read_csv(\"/content/drive/MyDrive/Lab/Data/Hitters.csv\")\nHitter.head()\n\n\n  \n    \n      \n\n\n\n\n\n\nAtBat\nHits\nHmRun\nRuns\nRBI\nWalks\nYears\nCAtBat\nCHits\nCHmRun\nCRuns\nCRBI\nCWalks\nLeague\nDivision\nPutOuts\nAssists\nErrors\nSalary\nNewLeague\n\n\n\n\n0\n293\n66\n1\n30\n29\n14\n1\n293\n66\n1\n30\n29\n14\nA\nE\n446\n33\n20\nNaN\nA\n\n\n1\n315\n81\n7\n24\n38\n39\n14\n3449\n835\n69\n321\n414\n375\nN\nW\n632\n43\n10\n475.0\nN\n\n\n2\n479\n130\n18\n66\n72\n76\n3\n1624\n457\n63\n224\n266\n263\nA\nW\n880\n82\n14\n480.0\nA\n\n\n3\n496\n141\n20\n65\n78\n37\n11\n5628\n1575\n225\n828\n838\n354\nN\nE\n200\n11\n3\n500.0\nN\n\n\n4\n321\n87\n10\n39\n42\n30\n2\n396\n101\n12\n48\n46\n33\nN\nE\n805\n40\n4\n91.5\nN\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\nprint(Hitter[\"Salary\"].isnull().sum())\n\n59\n\n\n\n# Print the dimensions of the original Hitters data (322 rows x 20 columns)\nprint(Hitter.shape)\n\n# Drop any rows the contain missing values\nHitter = Hitter.dropna()\n\n# Print the dimensions of the modified Hitters data (263 rows x 20 columns)\nprint(Hitter.shape)\n\n# One last check: should return 0\nprint(Hitter[\"Salary\"].isnull().sum())\n\n# Create dummy variables for qualitative features, since sklearn will not create dummy variables for categorical variables\nqual = ['League', 'Division', 'NewLeague']\nHitter = pd.get_dummies(Hitter, columns=qual, drop_first=True)\n\nHitter.head()\n\n(322, 20)\n(263, 20)\n0\n\n\n\n  \n    \n      \n\n\n\n\n\n\nAtBat\nHits\nHmRun\nRuns\nRBI\nWalks\nYears\nCAtBat\nCHits\nCHmRun\nCRuns\nCRBI\nCWalks\nPutOuts\nAssists\nErrors\nSalary\nLeague_N\nDivision_W\nNewLeague_N\n\n\n\n\n1\n315\n81\n7\n24\n38\n39\n14\n3449\n835\n69\n321\n414\n375\n632\n43\n10\n475.0\n1\n1\n1\n\n\n2\n479\n130\n18\n66\n72\n76\n3\n1624\n457\n63\n224\n266\n263\n880\n82\n14\n480.0\n0\n1\n0\n\n\n3\n496\n141\n20\n65\n78\n37\n11\n5628\n1575\n225\n828\n838\n354\n200\n11\n3\n500.0\n1\n0\n1\n\n\n4\n321\n87\n10\n39\n42\n30\n2\n396\n101\n12\n48\n46\n33\n805\n40\n4\n91.5\n1\n0\n1\n\n\n5\n594\n169\n4\n74\n51\n35\n11\n4408\n1133\n19\n501\n336\n194\n282\n421\n25\n750.0\n0\n1\n0\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nThe itertools.combinations function can help us to generate all possible combinations contains exactly k element within from a list of elements. We can then perform best subset selection by identifying the best model that contains a given number of predictors, where best is quantified using RSS. The following code implement the first stage of best subset selection.\n\nlist(itertools.combinations([1,2,3,4], 3))\n\n[(1, 2, 3), (1, 2, 4), (1, 3, 4), (2, 3, 4)]\n\n\n\ndef get_models(k_features, X, y):\n    \"\"\"\n    Fit all possible models that contain exactly k_features predictors.\n    X is predictor and y is target or response.\n    \"\"\"\n    \n    n_features = X.shape[1]\n    \n    X_combos = itertools.combinations(list(X.columns), k_features)\n    \n    best_score = np.inf\n    \n    for X_label in X_combos:\n        X_smf = ' + '.join(X_label)\n        f     = 'Salary ~ {}'.format(X_smf)\n        # Fit model\n        lin_reg = smf.ols(formula=f, data=pd.concat([X, y], axis=1)).fit()\n        score = lin_reg.ssr\n        if score &lt; best_score:\n            best_score, best_subset = score, X_label\n            best_reg = lin_reg\n    \n\n    return best_score, best_reg, best_subset\n\nWhich model with 2 predictors yields lowest RSS score?\n\nX = Hitter.drop('Salary', axis=1) #19 variables remain\ny = Hitter['Salary'] # put Salary as response variable\n\n# Set number for predictors in subset\nk = 2\n\n# Get best models in subset\nsubset = get_models(2, X, y)\n\n# Display results\nprint('This model yields the lowest RSS score for the subset of models with {} predictors:'.format(k))\nprint(subset)\n\nThis model yields the lowest RSS score for the subset of models with 2 predictors:\n(30646559.890372835, &lt;statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x7f81074d46d0&gt;, ('Hits', 'CRBI'))\n\n\nFirst we fit all possible models in each subset of models with \\(k\\) predictors. This turns out to be a very computationally expensive process as number of possible combinations without repetition is given by:\n\\(\\frac{p!}{k!(p-k)!}\\)\nWhere \\(p\\) is number of predictors to choose from and we choose \\(k\\) of them.\n\n# get all model results\nrss = []\nsub = []\nreg = []\nkft = []\n\nfor i in range(1,5):\n    best_score, best_reg, best_subset = get_models(i, X, y)\n    \n    rss.append(best_score)\n    sub.append(best_subset)\n    reg.append(best_reg)\n    kft.append(i)\n    print('Progess: i = {}, done'.format(i))\n\nProgess: i = 1, done\nProgess: i = 2, done\nProgess: i = 3, done\nProgess: i = 4, done\n\n\n\nresults = pd.DataFrame({'kft': kft, 'rss': rss, 'reg': reg, 'sub': sub},\n                           columns = ['rss', 'reg', 'sub', 'kft']).set_index('kft')\n\n\nresults\n\n\n  \n    \n      \n\n\n\n\n\n\nrss\nreg\nsub\n\n\nkft\n\n\n\n\n\n\n\n1\n3.617968e+07\n&lt;statsmodels.regression.linear_model.Regressio...\n(CRBI,)\n\n\n2\n3.064656e+07\n&lt;statsmodels.regression.linear_model.Regressio...\n(Hits, CRBI)\n\n\n3\n2.924930e+07\n&lt;statsmodels.regression.linear_model.Regressio...\n(Hits, CRBI, PutOuts)\n\n\n4\n2.797085e+07\n&lt;statsmodels.regression.linear_model.Regressio...\n(Hits, CRBI, PutOuts, Division_W)\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nWe can then choose the best model using different criteria.\n\nplt.figure(figsize=(15,9))\nplt.rcParams.update({'font.size': 14, 'lines.markersize': 10})\n\n# Set up a 2x2 grid so we can look at 4 plots at once\nplt.subplot(2, 2, 1)\n\n# We will now plot a red dot to indicate the model with the largest adjusted R^2 statistic.\n# The argmax() function can be used to identify the location of the maximum point of a vector\nax = sns.lineplot(x = \"kft\", y = \"rss\", data = results)\nax.set_xlabel('# Predictors')\nax.set_ylabel('RSS')\n\n# We will now plot a red dot to indicate the model with the largest adjusted R^2 statistic.\n# The argmax() function can be used to identify the location of the maximum point of a vector\n\nresults[\"rsquared_adj\"] = results.apply(lambda row: row[1].rsquared_adj, axis=1)\n\nplt.subplot(2, 2, 2)\nax = sns.lineplot(x = \"kft\", y = \"rsquared_adj\", data = results)\nplt.plot(results[\"rsquared_adj\"].argmax()+1, results[\"rsquared_adj\"].max(), \"or\")\nax.set_xlabel('# Predictors')\nax.set_ylabel('adjusted rsquared')\n\n# We'll do the same for AIC and BIC, this time looking for the models with the SMALLEST statistic\nresults[\"aic\"] = results.apply(lambda row: row[1].aic, axis=1)\n\nplt.subplot(2, 2, 3)\nax = sns.lineplot(x = \"kft\", y = \"aic\", data = results)\nplt.plot(results[\"aic\"].argmin()+1, results[\"aic\"].min(), \"or\")\nax.set_xlabel('# Predictors')\nax.set_ylabel('AIC')\n\nresults[\"bic\"] = results.apply(lambda row: row[1].bic, axis=1)\n\nplt.subplot(2, 2, 4)\nax = sns.lineplot(x = \"kft\", y = \"bic\", data = results)\nplt.plot(results[\"bic\"].argmin()+1, results[\"bic\"].min(), \"or\")\nax.set_xlabel('# Predictors')\nax.set_ylabel('BIC')\n\nText(0, 0.5, 'BIC')\n\n\n\n\n\nThe package mlxtend can help us to perform feature selection, it internally use cross-validation to estimate test errors for all possible combinations of features instead of dividing the procedure into two stage\n\n# Perform an Exhaustive Search. The EFS and SFS packages use 'neg_mean_squared_error'. The 'mean_squared_error' seems to have been deprecated. I think this is just the MSE with the a negative sign.\nlr = LinearRegression()\nefs1 = EFS(lr, \n           min_features=1,\n           max_features=4,\n           scoring='neg_mean_squared_error',\n           print_progress=True,\n           n_jobs=-1,\n           cv=5) #5-fold\n\n\nefs1.fit(X, y)\n\nFeatures: 5035/5035\n\n\nExhaustiveFeatureSelector(estimator=LinearRegression(),\n                          feature_groups=[[0], [1], [2], [3], [4], [5], [6],\n                                          [7], [8], [9], [10], [11], [12], [13],\n                                          [14], [15], [16], [17], [18]],\n                          max_features=4, n_jobs=-1,\n                          scoring='neg_mean_squared_error')\n\n\n\nefs1.best_feature_names_\n\n('AtBat', 'Hits', 'Walks', 'CRBI')\n\n\n\n## This is a time consuming process, be careful. You may consider setting --ServerApp.iopub_msg_rate_limit to a larger value or not to execute this cell\nr22 = []\nsub = []\nkft = []\nfor i in range(1,20):\n    efs1 = EFS(lr, \n           min_features=i,\n           max_features=i,\n           #scoring='neg_mean_squared_error', to calculate AIC, BIC you will need rss and the estimation of sigma (also from rss) https://xavierbourretsicotte.github.io/subset_selection.html\n           scoring='r2',\n           print_progress=True,\n           cv = 0, #no CV \n           n_jobs=-1) #parallelism\n    efs1.fit(X, y)\n    best_score, best_subset = efs1.best_score_, efs1.best_feature_names_\n    \n    r22.append(best_score)\n    sub.append(best_subset)\n    kft.append(i)\n    print('Progess: i = {}, done'.format(i))\n\nFeatures: 19/19\n\n\nProgess: i = 1, done\n\n\nFeatures: 171/171\n\n\nProgess: i = 2, done\n\n\nFeatures: 969/969\n\n\nProgess: i = 3, done\n\n\nFeatures: 3876/3876\n\n\nProgess: i = 4, done\n\n\nFeatures: 11628/11628\n\n\nProgess: i = 5, done\n\n\nFeatures: 27132/27132\n\n\nProgess: i = 6, done\n\n\nFeatures: 50388/50388\n\n\nProgess: i = 7, done\n\n\nFeatures: 75582/75582\n\n\nProgess: i = 8, done\n\n\nFeatures: 92378/92378\n\n\nProgess: i = 9, done\n\n\nFeatures: 92378/92378\n\n\nProgess: i = 10, done\n\n\nFeatures: 75582/75582\n\n\nProgess: i = 11, done\n\n\nFeatures: 50388/50388\n\n\nProgess: i = 12, done\n\n\nFeatures: 27132/27132\n\n\nProgess: i = 13, done\n\n\nFeatures: 11628/11628\n\n\nProgess: i = 14, done\n\n\nFeatures: 3876/3876\n\n\nProgess: i = 15, done\n\n\nFeatures: 969/969\n\n\nProgess: i = 16, done\n\n\nFeatures: 18/19\n\n\nProgess: i = 17, done\n\n\nFeatures: 1/1\n\n\nProgess: i = 18, done\nProgess: i = 19, done\n\n\n\nresults2 = pd.DataFrame({'kft': kft, 'r2': r22, 'sub': sub},\n                           columns = ['r2', 'sub', 'kft']).set_index('kft')\nresults2['sub']\n\nkft\n1                                               (CRBI,)\n2                                          (Hits, CRBI)\n3                                 (Hits, CRBI, PutOuts)\n4                     (Hits, CRBI, PutOuts, Division_W)\n5              (AtBat, Hits, CRBI, PutOuts, Division_W)\n6       (AtBat, Hits, Walks, CRBI, PutOuts, Division_W)\n7     (Hits, Walks, CAtBat, CHits, CHmRun, PutOuts, ...\n8     (AtBat, Hits, Walks, CHmRun, CRuns, CWalks, Pu...\n9     (AtBat, Hits, Walks, CAtBat, CRuns, CRBI, CWal...\n10    (AtBat, Hits, Walks, CAtBat, CRuns, CRBI, CWal...\n11    (AtBat, Hits, Walks, CAtBat, CRuns, CRBI, CWal...\n12    (AtBat, Hits, Runs, Walks, CAtBat, CRuns, CRBI...\n13    (AtBat, Hits, Runs, Walks, CAtBat, CRuns, CRBI...\n14    (AtBat, Hits, HmRun, Runs, Walks, CAtBat, CRun...\n15    (AtBat, Hits, HmRun, Runs, Walks, CAtBat, CHit...\n16    (AtBat, Hits, HmRun, Runs, RBI, Walks, CAtBat,...\n17    (AtBat, Hits, HmRun, Runs, RBI, Walks, CAtBat,...\n18    (AtBat, Hits, HmRun, Runs, RBI, Walks, Years, ...\n19    (AtBat, Hits, HmRun, Runs, RBI, Walks, Years, ...\nName: sub, dtype: object\n\n\n\ndef adjust_r2(r2, num_examples, num_features):\n    coef = (num_examples - 1) / (num_examples - num_features - 1) \n    return 1 - (1 - r2) * coef\n\n\nadj_r2 = []\nfor i, r2 in enumerate(r22):\n    adj_r2.append(adjust_r2(r2=r2,\n                  num_examples=X.shape[0],\n                  num_features=i+1))\n\n\nadj_r2 = np.array(adj_r2)\nsns.lineplot(x=range(1,20), y=adj_r2)\nplt.plot(adj_r2.argmax()+1, adj_r2.max(), \"or\")\nplt.xticks(range(1,20))\nplt.xlabel('# Predictors')\nplt.ylabel('adjusted rsquared')\n\nText(0, 0.5, 'adjusted rsquared')\n\n\n\n\n\nAccording to the adjusted \\(R^2\\), we see that the eleven-variable model is the best model. The predictor associated with the model is listed below.\n\nresults2['sub'][11]\n\n('AtBat',\n 'Hits',\n 'Walks',\n 'CAtBat',\n 'CRuns',\n 'CRBI',\n 'CWalks',\n 'PutOuts',\n 'Assists',\n 'League_N',\n 'Division_W')\n\n\n\n\n6.1.2 Forward and Backward Stepwise Selection\nIn practice, the best subset is quite time-consuming, therefore forward or backward selection is usually prefered. Note that we do not consider null model below.\n\ndef processSubset(X_label):\n    # Fit model on feature_set and calculate RSS\n    X_smf = ' + '.join(X_label)\n    f     = 'Salary ~ {}'.format(X_smf)\n    # Fit model\n    lin_reg = smf.ols(formula=f, data=pd.concat([X, y], axis=1)).fit()\n    RSS = lin_reg.ssr\n    return {'model': lin_reg, 'RSS': RSS}\n\n\ndef forward(predictors):\n\n    # Pull out predictors we still need to process\n    remaining_predictors = [p for p in X.columns if p not in predictors]\n      \n    results = []\n    \n    for p in remaining_predictors:\n        results.append(processSubset(predictors+[p]))\n    \n    # Wrap everything up in a nice dataframe\n    models = pd.DataFrame(results)\n    \n    # Choose the model with the lowest RSS\n    best_model = models.loc[models['RSS'].argmin()]\n    \n    # Return the best model, along with some other useful information about the model\n    return best_model\n\ndef backward(predictors):\n    \n    results = []\n    \n    for combo in itertools.combinations(predictors, len(predictors)-1):\n        results.append(processSubset(combo))\n    \n    # Wrap everything up in a nice dataframe\n    models = pd.DataFrame(results)\n    \n    # Choose the model with the lowest RSS\n    best_model = models.loc[models['RSS'].argmin()]\n\n    return best_model\n\n\nmodels2 = pd.DataFrame(columns=['RSS', 'model'])\n\npredictors = [] # we start with null model M0\n\nfor i in range(1,len(X.columns)+1):\n    models2.loc[i] = forward(predictors)\n    exog = models2.loc[i]['model'].model.exog_names.copy()\n    exog.remove('Intercept') #smf will automatically adds intercept\n    predictors = exog\n    print(i, predictors)\n\n1 ['CRBI']\n2 ['CRBI', 'Hits']\n3 ['CRBI', 'Hits', 'PutOuts']\n4 ['CRBI', 'Hits', 'PutOuts', 'Division_W']\n5 ['CRBI', 'Hits', 'PutOuts', 'Division_W', 'AtBat']\n6 ['CRBI', 'Hits', 'PutOuts', 'Division_W', 'AtBat', 'Walks']\n7 ['CRBI', 'Hits', 'PutOuts', 'Division_W', 'AtBat', 'Walks', 'CWalks']\n8 ['CRBI', 'Hits', 'PutOuts', 'Division_W', 'AtBat', 'Walks', 'CWalks', 'CRuns']\n9 ['CRBI', 'Hits', 'PutOuts', 'Division_W', 'AtBat', 'Walks', 'CWalks', 'CRuns', 'CAtBat']\n10 ['CRBI', 'Hits', 'PutOuts', 'Division_W', 'AtBat', 'Walks', 'CWalks', 'CRuns', 'CAtBat', 'Assists']\n11 ['CRBI', 'Hits', 'PutOuts', 'Division_W', 'AtBat', 'Walks', 'CWalks', 'CRuns', 'CAtBat', 'Assists', 'League_N']\n12 ['CRBI', 'Hits', 'PutOuts', 'Division_W', 'AtBat', 'Walks', 'CWalks', 'CRuns', 'CAtBat', 'Assists', 'League_N', 'Runs']\n13 ['CRBI', 'Hits', 'PutOuts', 'Division_W', 'AtBat', 'Walks', 'CWalks', 'CRuns', 'CAtBat', 'Assists', 'League_N', 'Runs', 'Errors']\n14 ['CRBI', 'Hits', 'PutOuts', 'Division_W', 'AtBat', 'Walks', 'CWalks', 'CRuns', 'CAtBat', 'Assists', 'League_N', 'Runs', 'Errors', 'HmRun']\n15 ['CRBI', 'Hits', 'PutOuts', 'Division_W', 'AtBat', 'Walks', 'CWalks', 'CRuns', 'CAtBat', 'Assists', 'League_N', 'Runs', 'Errors', 'HmRun', 'CHits']\n16 ['CRBI', 'Hits', 'PutOuts', 'Division_W', 'AtBat', 'Walks', 'CWalks', 'CRuns', 'CAtBat', 'Assists', 'League_N', 'Runs', 'Errors', 'HmRun', 'CHits', 'RBI']\n17 ['CRBI', 'Hits', 'PutOuts', 'Division_W', 'AtBat', 'Walks', 'CWalks', 'CRuns', 'CAtBat', 'Assists', 'League_N', 'Runs', 'Errors', 'HmRun', 'CHits', 'RBI', 'NewLeague_N']\n18 ['CRBI', 'Hits', 'PutOuts', 'Division_W', 'AtBat', 'Walks', 'CWalks', 'CRuns', 'CAtBat', 'Assists', 'League_N', 'Runs', 'Errors', 'HmRun', 'CHits', 'RBI', 'NewLeague_N', 'Years']\n19 ['CRBI', 'Hits', 'PutOuts', 'Division_W', 'AtBat', 'Walks', 'CWalks', 'CRuns', 'CAtBat', 'Assists', 'League_N', 'Runs', 'Errors', 'HmRun', 'CHits', 'RBI', 'NewLeague_N', 'Years', 'CHmRun']\n\n\nWe can then use AIC or BIC to choose the best model in the second stage\n\nbic_f = []\n\nfor m in models2.model:\n    bic_f.append(m.bic)\n    \nnp.array(bic_f).argmin()\n\n5\n\n\n\nmodels3 = pd.DataFrame(columns=['RSS', 'model'], index = range(1, len(X.columns)))\n\npredictors = X.columns # we start with full model Mp\nmodels3.loc[len(predictors)] = processSubset(predictors)\n\nwhile(len(predictors) &gt; 1):  \n    models3.loc[len(predictors)-1] = backward(predictors)\n    exog = models3.loc[len(predictors)-1]['model'].model.exog_names.copy()\n    exog.remove('Intercept')\n    predictors = exog\n    print(len(predictors), predictors)\n\n18 ['AtBat', 'Hits', 'HmRun', 'Runs', 'RBI', 'Walks', 'Years', 'CAtBat', 'CHits', 'CRuns', 'CRBI', 'CWalks', 'PutOuts', 'Assists', 'Errors', 'League_N', 'Division_W', 'NewLeague_N']\n17 ['AtBat', 'Hits', 'HmRun', 'Runs', 'RBI', 'Walks', 'CAtBat', 'CHits', 'CRuns', 'CRBI', 'CWalks', 'PutOuts', 'Assists', 'Errors', 'League_N', 'Division_W', 'NewLeague_N']\n16 ['AtBat', 'Hits', 'HmRun', 'Runs', 'RBI', 'Walks', 'CAtBat', 'CHits', 'CRuns', 'CRBI', 'CWalks', 'PutOuts', 'Assists', 'Errors', 'League_N', 'Division_W']\n15 ['AtBat', 'Hits', 'HmRun', 'Runs', 'Walks', 'CAtBat', 'CHits', 'CRuns', 'CRBI', 'CWalks', 'PutOuts', 'Assists', 'Errors', 'League_N', 'Division_W']\n14 ['AtBat', 'Hits', 'HmRun', 'Runs', 'Walks', 'CAtBat', 'CRuns', 'CRBI', 'CWalks', 'PutOuts', 'Assists', 'Errors', 'League_N', 'Division_W']\n13 ['AtBat', 'Hits', 'Runs', 'Walks', 'CAtBat', 'CRuns', 'CRBI', 'CWalks', 'PutOuts', 'Assists', 'Errors', 'League_N', 'Division_W']\n12 ['AtBat', 'Hits', 'Runs', 'Walks', 'CAtBat', 'CRuns', 'CRBI', 'CWalks', 'PutOuts', 'Assists', 'League_N', 'Division_W']\n11 ['AtBat', 'Hits', 'Walks', 'CAtBat', 'CRuns', 'CRBI', 'CWalks', 'PutOuts', 'Assists', 'League_N', 'Division_W']\n10 ['AtBat', 'Hits', 'Walks', 'CAtBat', 'CRuns', 'CRBI', 'CWalks', 'PutOuts', 'Assists', 'Division_W']\n9 ['AtBat', 'Hits', 'Walks', 'CAtBat', 'CRuns', 'CRBI', 'CWalks', 'PutOuts', 'Division_W']\n8 ['AtBat', 'Hits', 'Walks', 'CRuns', 'CRBI', 'CWalks', 'PutOuts', 'Division_W']\n7 ['AtBat', 'Hits', 'Walks', 'CRuns', 'CWalks', 'PutOuts', 'Division_W']\n6 ['AtBat', 'Hits', 'Walks', 'CRuns', 'PutOuts', 'Division_W']\n5 ['AtBat', 'Hits', 'Walks', 'CRuns', 'PutOuts']\n4 ['AtBat', 'Hits', 'CRuns', 'PutOuts']\n3 ['Hits', 'CRuns', 'PutOuts']\n2 ['Hits', 'CRuns']\n1 ['CRuns']\n\n\n\nbic_b = []\n\nfor m in models3.model:\n    bic_b.append(m.bic)\n    \nnp.array(bic_b).argmin()\n\n7\n\n\nWe see that using forward stepwise selection, the best one-variable model contains only CRBI, and the best two-variable model additionally includes Hits. For this data, the best one-variable through six-variable models are each identical for best subset and forward selection. However, the best seven-variable models identified by forward stepwise selection, backward stepwise selection, and best subset selection are different.\n\nprint(results2['sub'][6]) #best subset\nprint(models2['model'][6].model.exog_names[1:]) # Do not print Intercept\n\n('AtBat', 'Hits', 'Walks', 'CRBI', 'PutOuts', 'Division_W')\n['CRBI', 'Hits', 'PutOuts', 'Division_W', 'AtBat', 'Walks']\n\n\n\nprint(results2['sub'][7])\nprint(models2['model'][7].model.exog_names[1:]) # Do not print Intercept\nprint(models3['model'][7].model.exog_names[1:])\n\n('Hits', 'Walks', 'CAtBat', 'CHits', 'CHmRun', 'PutOuts', 'Division_W')\n['CRBI', 'Hits', 'PutOuts', 'Division_W', 'AtBat', 'Walks', 'CWalks']\n['AtBat', 'Hits', 'Walks', 'CRuns', 'CWalks', 'PutOuts', 'Division_W']\n\n\nThere are also other feature selection methods available in sklearn https://scikit-learn.org/stable/modules/feature_selection.html and mlxtend http://rasbt.github.io/mlxtend/api_subpackages/mlxtend.feature_selection/\n\nlr = LinearRegression()\nsfs1 = SFS(lr, \n          k_features=19, \n          forward=True, \n          floating=False, \n          scoring='neg_mean_squared_error',\n          cv=5)\n\nsfs1.fit(X, y)\n\nSequentialFeatureSelector(estimator=LinearRegression(), k_features=(19, 19),\n                          scoring='neg_mean_squared_error')\n\n\n\npd.DataFrame.from_dict(sfs1.get_metric_dict()).T\n\n\n  \n    \n      \n\n\n\n\n\n\nfeature_idx\ncv_scores\navg_score\nfeature_names\nci_bound\nstd_dev\nstd_err\n\n\n\n\n1\n(11,)\n[-69552.92336716877, -213009.92207213468, -118...\n-142142.865462\n(CRBI,)\n74428.969884\n57908.267167\n28954.133583\n\n\n2\n(1, 11)\n[-53798.49669675372, -163254.80704040316, -104...\n-124277.839415\n(Hits, CRBI)\n67031.533473\n52152.810324\n26076.405162\n\n\n3\n(1, 11, 17)\n[-62163.905140527975, -153941.83393405456, -97...\n-120082.156746\n(Hits, CRBI, Division_W)\n59591.942149\n46364.555526\n23182.277763\n\n\n4\n(1, 11, 13, 17)\n[-65576.39003922763, -141278.46897398788, -825...\n-117217.983933\n(Hits, CRBI, PutOuts, Division_W)\n56263.230127\n43774.704486\n21887.352243\n\n\n5\n(0, 1, 11, 13, 17)\n[-64199.08545469596, -133171.50527159323, -836...\n-114112.221253\n(AtBat, Hits, CRBI, PutOuts, Division_W)\n54818.569767\n42650.709646\n21325.354823\n\n\n6\n(0, 1, 5, 11, 13, 17)\n[-60075.53412214324, -132306.39833204867, -765...\n-110162.261438\n(AtBat, Hits, Walks, CRBI, PutOuts, Division_W)\n57689.191989\n44884.151259\n22442.075629\n\n\n7\n(0, 1, 3, 5, 11, 13, 17)\n[-60330.25929006559, -132346.01741313486, -766...\n-110553.231464\n(AtBat, Hits, Runs, Walks, CRBI, PutOuts, Divi...\n58252.099546\n45322.112462\n22661.056231\n\n\n8\n(0, 1, 3, 5, 6, 11, 13, 17)\n[-64303.54034551623, -130892.95408839498, -745...\n-110934.554619\n(AtBat, Hits, Runs, Walks, Years, CRBI, PutOut...\n57911.784603\n45057.335875\n22528.667937\n\n\n9\n(0, 1, 3, 5, 6, 10, 11, 13, 17)\n[-63508.043800155414, -137971.21159579078, -75...\n-111236.70073\n(AtBat, Hits, Runs, Walks, Years, CRuns, CRBI,...\n57344.878076\n44616.263337\n22308.131669\n\n\n10\n(0, 1, 3, 5, 6, 10, 11, 12, 13, 17)\n[-75371.33424690235, -128223.28240695987, -742...\n-110021.54195\n(AtBat, Hits, Runs, Walks, Years, CRuns, CRBI,...\n52372.497258\n40747.582133\n20373.791066\n\n\n11\n(0, 1, 3, 5, 6, 10, 11, 12, 13, 16, 17)\n[-75554.06873028616, -129427.52046763463, -741...\n-110792.912735\n(AtBat, Hits, Runs, Walks, Years, CRuns, CRBI,...\n52633.998179\n40951.038733\n20475.519367\n\n\n12\n(0, 1, 3, 5, 6, 10, 11, 12, 13, 16, 17, 18)\n[-75588.46350075006, -130320.07532800487, -754...\n-111570.449036\n(AtBat, Hits, Runs, Walks, Years, CRuns, CRBI,...\n52803.480655\n41082.901857\n20541.450929\n\n\n13\n(0, 1, 2, 3, 5, 6, 10, 11, 12, 13, 16, 17, 18)\n[-75578.82866274133, -130098.48628552802, -797...\n-112677.57758\n(AtBat, Hits, HmRun, Runs, Walks, Years, CRuns...\n51485.083739\n40057.144267\n20028.572133\n\n\n14\n(0, 1, 2, 3, 4, 5, 6, 10, 11, 12, 13, 16, 17, 18)\n[-75574.5961843588, -132822.10224589257, -8117...\n-113601.744452\n(AtBat, Hits, HmRun, Runs, RBI, Walks, Years, ...\n51592.950752\n40141.068467\n20070.534233\n\n\n15\n(0, 1, 2, 3, 4, 5, 6, 10, 11, 12, 13, 15, 16, ...\n[-75795.22141743562, -132827.4310618751, -8116...\n-115593.410667\n(AtBat, Hits, HmRun, Runs, RBI, Walks, Years, ...\n53008.106168\n41242.107459\n20621.053729\n\n\n16\n(0, 1, 2, 3, 4, 5, 6, 10, 11, 12, 13, 14, 15, ...\n[-77460.55999200376, -130968.8896155133, -8015...\n-116290.756812\n(AtBat, Hits, HmRun, Runs, RBI, Walks, Years, ...\n51687.395454\n40214.549654\n20107.274827\n\n\n17\n(0, 1, 2, 3, 4, 5, 6, 7, 10, 11, 12, 13, 14, 1...\n[-74741.82159316978, -128751.90512264056, -799...\n-118642.76362\n(AtBat, Hits, HmRun, Runs, RBI, Walks, Years, ...\n56165.48438\n43698.654974\n21849.327487\n\n\n18\n(0, 1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 13, 14...\n[-75598.10118706549, -128381.8825323173, -7998...\n-118980.278688\n(AtBat, Hits, HmRun, Runs, RBI, Walks, Years, ...\n56546.558941\n43995.143928\n21997.571964\n\n\n19\n(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...\n[-76408.91409133923, -129937.76968228162, -799...\n-121136.310318\n(AtBat, Hits, HmRun, Runs, RBI, Walks, Years, ...\n59785.432226\n46515.09738\n23257.54869\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\n6.1.3 Choosing Among Models Using the Validation-Set Approach and Cross-Validation\nWe just saw that it is possible to choose among a set of models of different sizes using AIC, BIC, and adjusted \\(R^2\\) in the second stage. We will now consider how to do this using the validation set and cross-validation approaches.\nIn order for these approaches to yield accurate estimates of the test error, we must use only the training observations to perform all aspects of model-fitting—including variable selection. Therefore, the determination of which model of a given size is best must be made using only the training observations. This point is subtle but important. If the full data set is used to perform the best subset selection step, the validation set errors and cross-validation errors that we obtain will not be accurate estimates of the test error.\nIn order to use the validation set approach, we begin by splitting the observations into a training set and a test set. We also set a random seed so that the user will obtain the same training set/test set split.\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=1)\n\n\ndef processSubset(X_label, X_train, y_train, X_test, y_test):\n    # Fit model on feature_set and calculate RSS\n    X_smf = ' + '.join(X_label)\n    f     = 'Salary ~ {}'.format(X_smf)\n    # Fit model\n    lin_reg = smf.ols(formula=f, data=pd.concat([X_train, y_train], axis=1)).fit()\n    RSS = ((lin_reg.predict(X_test[list(X_label)]) - y_test) ** 2).sum() #on test set\n    return {'model': lin_reg, 'RSS': RSS}\n\n\ndef forward(predictors, X_train, y_train, X_test, y_test):\n\n    # Pull out predictors we still need to process\n    remaining_predictors = [p for p in X_train.columns if p not in predictors]\n    \n    results = []\n    \n    for p in remaining_predictors:\n        results.append(processSubset(predictors+[p], X_train, y_train, X_test, y_test))\n    \n    # Wrap everything up in a nice dataframe\n    models = pd.DataFrame(results)\n    \n    # Choose the model with the highest RSS\n    best_model = models.loc[models['RSS'].argmin()]\n    \n    # Return the best model, along with some other useful information about the model\n    return best_model\n\n\nmodels_test = pd.DataFrame(columns=['RSS', 'model'])\n\npredictors = []\n\nfor i in range(1,len(X.columns)+1):\n    models_test.loc[i] = forward(predictors, X_train, y_train, X_test, y_test)\n    exog = models_test.loc[i]['model'].model.exog_names.copy()\n    exog.remove('Intercept')\n    predictors = exog\n    print(i, predictors)\n\n1 ['CRuns']\n2 ['CRuns', 'Hits']\n3 ['CRuns', 'Hits', 'Walks']\n4 ['CRuns', 'Hits', 'Walks', 'CWalks']\n5 ['CRuns', 'Hits', 'Walks', 'CWalks', 'AtBat']\n6 ['CRuns', 'Hits', 'Walks', 'CWalks', 'AtBat', 'CHmRun']\n7 ['CRuns', 'Hits', 'Walks', 'CWalks', 'AtBat', 'CHmRun', 'Years']\n8 ['CRuns', 'Hits', 'Walks', 'CWalks', 'AtBat', 'CHmRun', 'Years', 'CAtBat']\n9 ['CRuns', 'Hits', 'Walks', 'CWalks', 'AtBat', 'CHmRun', 'Years', 'CAtBat', 'Assists']\n10 ['CRuns', 'Hits', 'Walks', 'CWalks', 'AtBat', 'CHmRun', 'Years', 'CAtBat', 'Assists', 'HmRun']\n11 ['CRuns', 'Hits', 'Walks', 'CWalks', 'AtBat', 'CHmRun', 'Years', 'CAtBat', 'Assists', 'HmRun', 'Runs']\n12 ['CRuns', 'Hits', 'Walks', 'CWalks', 'AtBat', 'CHmRun', 'Years', 'CAtBat', 'Assists', 'HmRun', 'Runs', 'League_N']\n13 ['CRuns', 'Hits', 'Walks', 'CWalks', 'AtBat', 'CHmRun', 'Years', 'CAtBat', 'Assists', 'HmRun', 'Runs', 'League_N', 'NewLeague_N']\n14 ['CRuns', 'Hits', 'Walks', 'CWalks', 'AtBat', 'CHmRun', 'Years', 'CAtBat', 'Assists', 'HmRun', 'Runs', 'League_N', 'NewLeague_N', 'Errors']\n15 ['CRuns', 'Hits', 'Walks', 'CWalks', 'AtBat', 'CHmRun', 'Years', 'CAtBat', 'Assists', 'HmRun', 'Runs', 'League_N', 'NewLeague_N', 'Errors', 'Division_W']\n16 ['CRuns', 'Hits', 'Walks', 'CWalks', 'AtBat', 'CHmRun', 'Years', 'CAtBat', 'Assists', 'HmRun', 'Runs', 'League_N', 'NewLeague_N', 'Errors', 'Division_W', 'CRBI']\n17 ['CRuns', 'Hits', 'Walks', 'CWalks', 'AtBat', 'CHmRun', 'Years', 'CAtBat', 'Assists', 'HmRun', 'Runs', 'League_N', 'NewLeague_N', 'Errors', 'Division_W', 'CRBI', 'RBI']\n18 ['CRuns', 'Hits', 'Walks', 'CWalks', 'AtBat', 'CHmRun', 'Years', 'CAtBat', 'Assists', 'HmRun', 'Runs', 'League_N', 'NewLeague_N', 'Errors', 'Division_W', 'CRBI', 'RBI', 'PutOuts']\n19 ['CRuns', 'Hits', 'Walks', 'CWalks', 'AtBat', 'CHmRun', 'Years', 'CAtBat', 'Assists', 'HmRun', 'Runs', 'League_N', 'NewLeague_N', 'Errors', 'Division_W', 'CRBI', 'RBI', 'PutOuts', 'CHits']\n\n\n\nsns.lineplot(x=range(1,20), y=models_test['RSS'])\nplt.xlabel('# Predictors')\nplt.ylabel('RSS')\nplt.plot(models_test['RSS'].argmin()+1, models_test['RSS'].min(), 'or')\n\n\n\n\nNow that we know what we’re looking for, let’s perform forward selection on the full dataset and select the best 7-predictor model. It is important that we make use of the full data set in order to obtain more accurate coefficient estimates. Note that we perform forward selection on the full data set and select the best 7-predictor model, rather than simply using the predictors that we obtained from the training set, because the best 7-predictor model on the full data set may differ from the corresponding model on the training set.\n\nmodels_full = pd.DataFrame(columns=['RSS', 'model'])\n\npredictors = []\n   \nfor i in range(1,8):\n    models_full.loc[i] = forward(predictors, X, y, X, y) #use full dataset\n    exog = models_full.loc[i]['model'].model.exog_names.copy()\n    exog.remove('Intercept')\n    predictors = exog\n    print(i, predictors)\n\n1 ['CRBI']\n2 ['CRBI', 'Hits']\n3 ['CRBI', 'Hits', 'PutOuts']\n4 ['CRBI', 'Hits', 'PutOuts', 'Division_W']\n5 ['CRBI', 'Hits', 'PutOuts', 'Division_W', 'AtBat']\n6 ['CRBI', 'Hits', 'PutOuts', 'Division_W', 'AtBat', 'Walks']\n7 ['CRBI', 'Hits', 'PutOuts', 'Division_W', 'AtBat', 'Walks', 'CWalks']\n\n\n\nprint(models_test.loc[7, 'model'].model.exog_names)\nprint(models_full.loc[7, 'model'].model.exog_names) # we will use this one as our final model!\n\n['Intercept', 'CRuns', 'Hits', 'Walks', 'CWalks', 'AtBat', 'CHmRun', 'Years']\n['Intercept', 'CRBI', 'Hits', 'PutOuts', 'Division_W', 'AtBat', 'Walks', 'CWalks']\n\n\n\n\n6.1.4 Model selection using Cross-Validation\nNow let’s try to choose among the models of different sizes using cross-validation. This approach is somewhat involved, as we must perform forward selection within each of the \\(k\\) training sets. Despite this, we see that with its clever subsetting syntax, python makes this job quite easy. First, we create a vector that assigns each observation to one of \\(k = 10\\) folds, and we create a DataFrame in which we will store the results:\n\nk = 10\nkf = KFold(n_splits=k, shuffle=True, random_state=2)\n\n\n# Create a DataFrame to store the results of our upcoming calculations\ncv_errors = pd.DataFrame(columns=range(1,k+1), index=range(1,20))\ncv_errors = cv_errors.fillna(0)\n\n\nmodels_cv = pd.DataFrame(columns=[\"RSS\", \"model\"])\n\nj = 0\n# Outer loop iterates over all folds\nfor train_index, test_index in kf.split(X):\n    j = j+1\n    # Reset predictors\n    predictors = []\n    \n    X_train2, X_test2 = X.iloc[train_index], X.iloc[test_index]\n    y_train2, y_test2 = y.iloc[train_index], y.iloc[test_index]\n    \n    # Inner loop iterates over each size i\n    for i in range(1,len(X.columns)+1):    \n    \n        # The perform forward selection on the full dataset minus the jth fold, test on jth fold\n        models_cv.loc[i] = forward(predictors, X_train2, y_train2, X_test2, y_test2)\n        \n        # Save the cross-validated error for this fold\n        cv_errors[j][i] = models_cv.loc[i][\"RSS\"]\n\n        exog = models_cv.loc[i]['model'].model.exog_names.copy()\n        exog.remove('Intercept')\n        predictors = exog\n\n\ncv_mean = cv_errors.apply(np.mean, axis=1)\n\nsns.lineplot(x=range(1,20), y=cv_mean)\nplt.xlabel('# Predictors')\nplt.ylabel('CV Error')\nplt.plot(cv_mean.argmin()+1, cv_mean.min(), \"or\")\n\n\n\n\nCross-validation suggest 8-predictor model is the best model. We now perform best subset selection on the full data set in order to obtain the 8-variable model.\n\nmodels_full = pd.DataFrame(columns=['RSS', 'model'])\n\npredictors = []\n   \nfor i in range(1,9):\n    models_full.loc[i] = forward(predictors, X, y, X, y) #use full dataset\n    exog = models_full.loc[i]['model'].model.exog_names.copy()\n    exog.remove('Intercept')\n    predictors = exog\n    print(i, predictors)\n\n1 ['CRBI']\n2 ['CRBI', 'Hits']\n3 ['CRBI', 'Hits', 'PutOuts']\n4 ['CRBI', 'Hits', 'PutOuts', 'Division_W']\n5 ['CRBI', 'Hits', 'PutOuts', 'Division_W', 'AtBat']\n6 ['CRBI', 'Hits', 'PutOuts', 'Division_W', 'AtBat', 'Walks']\n7 ['CRBI', 'Hits', 'PutOuts', 'Division_W', 'AtBat', 'Walks', 'CWalks']\n8 ['CRBI', 'Hits', 'PutOuts', 'Division_W', 'AtBat', 'Walks', 'CWalks', 'CRuns']\n\n\nThere are also other supports from http://rasbt.github.io/mlxtend/user_guide/feature_selection/SequentialFeatureSelector/ and https://scikit-learn.org/stable/modules/feature_selection.html#feature-selection which implement different selection strategies."
  },
  {
    "objectID": "Chapter_6_Lab.html#ridge-regression-and-the-lasso",
    "href": "Chapter_6_Lab.html#ridge-regression-and-the-lasso",
    "title": "6  Linear Models and Regularization Methods",
    "section": "6.2 Ridge Regression and the Lasso",
    "text": "6.2 Ridge Regression and the Lasso\nThe glmnet algorithms in R optimize the objective function using cyclical coordinate descent, while scikit-learn Ridge regression uses linear least squares with L2 regularization. They are rather different implementations, but the general principles are the same.\nThe glmnet() function in R optimizes: ### \\[ \\frac{1}{N}|| X\\beta-y||^2_2+\\lambda\\bigg(\\frac{1}{2}(1−\\alpha)||\\beta||^2_2 \\ +\\ \\alpha||\\beta||_1\\bigg) \\] (See R documentation and https://cran.r-project.org/web/packages/glmnet/vignettes/glmnet_beta.pdf) The function supports L1 and L2 regularization. For just Ridge regression we need to use $= 0 $. This reduces the above cost function to ### \\[ \\frac{1}{N}|| X\\beta-y||^2_2+\\frac{1}{2}\\lambda ||\\beta||^2_2 \\]\nIn python, you can also find similar functions in https://github.com/civisanalytics/python-glmnet\nThe sklearn Ridge() function on the other hand optimizes: ### \\[ ||X\\beta - y||^2_2 + \\alpha ||\\beta||^2_2 \\] which is equivalent to optimizing ### \\[ \\frac{1}{N}||X\\beta - y||^2_2 + \\frac{\\alpha}{N} ||\\beta||^2_2 \\]\nBy default the Ridge() function performs ridge regression for an automatically selected range of \\(\\alpha\\) (or \\(\\lambda\\) in the textbook) values. However, here we have chosen to implement the function over a grid of values ranging from \\(\\alpha=10^{10}\\) to \\(\\alpha=10^{-2}\\), essentially covering the full range of scenarios from the null model containing only the intercept, to the least squares fit. The following we plot 19 coefficients (excluding intercept) for 100 different alphas\n\nalphas = 10**np.linspace(10,-2,100)\n\nridge = Ridge()\ncoefs = []\n\nfor a in alphas:\n    ridge.set_params(alpha=a)\n    ridge.fit(scale(X), y) # We standardize features before ridge regression\n    coefs.append(ridge.coef_)\n\nax = plt.gca()\nax.plot(alphas, coefs)\nax.set_xscale('log')\nax.set_xlim(ax.get_xlim()[::-1])  # reverse axis\nplt.axis('tight')\nplt.xlabel('alpha')\nplt.ylabel('weights')\nplt.title('Ridge coefficients as a function of the regularization')\n\nText(0.5, 1.0, 'Ridge coefficients as a function of the regularization')\n\n\n\n\n\nThe above plot shows that the Ridge coefficients get larger when we decrease alpha. We now split the samples into a training set and a test set in order to estimate the test error of ridge regression and the lasso.\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=1)\n\nNote that if we had instead simply fit a model with just an intercept, we would have predicted each test observation using the mean of the training observations. In that case, we could compute the test set MSE like this:\n\nnp.mean((np.mean(y_train) - y_test)**2)\n\n172862.23592080915\n\n\nWe could also get similar result by fitting a ridge regression model with a very large value of \\(\\lambda\\). Note that 1e10 means \\(10^{10}\\). This big penalty shrinks the coefficients to a very large degree and makes the model more biased, resulting in a higher MSE.\n\nscaler = StandardScaler()\nscaler.fit(X_train)\n\nStandardScaler()\n\n\n\nmodel = Ridge()\nmodel.set_params(alpha=10**10)\nX_scale = scaler.transform(X_train)\nmodel.fit(X_scale, y_train)\nX_scale_t = scaler.transform(X_test)\npred = model.predict(X_scale_t)\nmean_squared_error(y_test, pred)\n\n172862.22059245987\n\n\nNext we fit a ridge regression model on the training set, and evaluate its MSE on the test set\n\nvalidation_score = []\nX_scale_t = scaler.transform(X_test)\nfor alpha in alphas:\n    model = Ridge(alpha=alpha)\n    model.fit(X_scale,y_train) #compare normalize=True vs standardize\n    validation_score.append(mean_squared_error(model.predict(X_scale_t),y_test))\n\n\nsns.lineplot(x=alphas,y=validation_score)\nplt.xscale('log')\n\n\n\n\n\nnp.argmin(validation_score)\n\n65\n\n\nTherefore, we see that the value of alpha that results in the smallest cross-validation error is the 64th. What is the test MSE associated with this value of alpha?\n\nalphas[65], validation_score[65]\n\n(132.19411484660287, 99557.32756698191)\n\n\nWe can also do built-in cross-validation using RidgeCV\n\nkf = KFold(n_splits=10, shuffle=True, random_state=2)\nridgecv = RidgeCV(alphas=alphas, scoring='neg_mean_squared_error', cv=kf)\nridgecv.fit(X_scale, y_train)\n\nRidgeCV(alphas=array([1.00000000e+10, 7.56463328e+09, 5.72236766e+09, 4.32876128e+09,\n       3.27454916e+09, 2.47707636e+09, 1.87381742e+09, 1.41747416e+09,\n       1.07226722e+09, 8.11130831e+08, 6.13590727e+08, 4.64158883e+08,\n       3.51119173e+08, 2.65608778e+08, 2.00923300e+08, 1.51991108e+08,\n       1.14975700e+08, 8.69749003e+07, 6.57933225e+07, 4.97702356e+07,\n       3.76493581e+07, 2.84803587e+0...\n       2.00923300e+00, 1.51991108e+00, 1.14975700e+00, 8.69749003e-01,\n       6.57933225e-01, 4.97702356e-01, 3.76493581e-01, 2.84803587e-01,\n       2.15443469e-01, 1.62975083e-01, 1.23284674e-01, 9.32603347e-02,\n       7.05480231e-02, 5.33669923e-02, 4.03701726e-02, 3.05385551e-02,\n       2.31012970e-02, 1.74752840e-02, 1.32194115e-02, 1.00000000e-02]),\n        cv=KFold(n_splits=10, random_state=2, shuffle=True),\n        scoring='neg_mean_squared_error')\n\n\n\nridgecv.alpha_\n\n100.0\n\n\n\nmodel.set_params(alpha=ridgecv.alpha_)\nmodel.fit(X_scale, y_train)\nmean_squared_error(y_test, model.predict(X_scale_t))\n\n99586.56834382795\n\n\n\n6.2.0.1 Fit model to full data set\n\nmodel.fit(scale(X), y)\n\nRidge(alpha=100.0)\n\n\n\npd.Series(model.coef_.flatten(), index=X.columns)\n\nAtBat          -0.006619\nHits           49.467498\nHmRun          -0.859718\nRuns           28.891970\nRBI            22.425519\nWalks          41.183554\nYears          -2.737654\nCAtBat         24.915049\nCHits          44.636902\nCHmRun         38.851734\nCRuns          45.225776\nCRBI           47.320463\nCWalks          3.558932\nPutOuts        56.671318\nAssists         7.335151\nErrors        -13.493719\nLeague_N       14.878188\nDivision_W    -48.564908\nNewLeague_N     2.817544\ndtype: float64\n\n\nAs expected, none of the coefficients are zero—ridge regression does not perform variable selection!\nYou can also fit the ridge regression with LOOCV, this should be efficient since their is corresponding shortcut equation\n\nridgeloocv = RidgeCV(alphas=alphas, scoring='neg_mean_squared_error', gcv_mode='auto')\nridgeloocv.fit(X_scale, y_train)\n\nRidgeCV(alphas=array([1.00000000e+10, 7.56463328e+09, 5.72236766e+09, 4.32876128e+09,\n       3.27454916e+09, 2.47707636e+09, 1.87381742e+09, 1.41747416e+09,\n       1.07226722e+09, 8.11130831e+08, 6.13590727e+08, 4.64158883e+08,\n       3.51119173e+08, 2.65608778e+08, 2.00923300e+08, 1.51991108e+08,\n       1.14975700e+08, 8.69749003e+07, 6.57933225e+07, 4.97702356e+07,\n       3.76493581e+07, 2.84803587e+0...\n       2.00923300e+00, 1.51991108e+00, 1.14975700e+00, 8.69749003e-01,\n       6.57933225e-01, 4.97702356e-01, 3.76493581e-01, 2.84803587e-01,\n       2.15443469e-01, 1.62975083e-01, 1.23284674e-01, 9.32603347e-02,\n       7.05480231e-02, 5.33669923e-02, 4.03701726e-02, 3.05385551e-02,\n       2.31012970e-02, 1.74752840e-02, 1.32194115e-02, 1.00000000e-02]),\n        gcv_mode='auto', scoring='neg_mean_squared_error')\n\n\n\nridgeloocv.alpha_\n\n75.64633275546291\n\n\n\nmodel.set_params(alpha=ridgeloocv.alpha_)\nmodel.fit(X_scale, y_train)\nmean_squared_error(y_test, model.predict(X_scale_t))\n\n99820.74305168974\n\n\n\nmodel.fit(scale(X), y)\n\nRidge(alpha=75.64633275546291)\n\n\n\n\n6.2.1 The Lasso\nWe saw that ridge regression with a wise choice of alpha may outperform least squares as well as the null model on the Hitters data set. We now ask whether the lasso can yield either a more accurate or a more interpretable model than ridge regression. The following we plot 19 coefficients (excluding intercept) for 100 different alphas.\n\nlasso = Lasso(max_iter=10000)\ncoefs = []\n\nfor a in alphas:\n    lasso.set_params(alpha=a)\n    lasso.fit(scale(X_train), y_train)\n    coefs.append(lasso.coef_)\n\nax = plt.gca()\nax.plot(alphas, coefs)\nax.set_xscale('log')\nax.set_xlim(ax.get_xlim()[::-1])  # reverse axis\nplt.axis('tight')\nplt.xlabel('alpha')\nplt.ylabel('weights')\nplt.title('Lasso coefficients as a function of the regularization');\n\n\n\n\nWe can see from the coefficient plot that depending on the choice of tuning parameter, some of the coefficients will be exactly equal to zero. We now perform cross-validation and compute the associated test error.\n\nkf = KFold(n_splits=10, shuffle=True, random_state=2)\nlassocv = LassoCV(alphas=None, cv=kf, max_iter=10000)\nlassocv.fit(X_scale, y_train.values.ravel())\n\nLassoCV(cv=KFold(n_splits=10, random_state=2, shuffle=True), max_iter=10000)\n\n\n\nlassocv.alpha_\n\n25.649654020305032\n\n\n\nlasso.set_params(alpha=lassocv.alpha_)\nlasso.fit(X_scale, y_train)\nmean_squared_error(y_test, lasso.predict(X_scale_t))\n\n104853.47279766494\n\n\n\n# fit the full model\nlasso.fit(scale(X), y)\n\nLasso(alpha=25.649654020305032, max_iter=10000)\n\n\n\n# Some of the coefficients are now reduced to exactly zero.\npd.Series(lasso.coef_, index=X.columns)\n\nAtBat            0.000000\nHits            80.723891\nHmRun            0.000000\nRuns             0.000000\nRBI              0.000000\nWalks           45.833055\nYears            0.000000\nCAtBat           0.000000\nCHits            0.000000\nCHmRun           0.000000\nCRuns           65.022016\nCRBI           129.889483\nCWalks           0.000000\nPutOuts         55.241441\nAssists         -0.000000\nErrors          -0.000000\nLeague_N         0.000000\nDivision_W     -43.731588\nNewLeague_N      0.000000\ndtype: float64\n\n\nThe test set MSE is very similar to the test MSE of ridge regression with \\(\\alpha\\) chosen by cross-validation.\nHowever, the lasso has a substantial advantage over ridge regression in that the resulting coefficient estimates are sparse. Here we see that 13 of the 19 coefficient estimates are exactly zero. So the lasso model with \\(\\alpha\\) chosen by cross-validation contains only six variables."
  },
  {
    "objectID": "Chapter_6_Lab.html#pcr-and-pls-regression",
    "href": "Chapter_6_Lab.html#pcr-and-pls-regression",
    "title": "6  Linear Models and Regularization Methods",
    "section": "6.3 PCR and PLS Regression",
    "text": "6.3 PCR and PLS Regression\n\n6.3.1 Principal Components Regression\nPrincipal components regression (PCR) can be performed using the pca function. We now apply PCR to the Hitters data, in order to predict Salary. Again, we ensure that the missing values have been removed from the data, as described in Section 6.5.1.\n\npca = PCA()\nX_reduced = pca.fit_transform(scale(X)) #standardize before PCA\n\nprint(pca.components_.shape)\npd.DataFrame(pca.components_.T).loc[:4,:5]\n\n(19, 19)\n\n\n\n  \n    \n      \n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n\n\n\n\n0\n0.198290\n-0.383784\n0.088626\n0.031967\n0.028117\n-0.070646\n\n\n1\n0.195861\n-0.377271\n0.074032\n0.017982\n-0.004652\n-0.082240\n\n\n2\n0.204369\n-0.237136\n-0.216186\n-0.235831\n0.077660\n-0.149646\n\n\n3\n0.198337\n-0.377721\n-0.017166\n-0.049942\n-0.038536\n-0.136660\n\n\n4\n0.235174\n-0.314531\n-0.073085\n-0.138985\n0.024299\n-0.111675\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\nprint(X_reduced.shape)\npd.DataFrame(X_reduced).loc[:4,:5]\n\n(263, 19)\n\n\n\n  \n    \n      \n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n\n\n\n\n0\n-0.009649\n1.870522\n1.265145\n-0.935481\n1.109636\n1.211972\n\n\n1\n0.411434\n-2.429422\n-0.909193\n-0.264212\n1.232031\n1.826617\n\n\n2\n3.466822\n0.825947\n0.555469\n-1.616726\n-0.857488\n-1.028712\n\n\n3\n-2.558317\n-0.230984\n0.519642\n-2.176251\n-0.820301\n1.491696\n\n\n4\n1.027702\n-1.573537\n1.331382\n3.494004\n0.983427\n0.513675\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n# Variance explained by the principal components\nnp.cumsum(np.round(pca.explained_variance_ratio_, decimals=4)*100)\n\narray([38.31, 60.15, 70.84, 79.03, 84.29, 88.63, 92.26, 94.96, 96.28,\n       97.25, 97.97, 98.64, 99.14, 99.46, 99.73, 99.88, 99.95, 99.98,\n       99.99])\n\n\n\nplt.plot(np.cumsum(pca.explained_variance_ratio_)) #scree plot\nplt.xlabel('number of components')\nplt.ylabel('cumulative explained variance');\n\n\n\n\n\n6.3.1.1 Fitting PCA with training data\nIn this method, we use the train/val/test split scheme like the lab in the textbook\n\npca2 = PCA()\nX_reduced_train = pca2.fit_transform(X_scale)\nn = len(X_reduced_train)\n\n# 10-fold CV, with shuffle\nkf_10 = KFold(n_splits=10, shuffle=True, random_state=2)\n\nmse = []\nregr = LinearRegression()\n# Calculate MSE with only the intercept (no principal components in regression) \nscore = -1*cross_val_score(regr, np.ones((n,1)), y_train, cv=kf_10, scoring='neg_mean_squared_error').mean()    \nmse.append(score)\n\n# Calculate MSE using CV for the 19 principle components, adding one component at the time.\nfor i in np.arange(1, 20):\n    score = -1*cross_val_score(regr, X_reduced_train[:,:i], y_train, cv=kf_10, scoring='neg_mean_squared_error').mean()\n    mse.append(score)\n\nplt.plot(np.arange(0, 20), np.array(mse), '-v')\nplt.xlabel('Number of principal components in regression')\nplt.ylabel('MSE')\nplt.title('Salary')\nplt.xlim(xmin=-1);\n\n\n\n\n\nnp.array(mse).argmin() #note mse contains no pc\n\n6\n\n\n\n\n6.3.1.2 Transform test data with PCA loadings and fit regression on 6 principal components\n\nX_reduced_test = pca2.transform(X_scale_t)[:,:6]\n\n# Train regression model on training data \nregr = LinearRegression()\nregr.fit(X_reduced_train[:,:6], y_train)\n\n# Prediction with test data\npred = regr.predict(X_reduced_test)\nmean_squared_error(y_test, pred)\n\n108195.77957462525\n\n\n\n# Fit regression model on the whole data \nregr = LinearRegression()\nregr.fit(X_reduced[:,:6], y)\n\nLinearRegression()\n\n\n\n\n\n6.3.2 Partial Least Squares\nScikit-learn PLSRegression gives same results as the pls package in R when using ‘method=’oscorespls’.\nSee documentation: http://scikit-learn.org/dev/modules/generated/sklearn.cross_decomposition.PLSRegression.html#sklearn.cross_decomposition.PLSRegression.\n\nn = len(X_train)\n\n# 10-fold CV, with shuffle\nkf_10 = KFold(n_splits=10, shuffle=True, random_state=2)\n\nmse = []\n\nscore = -1*cross_val_score(regr, np.ones((n,1)), y_train, cv=kf_10, scoring='neg_mean_squared_error').mean()    \nmse.append(score)\n\nfor i in np.arange(1, 20):\n    pls = PLSRegression(n_components=i)\n    score = -1*cross_val_score(pls, X_scale, y_train, cv=kf_10, scoring='neg_mean_squared_error').mean()\n    mse.append(score)\n\nplt.plot(np.arange(0, 20), np.array(mse), '-v')\nplt.xlabel('Number of principal components in regression')\nplt.ylabel('MSE')\nplt.title('Salary')\nplt.xlim(xmin=-1);\n\n\n\n\n\nnp.array(mse).argmin()\n\n1\n\n\nThe lowest cross-validation error occurs when only \\(M=1\\) partial least squares directions are used. We now evaluate the corresponding test set MSE.\n\npls = PLSRegression(n_components=1)\npls.fit(X_scale, y_train)\n\nmean_squared_error(y_test, pls.predict(X_scale_t))\n\n101462.13107757183"
  },
  {
    "objectID": "Chapter_7_Lab.html#setup",
    "href": "Chapter_7_Lab.html#setup",
    "title": "7  Non-linear Modeling",
    "section": "7.1 Setup",
    "text": "7.1 Setup\nIn this lab, we re-analyze the Wage data considered in the examples throughout this chapter, in order to illustrate the fact that many of the complex non-linear fitting procedures discussed can be easily implemented in Python.\n\n!pip install pygam\n\nLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\nCollecting pygam\n  Downloading pygam-0.8.0-py2.py3-none-any.whl (1.8 MB)\n     |████████████████████████████████| 1.8 MB 4.3 MB/s \nRequirement already satisfied: progressbar2 in /usr/local/lib/python3.7/dist-packages (from pygam) (3.38.0)\nRequirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pygam) (0.16.0)\nRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from pygam) (1.7.3)\nRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pygam) (1.21.6)\nRequirement already satisfied: python-utils&gt;=2.3.0 in /usr/local/lib/python3.7/dist-packages (from progressbar2-&gt;pygam) (3.3.3)\nRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from progressbar2-&gt;pygam) (1.15.0)\nInstalling collected packages: pygam\nSuccessfully installed pygam-0.8.0\n\n\n\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\n\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nfrom statsmodels.gam.api import GLMGam, BSplines\nfrom statsmodels.gam.generalized_additive_model import LogitGam\nfrom patsy import dmatrix\nimport patsy as pt\nfrom sklearn.preprocessing import PolynomialFeatures, SplineTransformer\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom typing import List\nfrom numpy.linalg import inv\nfrom scipy import optimize\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\nplt.style.use('seaborn-white')\nsns.set_context(\"notebook\", font_scale=1.5, rc={\"lines.linewidth\": 2.5})\n\n\nclass SmoothingSpline1D(BaseEstimator, TransformerMixin):\n# https://github.com/empathy87/The-Elements-of-Statistical-Learning-Python-Notebooks/blob/master/examples/Bone%20Mineral%20Density.ipynb  \n    \"\"\"One dimensional smoothing spline.\n    Parameters\n    ----------\n    dof :\n        The target effective degrees of of freedom of a smoothing spline.\n    Attributes\n    ----------\n    knots_:\n        Unique values of X training data.\n    smooth_:\n        The smoothing parameter that results in the target degrees of freedom.\n    coef_:\n        The vector of fitted coefficients for linear regression.\"\"\"\n    def __init__(self, dof: float):\n        self.dof = dof\n\n    def fit(self, X: np.ndarray, y: np.array) -&gt; 'SmoothingSpline1D':\n        \"\"\"Fit SmoothingSpline1D model according to the given training data\n           and parameters.\n        Parameters\n        ----------\n        X :\n            Training data.\n        y :\n            Target values.\n        \"\"\"\n        y = np.atleast_2d(y).T\n        self.knots_ = np.unique(X)\n        N = self.__expand_natural_cubic(X, self.knots_)\n        O = self.__calc_integral_matrix(self.knots_)\n        self.smooth_ = optimize.newton(\n            lambda s: self.__calc_dof(N, O, s) - self.dof,\n            0.5, maxiter=400)\n        self.coef_ = inv(N.T @ N + self.smooth_ * O) @ N.T @ y\n        self.Sl_ = self.__calc_Sl(N, O, self.smooth_)\n        return self\n\n    def transform(self, X: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Natural cubic spline basis expansion.\n        Parameters\n        ----------\n        X :\n            Input data.\n        Returns\n        -------\n        X_new :\n            Transformed data.\n        \"\"\"\n        return self.__expand_natural_cubic(X, self.knots_)\n\n    def predict(self, X: np.ndarray) -&gt; np.array:\n        return self.transform(X) @ self.coef_\n\n    @staticmethod\n    def __calc_Sl(N, O, smoothing):\n        return N @ inv(N.T @ N + smoothing * O) @ N.T\n\n    @staticmethod\n    def __calc_dof(N, O, smoothing):\n        if smoothing &lt; 0:\n            smoothing = 0\n        return np.trace(SmoothingSpline1D.__calc_Sl(N, O, smoothing))\n\n    @staticmethod\n    def __dk(X: np.ndarray, knot: float, knot_last: float) -&gt; np.ndarray:\n        return (X - knot).clip(0) ** 3 / (knot_last - knot)\n\n    @staticmethod\n    def __expand_natural_cubic(X: np.ndarray, knots: np.array) -&gt; np.ndarray:\n        basis_splines = [np.ones(shape=(X.size, 1)), X]\n        dk_last = SmoothingSpline1D.__dk(X, knots[-2], knots[-1])\n        for knot in knots[:-2]:\n            dk = SmoothingSpline1D.__dk(X, knot, knots[-1])\n            basis_splines.append(dk - dk_last)\n        return np.hstack(basis_splines)\n\n    @staticmethod\n    def __calc_integral_matrix(knots: np.array) -&gt; np.ndarray:\n        O = np.zeros(shape=(knots.size, knots.size))\n        for i in range(2, knots.size):\n            for j in range(i, knots.size):\n                O[i, j] = O[j, i] = SmoothingSpline1D.__calc_integral(\n                    knots[i-2], knots[j-2], knots[-2], knots[-1])\n        return O\n\n    @staticmethod\n    def __calc_integral(i, j, p, l):\n        return (-18*i*j*j + 12*i*j*l + 24*i*j*p - 12*i*l*p - 6*i*p*p +\n                6*j*j*j - 12*j*l*p - 6*j*p*p + 12*l*p*p) / \\\n               (i*j - i*l - j*l + l*l)"
  },
  {
    "objectID": "Chapter_7_Lab.html#polynomial-regression",
    "href": "Chapter_7_Lab.html#polynomial-regression",
    "title": "7  Non-linear Modeling",
    "section": "7.2 Polynomial Regression",
    "text": "7.2 Polynomial Regression\nWe now examine how Figure 7.1 was produced. We first fit the model using the following command:\n\nfrom google.colab import drive\ndrive.mount('/content/drive', force_remount=True)\n\nMounted at /content/drive\n\n\n\nWage = pd.read_csv('/content/drive/MyDrive/Lab/Data/Wage.csv')\nprint(Wage.shape)\nWage.head()\n\n(3000, 11)\n\n\n\n  \n    \n      \n\n\n\n\n\n\nyear\nage\nmaritl\nrace\neducation\nregion\njobclass\nhealth\nhealth_ins\nlogwage\nwage\n\n\n\n\n0\n2006\n18\n1. Never Married\n1. White\n1. &lt; HS Grad\n2. Middle Atlantic\n1. Industrial\n1. &lt;=Good\n2. No\n4.318063\n75.043154\n\n\n1\n2004\n24\n1. Never Married\n1. White\n4. College Grad\n2. Middle Atlantic\n2. Information\n2. &gt;=Very Good\n2. No\n4.255273\n70.476020\n\n\n2\n2003\n45\n2. Married\n1. White\n3. Some College\n2. Middle Atlantic\n1. Industrial\n1. &lt;=Good\n1. Yes\n4.875061\n130.982177\n\n\n3\n2003\n43\n2. Married\n3. Asian\n4. College Grad\n2. Middle Atlantic\n2. Information\n2. &gt;=Very Good\n1. Yes\n5.041393\n154.685293\n\n\n4\n2005\n50\n4. Divorced\n1. White\n2. HS Grad\n2. Middle Atlantic\n2. Information\n1. &lt;=Good\n1. Yes\n4.318063\n75.043154\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n#Wage = Wage.drop(Wage.columns[0], axis=1) # the first col dones't seem to be that relevant\n\nWage['education'] = Wage['education'].map({'1. &lt; HS Grad': 1.0, \n                                                 '2. HS Grad': 2.0, \n                                                 '3. Some College': 3.0,\n                                                 '4. College Grad': 4.0,\n                                                 '5. Advanced Degree': 5.0\n                                                })\n\nWage.head()\n\n\n  \n    \n      \n\n\n\n\n\n\nyear\nage\nmaritl\nrace\neducation\nregion\njobclass\nhealth\nhealth_ins\nlogwage\nwage\n\n\n\n\n0\n2006\n18\n1. Never Married\n1. White\n1.0\n2. Middle Atlantic\n1. Industrial\n1. &lt;=Good\n2. No\n4.318063\n75.043154\n\n\n1\n2004\n24\n1. Never Married\n1. White\n4.0\n2. Middle Atlantic\n2. Information\n2. &gt;=Very Good\n2. No\n4.255273\n70.476020\n\n\n2\n2003\n45\n2. Married\n1. White\n3.0\n2. Middle Atlantic\n1. Industrial\n1. &lt;=Good\n1. Yes\n4.875061\n130.982177\n\n\n3\n2003\n43\n2. Married\n3. Asian\n4.0\n2. Middle Atlantic\n2. Information\n2. &gt;=Very Good\n1. Yes\n5.041393\n154.685293\n\n\n4\n2005\n50\n4. Divorced\n1. White\n2.0\n2. Middle Atlantic\n2. Information\n1. &lt;=Good\n1. Yes\n4.318063\n75.043154\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\npoly = PolynomialFeatures(4)\nX = poly.fit_transform(Wage['age'].to_frame())\ny = Wage['wage']\n# X.shape\n\nmodel = sm.OLS(y,X).fit()\nprint(model.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   wage   R-squared:                       0.086\nModel:                            OLS   Adj. R-squared:                  0.085\nMethod:                 Least Squares   F-statistic:                     70.69\nDate:                Sun, 06 Nov 2022   Prob (F-statistic):           2.77e-57\nTime:                        07:35:04   Log-Likelihood:                -15315.\nNo. Observations:                3000   AIC:                         3.064e+04\nDf Residuals:                    2995   BIC:                         3.067e+04\nDf Model:                           4                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst       -184.1542     60.040     -3.067      0.002    -301.879     -66.430\nx1            21.2455      5.887      3.609      0.000       9.703      32.788\nx2            -0.5639      0.206     -2.736      0.006      -0.968      -0.160\nx3             0.0068      0.003      2.221      0.026       0.001       0.013\nx4         -3.204e-05   1.64e-05     -1.952      0.051   -6.42e-05    1.45e-07\n==============================================================================\nOmnibus:                     1097.594   Durbin-Watson:                   1.960\nProb(Omnibus):                  0.000   Jarque-Bera (JB):             4965.521\nSkew:                           1.722   Prob(JB):                         0.00\nKurtosis:                       8.279   Cond. No.                     5.67e+08\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 5.67e+08. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n\n\n\n# STATS\n# ----------------------------------\n# Reference: https://stats.stackexchange.com/questions/44838/how-are-the-standard-errors-of-coefficients-calculated-in-a-regression\n\ny_hat = model.predict(X)\n\n# Covariance of coefficient estimates\nmse = np.sum(np.square(y_hat - y)) / y.size\ncov = mse * np.linalg.inv(X.T @ X)\n# ...or alternatively this stat is provided by stats models:\n#cov = model.cov_params()\n\n# Calculate variance of f(x)\nvar_f = np.diagonal((X @ cov) @ X.T)\n# Derive standard error of f(x) from variance\nse       = np.sqrt(var_f)\nconf_int = 2*se\n\n# PLOT\n# ----------------------------------\n# Setup axes\nfig, ax = plt.subplots(figsize=(10,10))\n\n# Plot datapoints\nsns.scatterplot(x='age', y='wage',\n                color='tab:gray',\n                alpha=0.2,\n                ax=ax,\n                data=Wage)\n\n# Plot estimated f(x)\nsns.lineplot(x=X[:, 1], y=y_hat, ax=ax, color='blue');\n\n# Plot confidence intervals\nsns.lineplot(x=X[:, 1], y=y_hat+conf_int, color='blue');\nsns.lineplot(x=X[:, 1], y=y_hat-conf_int, color='blue');\n# dash confidnece int\nax.lines[1].set_linestyle(\"--\")\nax.lines[2].set_linestyle(\"--\")\n\n\n\n\n\ny_hat = model.predict(X)\npredictions = model.get_prediction()\ndf_predictions = predictions.summary_frame()\n\n\nfig, ax = plt.subplots(figsize=(10,10))\n\n# Plot datapoints\nsns.scatterplot(x='age', y='wage',\n                color='tab:gray',\n                alpha=0.2,\n                ax=ax,\n                data=Wage)\n\n# Plot estimated f(x)\nsns.lineplot(x=X[:, 1], y=y_hat, ax=ax, color='blue')\n\n# Plot confidence intervals\n#plt.fill_between(df_predictions.index, df_predictions.obs_ci_lower, df_predictions.obs_ci_upper, alpha=.1, color='crimson') #prediction interval\nsns.lineplot(x=X[:, 1], y=df_predictions.mean_ci_lower, color='blue')\nsns.lineplot(x=X[:, 1], y=df_predictions.mean_ci_upper, color='blue')\n# dash confidnece int\nax.lines[1].set_linestyle(\"--\")\nax.lines[2].set_linestyle(\"--\")\n\n\n\n\nIf your goal is purely visualization, then you can simply use the seaborn package\n\n# Easy to plot higher polynomial order regressions from seaborn\nplt.figure(figsize=(10,10))\nsns.regplot(x='age', y='wage', data=Wage, order=4, \n            scatter_kws={'alpha': 0.2, 'color': 'gray', 'facecolor': None});\n\n\n\n\nYou can also try to use bootstrap methods https://stackoverflow.com/questions/27164114/show-confidence-limits-and-prediction-limits-in-scatter-plot.\nIn performing a polynomial regression we must decide on the degree of the polynomial to use. One way to do this is by using hypothesis tests. We now fit models ranging from linear to a degree-5 polynomial and seek to determine the simplest model which is sufficient to explain the relationship between wage and age. We use the anova() function, which performs an analysis of variance (ANOVA, using an F-test) in order to test the null hypothesis that a model \\(\\mathcal{M}_1\\) is sufficient to explain the data against the alternative hypothesis that a more complex model \\(\\mathcal{M}_2\\) is required. In order to use the anova() function, \\(\\mathcal{M}_1\\) and \\(\\mathcal{M}_2\\) must be nested models: the predictors in \\(\\mathcal{M}_1\\) must be a subset of the predictors in \\(\\mathcal{M}_2\\). In this case, we fit five different models and sequentially compare the simpler model to the more complex model.\nNull hypothesis is that a model \\(\\mathcal{M}_1\\) is sufficient to explain the data, and alternative hypothese is that a more complex model is needed.\nhttps://www.statsmodels.org/stable/anova.html.\n\npoly = PolynomialFeatures(5)\nX = poly.fit_transform(Wage['age'].to_frame()) # or reshape(-1, 1)\ny = Wage['wage']\nX_df = pd.DataFrame(X)\nX_df.columns = ['Constant']+['X_' + str(i) for i in range(1,6)]\nX_df.head()\n\n\n  \n    \n      \n\n\n\n\n\n\nConstant\nX_1\nX_2\nX_3\nX_4\nX_5\n\n\n\n\n0\n1.0\n18.0\n324.0\n5832.0\n104976.0\n1889568.0\n\n\n1\n1.0\n24.0\n576.0\n13824.0\n331776.0\n7962624.0\n\n\n2\n1.0\n45.0\n2025.0\n91125.0\n4100625.0\n184528125.0\n\n\n3\n1.0\n43.0\n1849.0\n79507.0\n3418801.0\n147008443.0\n\n\n4\n1.0\n50.0\n2500.0\n125000.0\n6250000.0\n312500000.0\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\nfit_1 = sm.OLS(y,X_df.iloc[:,:2]).fit() #degree 1\nfit_2 = sm.OLS(y,X_df.iloc[:,:3]).fit() #degree 2\nfit_3 = sm.OLS(y,X_df.iloc[:,:4]).fit() #degree 3\nfit_4 = sm.OLS(y,X_df.iloc[:,:5]).fit() #degree 4\nfit_5 = sm.OLS(y,X_df.iloc[:,:6]).fit() #degree 5\n\ntable = sm.stats.anova_lm(fit_1,fit_2,fit_3,fit_4,fit_5)\nprint(table)\n\n   df_resid           ssr  df_diff        ss_diff           F        Pr(&gt;F)\n0    2998.0  5.022216e+06      0.0            NaN         NaN           NaN\n1    2997.0  4.793430e+06      1.0  228786.010128  143.593107  2.363850e-32\n2    2996.0  4.777674e+06      1.0   15755.693664    9.888756  1.679202e-03\n3    2995.0  4.771604e+06      1.0    6070.152124    3.809813  5.104620e-02\n4    2994.0  4.770322e+06      1.0    1282.563017    0.804976  3.696820e-01\n\n\nWe can see from the table that p value for the second row is very small, of the order 10^-32. Through this we conclue that linear model is not sufficient to explain the data. Remember this was the alternative hypotheses. Since p value is very significant, we reject null hypothese, according to which linear model is enough to explain the data. the next couple of p values are small, not that small, but enough to reject the null hypotheses. But, if we see the p value of last row, which is comparing model with degree 4 and degre 5, we can see that the p value if approx 0.37. and hence its not that small. Through this we can conclude that model with degree 4 is enough to explain the data and we don’t need a higher degree than that. Hence we conlude that model with degree 3 or 4 are reasonable to fit the data, but lower degree model and higher degre models are not justified.\n\n# lets look at the model with degree 5\nprint(fit_5.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   wage   R-squared:                       0.087\nModel:                            OLS   Adj. R-squared:                  0.085\nMethod:                 Least Squares   F-statistic:                     56.71\nDate:                Sun, 06 Nov 2022   Prob (F-statistic):           1.67e-56\nTime:                        07:35:34   Log-Likelihood:                -15314.\nNo. Observations:                3000   AIC:                         3.064e+04\nDf Residuals:                    2994   BIC:                         3.068e+04\nDf Model:                           5                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nConstant     -49.7046    161.435     -0.308      0.758    -366.239     266.830\nX_1            3.9930     20.110      0.199      0.843     -35.438      43.424\nX_2            0.2760      0.958      0.288      0.773      -1.603       2.155\nX_3           -0.0126      0.022     -0.577      0.564      -0.056       0.030\nX_4            0.0002      0.000      0.762      0.446      -0.000       0.001\nX_5        -9.157e-07   1.02e-06     -0.897      0.370   -2.92e-06    1.09e-06\n==============================================================================\nOmnibus:                     1094.840   Durbin-Watson:                   1.961\nProb(Omnibus):                  0.000   Jarque-Bera (JB):             4940.229\nSkew:                           1.718   Prob(JB):                         0.00\nKurtosis:                       8.265   Cond. No.                     9.39e+10\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 9.39e+10. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n\n\nWe can see that the p value for X_5 is equal for the p value that we got in Annova, also the square of t value for X_5 give the value of F statistic that was there in ANNOVA. but thse are not true for X_1,X_2, and other predictors. Althoug if we look for p values for X_2, in fit_2, we will get the results as expected. Same is true for X_1 in fit 1, X_3 in fit_3, etc. (We don’t have orthogonal polynomial like R here!)\nAnnova can also be used when we have other terms in the model\n\n# Derive 5 degree polynomial features of age\ndegree = 3\nf = 'education +' + ' + '.join(['np.power(age, {})'.format(i) for i in np.arange(1, degree+1)])\nX = pt.dmatrix(f, Wage)\ny = np.asarray(Wage['wage'])\n\n# Get models of increasing degrees\nmodel_1 = sm.OLS(y, X[:, 0:3]).fit()\nmodel_2 = sm.OLS(y, X[:, 0:4]).fit()\nmodel_3 = sm.OLS(y, X[:, 0:5]).fit()\n\n# Compare models with ANOVA\ndisplay(sm.stats.anova_lm(model_1, model_2, model_3))\n\n\n  \n    \n      \n\n\n\n\n\n\ndf_resid\nssr\ndf_diff\nss_diff\nF\nPr(&gt;F)\n\n\n\n\n0\n2997.0\n3.902335e+06\n0.0\nNaN\nNaN\nNaN\n\n\n1\n2996.0\n3.759472e+06\n1.0\n142862.701185\n113.991883\n3.838075e-26\n\n\n2\n2995.0\n3.753546e+06\n1.0\n5926.207070\n4.728593\n2.974318e-02\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nAs an alternative to using hypothesis tests and ANOVA, we could choose the polynomial degree using cross-validation, as discussed in Chapter 5.\nNext we consider the task of predicting whether an individual earns more than \\(250,000\\) per year. We proceed much as before, except that first we create the appropriate response vector, and then apply the GLM() function using family = \"Binomial\" in order to fit a polynomial logistic regression model.\n\nWage['wage_binary'] = np.where(Wage['wage']&gt;250,1,0)\nprint(Wage['wage_binary'].value_counts())\ny = Wage['wage_binary']\npoly = PolynomialFeatures(4)\nX = poly.fit_transform(Wage['age'].to_frame())\nX_df = pd.DataFrame(X)\nX_df.columns = ['Constant']+['X_' + str(i) for i in range(1,5)]\nX_df.head()\n\n0    2921\n1      79\nName: wage_binary, dtype: int64\n\n\n\n  \n    \n      \n\n\n\n\n\n\nConstant\nX_1\nX_2\nX_3\nX_4\n\n\n\n\n0\n1.0\n18.0\n324.0\n5832.0\n104976.0\n\n\n1\n1.0\n24.0\n576.0\n13824.0\n331776.0\n\n\n2\n1.0\n45.0\n2025.0\n91125.0\n4100625.0\n\n\n3\n1.0\n43.0\n1849.0\n79507.0\n3418801.0\n\n\n4\n1.0\n50.0\n2500.0\n125000.0\n6250000.0\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n## Fit logistic model\nclf = sm.GLM(y, X_df, family=sm.families.Binomial(sm.families.links.logit()))\nmodel = clf.fit()\n\n#model = sm.Logit(y,X_df).fit()\n\n\ny_hat = model.predict(X)\n\npredictions = model.get_prediction()\ndf_predictions = predictions.summary_frame()\n\n\nfig, ax = plt.subplots(figsize=(10,10))\n\n# Plot datapoints\nsns.scatterplot(x='age', y=Wage['wage_binary']/5,\n                color='tab:gray',\n                alpha=0.8,\n                ax=ax,\n                data=Wage, marker = '|')\n\n# Plot estimated f(x)\nsns.lineplot(x=X[:, 1], y=y_hat, ax=ax, color='blue')\n\n# Plot confidence intervals\n#plt.fill_between(df_predictions.index, df_predictions.obs_ci_lower, df_predictions.obs_ci_upper, alpha=.1, color='crimson')\nsns.lineplot(x=X[:, 1], y=df_predictions.mean_ci_lower, color='blue')\nsns.lineplot(x=X[:, 1], y=df_predictions.mean_ci_upper, color='blue')\n# dash confidnece int\nax.lines[1].set_linestyle(\"--\")\nax.lines[2].set_linestyle(\"--\")\nax.set_ylim(-0.02,0.22)\nax.set_ylabel('Prob wage &gt; 250k, given age')\n\nText(0, 0.5, 'Prob wage &gt; 250k, given age')"
  },
  {
    "objectID": "Chapter_7_Lab.html#step-functions",
    "href": "Chapter_7_Lab.html#step-functions",
    "title": "7  Non-linear Modeling",
    "section": "7.3 Step Functions",
    "text": "7.3 Step Functions\nWe have drawn the age values corresponding to the observations with wage values above \\(250\\) as gray marks on the top of the plot, and those with wage values below \\(250\\) are shown as gray marks on the bottom of the plot.\nIn order to fit a step function, as discussed in Section 7.2, we use the following function.\n\n#X_cut = pd.cut(Wage['age'],[0,25,40,65,90])\n#X_cut\nX_cut = pd.cut(Wage['age'],4)\nX_cut\n\n0       (17.938, 33.5]\n1       (17.938, 33.5]\n2         (33.5, 49.0]\n3         (33.5, 49.0]\n4         (49.0, 64.5]\n             ...      \n2995      (33.5, 49.0]\n2996    (17.938, 33.5]\n2997    (17.938, 33.5]\n2998    (17.938, 33.5]\n2999      (49.0, 64.5]\nName: age, Length: 3000, dtype: category\nCategories (4, interval[float64, right]): [(17.938, 33.5] &lt; (33.5, 49.0] &lt; (49.0, 64.5] &lt;\n                                           (64.5, 80.0]]\n\n\n\nX_cut = pd.get_dummies(X_cut)\nX_cut.head()\n\n\n  \n    \n      \n\n\n\n\n\n\n(17.938, 33.5]\n(33.5, 49.0]\n(49.0, 64.5]\n(64.5, 80.0]\n\n\n\n\n0\n1\n0\n0\n0\n\n\n1\n1\n0\n0\n0\n\n\n2\n0\n1\n0\n0\n\n\n3\n0\n1\n0\n0\n\n\n4\n0\n0\n1\n0\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\ny = Wage['wage']\nmodel = sm.OLS(y,X_cut).fit()\nprint(model.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   wage   R-squared:                       0.062\nModel:                            OLS   Adj. R-squared:                  0.062\nMethod:                 Least Squares   F-statistic:                     66.58\nDate:                Sun, 06 Nov 2022   Prob (F-statistic):           1.13e-41\nTime:                        07:36:09   Log-Likelihood:                -15353.\nNo. Observations:                3000   AIC:                         3.071e+04\nDf Residuals:                    2996   BIC:                         3.074e+04\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n==================================================================================\n                     coef    std err          t      P&gt;|t|      [0.025      0.975]\n----------------------------------------------------------------------------------\n(17.938, 33.5]    94.1584      1.476     63.790      0.000      91.264      97.053\n(33.5, 49.0]     118.2119      1.081    109.379      0.000     116.093     120.331\n(49.0, 64.5]     117.8230      1.448     81.351      0.000     114.983     120.663\n(64.5, 80.0]     101.7990      4.764     21.368      0.000      92.458     111.140\n==============================================================================\nOmnibus:                     1062.354   Durbin-Watson:                   1.965\nProb(Omnibus):                  0.000   Jarque-Bera (JB):             4551.200\nSkew:                           1.681   Prob(JB):                         0.00\nKurtosis:                       8.011   Cond. No.                         4.41\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nHere pd.cut() automatically picked the cutpoints at \\(33.5\\), \\(49\\), and \\(64.5\\) years of age. The function returns an ordered categorical variable ; the get_dummies() function then creates a set of dummy variables for use in the regression. The age &lt; 33.5 category is not left out, so the intercept coefficient of \\(94.1584\\) can be interpreted as the average salary for those under \\(33.5\\) years of age, and the other coefficients can be also interpreted as the average salary for those in the other age groups. (Different from R here)\nWe can produce predictions and plots just as we did in the case of the polynomial fit.\n\npred_step = model.predict(X_cut)\npredictions = model.get_prediction()\ndf_predictions = predictions.summary_frame()\n\nfig, ax = plt.subplots(figsize=(10,10))\n\nsns.scatterplot(x='age', y='wage',\n                color='tab:gray',\n                alpha=0.2,\n                ax=ax,\n                data=Wage)\nsns.lineplot(x=Wage['age'],y=pred_step, ax=ax, color='blue')\n\nplt.xlabel('Age')\nplt.ylabel('Wage')\nplt.title('Step functions with steps - 4')\n\n# Plot confidence intervals\n#plt.fill_between(df_predictions.index, df_predictions.obs_ci_lower, df_predictions.obs_ci_upper, alpha=.1, color='crimson')\nsns.lineplot(x=X[:, 1], y=df_predictions.mean_ci_lower, color='blue')\nsns.lineplot(x=X[:, 1], y=df_predictions.mean_ci_upper, color='blue')\n# dash confidnece int\nax.lines[1].set_linestyle(\"--\")\nax.lines[2].set_linestyle(\"--\")"
  },
  {
    "objectID": "Chapter_7_Lab.html#splines",
    "href": "Chapter_7_Lab.html#splines",
    "title": "7  Non-linear Modeling",
    "section": "7.4 Splines",
    "text": "7.4 Splines\nIn Section 7.4, we saw that regression splines can be fit by constructing an appropriate matrix of basis functions. In order to fit regression splines in Python, we will do it by using bs() in dmatrix function from patsy library. So, briefly – Data -&gt; convert into matrix using dmatrix -&gt; fit this with OLS, or GLM (generalised linear model).\ncheckout this if you have some time - https://www.analyticsvidhya.com/blog/2018/03/introduction-regression-splines-python-codes/.\npasty.dmatrix - https://patsy.readthedocs.io/en/latest/API-reference.html#spline-regression.\n\n# fit a spline with knots at 25, 40 and 60\ntransformed_x = dmatrix(\"bs(age , knots = (25,40,60), degree = 3, include_intercept = False)\",data = {'age':Wage['age']}, return_type = 'dataframe')\ntransformed_x.shape\n\n(3000, 7)\n\n\n\ntransformed_x.head() #this looks complex (K+3 bais function)\n\n\n  \n    \n      \n\n\n\n\n\n\nIntercept\nbs(age, knots=(25, 40, 60), degree=3, include_intercept=False)[0]\nbs(age, knots=(25, 40, 60), degree=3, include_intercept=False)[1]\nbs(age, knots=(25, 40, 60), degree=3, include_intercept=False)[2]\nbs(age, knots=(25, 40, 60), degree=3, include_intercept=False)[3]\nbs(age, knots=(25, 40, 60), degree=3, include_intercept=False)[4]\nbs(age, knots=(25, 40, 60), degree=3, include_intercept=False)[5]\n\n\n\n\n0\n1.0\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.0\n\n\n1\n1.0\n0.559911\n0.403778\n0.033395\n0.000000\n0.000000\n0.0\n\n\n2\n1.0\n0.000000\n0.114796\n0.618564\n0.262733\n0.003906\n0.0\n\n\n3\n1.0\n0.000000\n0.167109\n0.633167\n0.198880\n0.000844\n0.0\n\n\n4\n1.0\n0.000000\n0.034014\n0.508194\n0.426542\n0.031250\n0.0\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n# fit a OLS model to the transformded data\nmodel = sm.OLS(Wage['wage'], transformed_x).fit()\n\n\ny_hat = model.predict(transformed_x)\n\npredictions = model.get_prediction()\ndf_predictions = predictions.summary_frame()\n\n\nfig, ax = plt.subplots(figsize=(10,10))\n\n# Plot datapoints\nsns.scatterplot(x='age', y='wage',\n                color='tab:gray',\n                alpha=0.2,\n                ax=ax,\n                data=Wage)\n\n# Plot estimated f(x)\nsns.lineplot(x=X[:, 1], y=y_hat, ax=ax, color='blue')\n\n# Plot confidence intervals\n#plt.fill_between(df_predictions.index, df_predictions.obs_ci_lower, df_predictions.obs_ci_upper, alpha=.1, color='crimson')\nsns.lineplot(x=X[:, 1], y=df_predictions.mean_ci_lower, color='blue')\nsns.lineplot(x=X[:, 1], y=df_predictions.mean_ci_upper, color='blue')\n# dash confidnece int\nax.lines[1].set_linestyle(\"--\")\nax.lines[2].set_linestyle(\"--\")\n#ax.set_ylim(-0.02,0.22)\n#ax.set_ylabel('Prob wage &gt; 250k, given age')\n\n\n\n\nHere we have prespecified knots at ages \\(25\\), \\(40\\), and \\(60\\). This produces a spline with six basis functions. (Recall that a cubic spline with three knots has seven degrees of freedom; these degrees of freedom are used up by an intercept, plus six basis functions.)\n\n7.4.1 Natural spline\nIn order to instead fit a natural spline, we use the cr() function. Here we fit a natural spline with four degrees of freedom.\nhttps://patsy.readthedocs.io/en/latest/API-reference.html#patsy.cr.\n\n# fit a spline with knots at 25, 40 and 60\n#transformed_x = dmatrix(\"cr(age , knots = (25,40,60))\",data = {'age':Wage['age']}, return_type = 'dataframe')\n#print(transformed_x.shape)\n#transformed_x.head()\ntransformed_x2 = dmatrix(\"cr(age,df = 4)\", {\"age\": Wage['age']}, return_type='dataframe')\ntransformed_x2.shape\n\n(3000, 5)\n\n\n\n# fit a OLS model to the transformded data\nmodel = sm.OLS(Wage['wage'], transformed_x2).fit()\nprint(model.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   wage   R-squared:                       0.084\nModel:                            OLS   Adj. R-squared:                  0.083\nMethod:                 Least Squares   F-statistic:                     91.74\nDate:                Sun, 06 Nov 2022   Prob (F-statistic):           8.48e-57\nTime:                        07:37:36   Log-Likelihood:                -15318.\nNo. Observations:                3000   AIC:                         3.064e+04\nDf Residuals:                    2996   BIC:                         3.067e+04\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n====================================================================================\n                       coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------------\nIntercept           79.6421      1.773     44.918      0.000      76.166      83.119\ncr(age, df=4)[0]   -14.6678      3.436     -4.269      0.000     -21.405      -7.931\ncr(age, df=4)[1]    36.8111      1.950     18.881      0.000      32.988      40.634\ncr(age, df=4)[2]    35.9349      2.056     17.476      0.000      31.903      39.967\ncr(age, df=4)[3]    21.5639      6.989      3.085      0.002       7.860      35.268\n==============================================================================\nOmnibus:                     1092.887   Durbin-Watson:                   1.963\nProb(Omnibus):                  0.000   Jarque-Bera (JB):             4928.836\nSkew:                           1.714   Prob(JB):                         0.00\nKurtosis:                       8.261   Cond. No.                     2.65e+15\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The smallest eigenvalue is 6.31e-28. This might indicate that there are\nstrong multicollinearity problems or that the design matrix is singular.\n\n\n\ny_hat = model.predict(transformed_x2)\n\npredictions = model.get_prediction()\ndf_predictions = predictions.summary_frame()\n\n\nfig, ax = plt.subplots(figsize=(10,10))\n\n# Plot datapoints\nsns.scatterplot(x='age', y='wage',\n                color='tab:gray',\n                alpha=0.2,\n                ax=ax,\n                data=Wage)\n\n# Plot estimated f(x)\nsns.lineplot(x=X[:, 1], y=y_hat, ax=ax, color='blue')\n\n# Plot confidence intervals\n#plt.fill_between(df_predictions.index, df_predictions.obs_ci_lower, df_predictions.obs_ci_upper, alpha=.1, color='crimson')\nsns.lineplot(x=X[:, 1], y=df_predictions.mean_ci_lower, color='blue')\nsns.lineplot(x=X[:, 1], y=df_predictions.mean_ci_upper, color='blue')\n# dash confidnece int\nax.lines[1].set_linestyle(\"--\")\nax.lines[2].set_linestyle(\"--\")\n#ax.set_ylim(-0.02,0.22)\n#ax.set_ylabel('Prob wage &gt; 250k, given age')\n\n\n\n\n\n\n7.4.2 B-spline\nSome of the advantages of splines over polynomials are:\n\nB-splines are very flexible and robust if you keep a fixed low degree, usually 3, and parsimoniously adapt the number of knots. Polynomials would need a higher degree, which leads to the next point.\nB-splines do not have oscillatory behaviour at the boundaries as have polynomials (the higher the degree, the worse). This is known as Runge’s phenomenon.\nB-splines provide good options for extrapolation beyond the boundaries, i.e. beyond the range of fitted values. Have a look at the option extrapolation.\nB-splines generate a feature matrix with a banded structure. For a single feature, every row contains only degree + 1 non-zero elements, which occur consecutively and are even positive. This results in a matrix with good numerical properties, e.g. a low condition number, in sharp contrast to a matrix of polynomials, which goes under the name Vandermonde matrix. A low condition number is important for stable algorithms of linear models.\n\n\n# B-spline with 4 + 3 - 1 = 6 basis functions\nspline = SplineTransformer(n_knots=4, degree=3)\nX = spline.fit_transform(Wage['age'].to_frame())\ny = Wage['wage']\n# X.shape\n\nmodel = sm.OLS(y,X).fit()\nprint(model.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   wage   R-squared:                       0.086\nModel:                            OLS   Adj. R-squared:                  0.085\nMethod:                 Least Squares   F-statistic:                     56.55\nDate:                Sun, 06 Nov 2022   Prob (F-statistic):           2.41e-56\nTime:                        07:40:10   Log-Likelihood:                -15315.\nNo. Observations:                3000   AIC:                         3.064e+04\nDf Residuals:                    2994   BIC:                         3.068e+04\nDf Model:                           5                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nx1          -114.1421     60.244     -1.895      0.058    -232.267       3.982\nx2            79.0702      8.974      8.811      0.000      61.474      96.666\nx3           126.9890      4.685     27.106      0.000     117.803     136.175\nx4           114.0959      5.635     20.249      0.000     103.048     125.144\nx5           115.2455     17.076      6.749      0.000      81.763     148.728\nx6          -125.2557    137.680     -0.910      0.363    -395.213     144.702\n==============================================================================\nOmnibus:                     1095.322   Durbin-Watson:                   1.961\nProb(Omnibus):                  0.000   Jarque-Bera (JB):             4945.655\nSkew:                           1.718   Prob(JB):                         0.00\nKurtosis:                       8.268   Cond. No.                         114.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\ntest_ages = np.linspace(20,80,100)\nX_test = spline.fit_transform(test_ages.reshape(-1,1))\ny_hat = model.predict(X_test)\n\n#y_hat = model.predict(X)\n#predictions = model.get_prediction()\n#df_predictions = predictions.summary_frame()\n\n\nfig, ax = plt.subplots(figsize=(10,10))\n\n# Plot datapoints\nsns.scatterplot(x='age', y='wage',\n                color='tab:gray',\n                alpha=0.2,\n                ax=ax,\n                data=Wage)\n\n# Plot estimated f(x)\nsns.lineplot(x=test_ages, y=y_hat, ax=ax, color='blue')\n\n&lt;matplotlib.axes._subplots.AxesSubplot at 0x7f2fc690f150&gt;\n\n\n\n\n\n\n\n7.4.3 Smooting spline\nUnfortunately, in Python it seems that we have to implement it ourselves\n\nX = Wage['age'].values.reshape(-1,1)\nsp = SmoothingSpline1D(dof=6.8)\nsp.fit(X, y.values)\n\nSmoothingSpline1D(dof=6.8)\n\n\n\nX_test = np.linspace(20,80,100).reshape(-1,1)\n\n\ny_hat = sp.predict(X_test)\n\n#y_hat = model.predict(X)\n#predictions = model.get_prediction()\n#df_predictions = predictions.summary_frame()\n\n\nfig, ax = plt.subplots(figsize=(10,10))\n\n# Plot datapoints\nsns.scatterplot(x='age', y='wage',\n                color='tab:gray',\n                alpha=0.2,\n                ax=ax,\n                data=Wage)\n\n# Plot estimated f(x)\nsns.lineplot(x=X_test.squeeze(), y=y_hat.squeeze(), ax=ax, color='blue')\n\n&lt;matplotlib.axes._subplots.AxesSubplot at 0x7f2fc579f8d0&gt;\n\n\n\n\n\nRefer to https://github.com/empathy87/The-Elements-of-Statistical-Learning-Python-Notebooks/blob/master/examples/Bone%20Mineral%20Density.ipynb for smoothing spline implementation and https://scikit-learn.org/stable/modules/preprocessing.html#spline-transformer for B-spline.\n\n\n7.4.4 Local regression\nIn order to perform local regression, we use the lowess() function.\n\nX = Wage['age']\nlowess = sm.nonparametric.lowess\ny_hat = lowess(y, X, return_sorted=False)\n\n\nfig, ax = plt.subplots(figsize=(10,10))\n\n# Plot datapoints\nsns.scatterplot(x='age', y='wage',\n                color='tab:gray',\n                alpha=0.2,\n                ax=ax,\n                data=Wage)\n\n# Plot estimated f(x)\nsns.lineplot(x=X, y=y_hat, ax=ax, color='blue')\n\n&lt;matplotlib.axes._subplots.AxesSubplot at 0x7f2fc56fc090&gt;"
  },
  {
    "objectID": "Chapter_7_Lab.html#gams",
    "href": "Chapter_7_Lab.html#gams",
    "title": "7  Non-linear Modeling",
    "section": "7.5 GAMs",
    "text": "7.5 GAMs\nWe now fit a GAM to predict wage using natural spline functions of year and age, treating education as a qualitative predictor, as in (7.16). Since this is just a big linear regression model using an appropriate choice of basis functions, we can simply do this using the OLS() function.\n\n# we now fit a GAM to predict wage using natural spline functions of year and age, treating education as a qualitative (i.e. categorical) predictor.\nage_basis = dmatrix(\"cr(Wage.age, df=5)\", {\"Wage.age\": Wage.age}, return_type='dataframe')\nyear_basis = dmatrix(\"cr(Wage.year, df=4)\", {\"Wage.year\": Wage.year}, return_type='dataframe').drop (['Intercept'], axis = 1)\neducation_dummies = pd.get_dummies(Wage.education)\neducation_dummies = education_dummies.drop([education_dummies.columns[0]], axis = 1)\n\n# we concatenate all the predictors\nx_all = pd.concat([age_basis, year_basis, education_dummies], axis=1)\n# fit the model\nmodel_gam = sm.OLS(Wage['wage'],x_all).fit()\npreds = model_gam.predict(x_all)\n\n\nsns.lineplot(x=Wage['year'], y=preds)\nplt.xlabel('Year')\nplt.ylabel('Preds')\n\nText(0, 0.5, 'Preds')\n\n\n\n\n\n\nsns.lineplot(x=Wage['age'],y=preds)\nplt.xlabel('age')\nplt.ylabel('Preds')\n# as it can be seen the figure is not similar as in the text, because here it is not a smooth spline as it was in the \n# book.\n\nText(0, 0.5, 'Preds')\n\n\n\n\n\n\nsns.boxplot(x=Wage['education'],y=preds)\nplt.ylabel('Preds')\n\nText(0, 0.5, 'Preds')\n\n\n\n\n\nNow we try to use the GAM module in statsmodel. Here the categorical variables are treated as linear terms and the effect of two explanatory variables is captured by penalized B-splines\nCheck https://www.statsmodels.org/stable/generated/statsmodels.gam.generalized_additive_model.GLMGam.html#statsmodels.gam.generalized_additive_model.GLMGam for more details.\n\nx_spline = Wage[['year', 'age']]\nbss = BSplines(x_spline, df=[4, 5], degree=[3, 3])\n\n\ngam_bs = GLMGam.from_formula('wage ~ C(education, Treatment(1))', data=Wage,\n                          smoother=bss)\nres_bs = gam_bs.fit()\nprint(res_bs.summary())\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:                   wage   No. Observations:                 3000\nModel:                         GLMGam   Df Residuals:                     2988\nModel Family:                Gaussian   Df Model:                        11.00\nLink Function:               identity   Scale:                          1238.8\nMethod:                         PIRLS   Log-Likelihood:                -14934.\nDate:                Sun, 06 Nov 2022   Deviance:                   3.7014e+06\nTime:                        08:07:42   Pearson chi2:                 3.70e+06\nNo. Iterations:                     3                                         \nCovariance Type:            nonrobust                                         \n=====================================================================================================\n                                        coef    std err          z      P&gt;|z|      [0.025      0.975]\n-----------------------------------------------------------------------------------------------------\nIntercept                            41.7250      5.268      7.921      0.000      31.401      52.049\nC(education, Treatment(1))[T.2.0]    10.7413      2.431      4.418      0.000       5.977      15.506\nC(education, Treatment(1))[T.3.0]    23.2067      2.563      9.056      0.000      18.184      28.229\nC(education, Treatment(1))[T.4.0]    37.8704      2.547     14.871      0.000      32.879      42.862\nC(education, Treatment(1))[T.5.0]    62.4355      2.764     22.591      0.000      57.019      67.852\nyear_s0                               6.7031      4.972      1.348      0.178      -3.041      16.448\nyear_s1                               5.2035      4.237      1.228      0.219      -3.101      13.508\nyear_s2                               7.5149      2.343      3.208      0.001       2.924      12.106\nage_s0                               26.8982      7.771      3.461      0.001      11.667      42.129\nage_s1                               64.4339      5.693     11.318      0.000      53.275      75.592\nage_s2                               33.2186      9.402      3.533      0.000      14.791      51.646\nage_s3                               27.9161     11.042      2.528      0.011       6.275      49.557\n=====================================================================================================\n\n\n\nres_bs.plot_partial(0, cpr=True)\n\n\n\n\n\n\n\n\nres_bs.plot_partial(1, cpr=True)\n\n\n\n\n\n\n\nFor the plotting, see https://www.statsmodels.org/dev/generated/statsmodels.gam.generalized_additive_model.GLMGamResults.html for more information.\nIn these plots, the function of year looks rather linear. We can perform a series of ANOVA tests in order to determine which of these three models is best: a GAM that excludes year (\\(\\mathcal{M}_1\\)), a GAM that uses a linear function of year (\\(\\mathcal{M}_2\\)), or a GAM that uses a spline function of year (\\(\\mathcal{M}_3\\)).\n\n#model1\nX_transformed1 = dmatrix('cr(age,df=5) + education', data = {'age':Wage['age'], 'education':Wage['education']}, return_type = 'dataframe')\nfit1 = sm.OLS(Wage['wage'],X_transformed1).fit(disp = 0)\n\n#model2\nX_transformed2 = dmatrix('year + cr(age,df=5) + education', data = {'year':Wage['year'],'age':Wage['age'], 'education':Wage['education']}, return_type = 'dataframe')\nfit2 = sm.OLS(Wage['wage'],X_transformed2).fit(disp = 0)\n\n#model3\nX_transformed3 = dmatrix('cr(year,df = 4) + cr(age,df=5) + education', data = {'year':Wage['year'],'age':Wage['age'], 'education':Wage['education']}, return_type = 'dataframe')\nfit3 = sm.OLS(Wage['wage'],X_transformed3).fit(disp = 0)\n\n\ntable = sm.stats.anova_lm(fit1,fit2,fit3)\nprint(table)\n\n   df_resid           ssr  df_diff       ss_diff          F    Pr(&gt;F)\n0    2994.0  3.750437e+06      0.0           NaN        NaN       NaN\n1    2993.0  3.732809e+06      1.0  17627.473318  14.129318  0.000174\n2    2991.0  3.731516e+06      2.0   1293.696286   0.518482  0.595477\n\n\nThe first p value is small enough to conclude that model1 is not enough to explain the data, and model2 is better. The second p value is not significant enough to say that model 3 is better than model2, Hence we conclude that model 2 is the best choice among the three.\n\nprint(fit2.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   wage   R-squared:                       0.285\nModel:                            OLS   Adj. R-squared:                  0.284\nMethod:                 Least Squares   F-statistic:                     199.0\nDate:                Sun, 06 Nov 2022   Prob (F-statistic):          5.75e-214\nTime:                        08:09:30   Log-Likelihood:                -14946.\nNo. Observations:                3000   AIC:                         2.991e+04\nDf Residuals:                    2993   BIC:                         2.995e+04\nDf Model:                           6                                         \nCovariance Type:            nonrobust                                         \n====================================================================================\n                       coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------------\nIntercept        -1957.3820    532.924     -3.673      0.000   -3002.317    -912.447\nyear                 1.1987      0.319      3.760      0.000       0.574       1.824\ncr(age, df=5)[0]  -421.8426    106.656     -3.955      0.000    -630.968    -212.717\ncr(age, df=5)[1]  -383.4361    106.582     -3.598      0.000    -592.417    -174.455\ncr(age, df=5)[2]  -374.4885    106.595     -3.513      0.000    -583.495    -165.482\ncr(age, df=5)[3]  -379.2830    106.708     -3.554      0.000    -588.511    -170.055\ncr(age, df=5)[4]  -398.3318    106.802     -3.730      0.000    -607.745    -188.918\neducation           15.2750      0.536     28.491      0.000      14.224      16.326\n==============================================================================\nOmnibus:                     1059.164   Durbin-Watson:                   1.971\nProb(Omnibus):                  0.000   Jarque-Bera (JB):             5654.829\nSkew:                           1.591   Prob(JB):                         0.00\nKurtosis:                       8.926   Cond. No.                     1.13e+19\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The smallest eigenvalue is 9.41e-29. This might indicate that there are\nstrong multicollinearity problems or that the design matrix is singular.\n\n\nWe can also use local regression fits as building blocks in a GAM.\n\nX = Wage['age']\ny = Wage['wage']\n# Create lowess feature for age\nWage['age_lowess'] = sm.nonparametric.lowess(y, X, frac=.7, return_sorted=False)\n\n# Fit logistic regression model\nX_transformed = dmatrix('cr(year, df=4)+ age_lowess + education', data = {'year':Wage['year'], 'age_lowess':Wage['age_lowess'], 'education':Wage['education']}, return_type = 'dataframe')\n\nmodel = sm.OLS(y, X_transformed).fit()\nprint(model.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   wage   R-squared:                       0.285\nModel:                            OLS   Adj. R-squared:                  0.284\nMethod:                 Least Squares   F-statistic:                     239.0\nDate:                Sun, 06 Nov 2022   Prob (F-statistic):          2.74e-215\nTime:                        08:09:31   Log-Likelihood:                -14946.\nNo. Observations:                3000   AIC:                         2.990e+04\nDf Residuals:                    2994   BIC:                         2.994e+04\nDf Model:                           5                                         \nCovariance Type:            nonrobust                                         \n=====================================================================================\n                        coef    std err          t      P&gt;|t|      [0.025      0.975]\n-------------------------------------------------------------------------------------\nIntercept           -38.2295      5.977     -6.396      0.000     -49.949     -26.510\ncr(year, df=4)[0]   -14.0090      1.973     -7.102      0.000     -17.877     -10.141\ncr(year, df=4)[1]    -9.7151      1.886     -5.151      0.000     -13.413      -6.017\ncr(year, df=4)[2]    -7.9607      1.917     -4.153      0.000     -11.719      -4.202\ncr(year, df=4)[3]    -6.5447      2.091     -3.129      0.002     -10.645      -2.444\nage_lowess            1.0823      0.071     15.208      0.000       0.943       1.222\neducation            15.3535      0.534     28.746      0.000      14.306      16.401\n==============================================================================\nOmnibus:                     1056.253   Durbin-Watson:                   1.970\nProb(Omnibus):                  0.000   Jarque-Bera (JB):             5640.862\nSkew:                           1.585   Prob(JB):                         0.00\nKurtosis:                       8.922   Cond. No.                     6.53e+17\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The smallest eigenvalue is 7.78e-29. This might indicate that there are\nstrong multicollinearity problems or that the design matrix is singular.\n\n\nCheck https://www.statsmodels.org/stable/generated/statsmodels.gam.generalized_additive_model.LogitGam.html#statsmodels.gam.generalized_additive_model.LogitGam for more information\nIn Python, we can also use the pyGAM package. It provides methods for regression and classification.\nCheck https://pygam.readthedocs.io/en/latest/notebooks/tour_of_pygam.html#Models for more details\n\nfrom pygam import LinearGAM, s, f\n\n\nWage = pd.read_csv('/content/drive/MyDrive/Lab/Data/Wage.csv')\nWage['education'] = Wage['education'].map({'1. &lt; HS Grad': 0.0, \n                                                 '2. HS Grad': 1.0, \n                                                 '3. Some College': 2.0,\n                                                 '4. College Grad': 3.0,\n                                                 '5. Advanced Degree': 4.0\n                                                })\n\n\nX = Wage[['year','age','education']]\ny = Wage['wage']\n\n\n## model where s means penalized B-spline and f is the factor term\ngam = LinearGAM(s(0) + s(1) + f(2))\ngam.gridsearch(X.to_numpy(), y.values)\n\n100% (11 of 11) |########################| Elapsed Time: 0:00:00 Time:  0:00:00\n\n\nLinearGAM(callbacks=[Deviance(), Diffs()], fit_intercept=True, \n   max_iter=100, scale=None, terms=s(0) + s(1) + f(2) + intercept, \n   tol=0.0001, verbose=False)\n\n\n\n## plotting\nplt.figure()\nfig, axs = plt.subplots(1,3, figsize=(15,10))\n\ntitles = ['year', 'age', 'education']\n\nfor i, ax in enumerate(axs):\n    XX = gam.generate_X_grid(term=i)\n    sns.lineplot(x=XX[:, i], y=gam.partial_dependence(term=i, X=XX), ax=ax)\n    sns.lineplot(x=XX[:, i], y=gam.partial_dependence(term=i, X=XX,  width=.95)[1][:,0], ls='--', ax=ax, color='red')\n    sns.lineplot(x=XX[:, i], y=gam.partial_dependence(term=i, X=XX,  width=.95)[1][:,1], ls='--', ax=ax, color='red')\n    if i == 0:\n        ax.set_ylim(-30,30)\n    ax.set_title(titles[i])\n\n&lt;Figure size 432x288 with 0 Axes&gt;\n\n\n\n\n\n\ngam.summary() #Even though our model allows coefficients, our smoothing penalty reduces us to just 19 effective degrees of freedom\n\nLinearGAM                                                                                                 \n=============================================== ==========================================================\nDistribution:                        NormalDist Effective DoF:                                     19.2602\nLink Function:                     IdentityLink Log Likelihood:                                -24116.7451\nNumber of Samples:                         3000 AIC:                                            48274.0107\n                                                AICc:                                           48274.2999\n                                                GCV:                                             1250.3656\n                                                Scale:                                           1235.9245\n                                                Pseudo R-Squared:                                   0.2945\n==========================================================================================================\nFeature Function                  Lambda               Rank         EDoF         P &gt; x        Sig. Code   \n================================= ==================== ============ ============ ============ ============\ns(0)                              [15.8489]            20           7.0          5.52e-03     **          \ns(1)                              [15.8489]            20           8.5          1.11e-16     ***         \nf(2)                              [15.8489]            5            3.8          1.11e-16     ***         \nintercept                                              1            0.0          1.11e-16     ***         \n==========================================================================================================\nSignificance codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nWARNING: Fitting splines and a linear function to a feature introduces a model identifiability problem\n         which can cause p-values to appear significant when they are not.\n\nWARNING: p-values calculated in this manner behave correctly for un-penalized models or models with\n         known smoothing parameters, but when smoothing parameters have been estimated, the p-values\n         are typically lower than they should be, meaning that the tests reject the null too readily.\n\n\n/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: UserWarning: KNOWN BUG: p-values computed in this summary are likely much smaller than they should be. \n \nPlease do not make inferences based on these values! \n\nCollaborate on a solution, and stay up to date at: \ngithub.com/dswah/pyGAM/issues/163 \n\n  \"\"\"Entry point for launching an IPython kernel.\n\n\nUse LogisticGAM for classification\n\nfrom pygam import LogisticGAM, s, f, l\n\n\nWage['wage_binary'] = np.where(Wage['wage']&gt;250,1,0)\ny = Wage['wage_binary']\n\ngam = LogisticGAM(s(0) + s(1) + f(2)).gridsearch(X.to_numpy(), y.values)\n\nplt.figure()\nfig, axs = plt.subplots(1,3, figsize=(15,10))\n\ntitles = ['year', 'age', 'education']\n\nfor i, ax in enumerate(axs):\n    XX = gam.generate_X_grid(term=i)\n    pdep, confi = gam.partial_dependence(term=i, width=.95)\n\n    ax.plot(XX[:, i], pdep)\n    ax.plot(XX[:, i], confi, c='r', ls='--')\n    ax.set_title(titles[i])\n    if i == 0:\n        ax.set_ylim(-4,4)\n    elif i==2:\n        ax.set_ylim(-4,4)\n\n100% (11 of 11) |########################| Elapsed Time: 0:00:01 Time:  0:00:01\n\n\n&lt;Figure size 432x288 with 0 Axes&gt;"
  },
  {
    "objectID": "Chapter_8_Lab.html#setup",
    "href": "Chapter_8_Lab.html#setup",
    "title": "8  Decision Trees",
    "section": "8.1 Setup",
    "text": "8.1 Setup\nWe first use classification trees to analyze the Carseats data set. In these data, Sales is a continuous variable, and so we begin by recoding it as a binary variable.\n\n!pip install git+https://github.com/JakeColtman/bartpy.git -qq\n!pip install xgboost -U -qq\n!pip install lightgbm -U -qq\n!pip install catboost -U -qq\n\n  Building wheel for bartpy (setup.py) ... done\n  Building wheel for sklearn (setup.py) ... done\n     |████████████████████████████████| 255.9 MB 37 kB/s \n     |████████████████████████████████| 2.0 MB 5.3 MB/s \nLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\nCollecting catboost\n  Downloading catboost-1.1.1-cp37-none-manylinux1_x86_64.whl (76.6 MB)\n     |████████████████████████████████| 76.6 MB 128 kB/s \nRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from catboost) (1.7.3)\nRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from catboost) (1.15.0)\nRequirement already satisfied: pandas&gt;=0.24.0 in /usr/local/lib/python3.7/dist-packages (from catboost) (1.3.5)\nRequirement already satisfied: numpy&gt;=1.16.0 in /usr/local/lib/python3.7/dist-packages (from catboost) (1.21.6)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from catboost) (3.2.2)\nRequirement already satisfied: graphviz in /usr/local/lib/python3.7/dist-packages (from catboost) (0.10.1)\nRequirement already satisfied: plotly in /usr/local/lib/python3.7/dist-packages (from catboost) (5.5.0)\nRequirement already satisfied: python-dateutil&gt;=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas&gt;=0.24.0-&gt;catboost) (2.8.2)\nRequirement already satisfied: pytz&gt;=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas&gt;=0.24.0-&gt;catboost) (2022.5)\nRequirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,&gt;=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib-&gt;catboost) (3.0.9)\nRequirement already satisfied: cycler&gt;=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib-&gt;catboost) (0.11.0)\nRequirement already satisfied: kiwisolver&gt;=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib-&gt;catboost) (1.4.4)\nRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver&gt;=1.0.1-&gt;matplotlib-&gt;catboost) (4.1.1)\nRequirement already satisfied: tenacity&gt;=6.2.0 in /usr/local/lib/python3.7/dist-packages (from plotly-&gt;catboost) (8.1.0)\nInstalling collected packages: catboost\nSuccessfully installed catboost-1.1.1\n\n\n\n!pip install mlxtend --upgrade --no-deps\n\nRequirement already satisfied: mlxtend in /usr/local/lib/python3.7/dist-packages (0.14.0)\nCollecting mlxtend\n  Downloading mlxtend-0.19.0-py2.py3-none-any.whl (1.3 MB)\n     |████████████████████████████████| 1.3 MB 5.4 MB/s \nInstalling collected packages: mlxtend\n  Attempting uninstall: mlxtend\n    Found existing installation: mlxtend 0.14.0\n    Uninstalling mlxtend-0.14.0:\n      Successfully uninstalled mlxtend-0.14.0\nSuccessfully installed mlxtend-0.19.0\n\n\nUnable to display output for mime type(s): application/vnd.colab-display-data+json\n\n\n\nimport pandas as pd\nimport numpy as np\nimport time\n\nfrom sklearn import datasets\nfrom sklearn.inspection import plot_partial_dependence\nfrom sklearn.model_selection import train_test_split,KFold, cross_val_score, GridSearchCV, RandomizedSearchCV, StratifiedKFold\nfrom sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, GradientBoostingClassifier, GradientBoostingRegressor, AdaBoostClassifier, AdaBoostRegressor, StackingClassifier, VotingClassifier\nfrom sklearn import tree\nfrom sklearn.metrics import confusion_matrix, roc_auc_score, mean_squared_error\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.preprocessing import LabelEncoder\n\n# Require extra installation\nfrom bartpy.sklearnmodel import SklearnModel \nfrom xgboost import XGBRegressor\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMRegressor\nimport lightgbm as lgb\nfrom catboost import CatBoostClassifier, CatBoostRegressor, Pool\n\nimport graphviz \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\nplt.style.use('seaborn-white')\nsns.set_context(\"notebook\", font_scale=1.5, rc={\"lines.linewidth\": 2.5})"
  },
  {
    "objectID": "Chapter_8_Lab.html#fitting-classification-trees",
    "href": "Chapter_8_Lab.html#fitting-classification-trees",
    "title": "8  Decision Trees",
    "section": "8.2 Fitting classification trees",
    "text": "8.2 Fitting classification trees\nThe current data we have is of an organization who sells car seats, each row represents a region. Therefore for each region, the columns are -\n\nSales: unit sales in thousands\nCompPrice: price charged by competitor at each location\nIncome: community income level in 1000s of dollars\nAdvertising: local ad budget at each location in 1000s of dollars\nPopulation: regional pop in thousands\nPrice: price for car seats at each site\nShelveLoc: Bad, Good or Medium indicates quality of shelving location\nAge: age level of the population\nEducation: ed level at location\nUrban: Yes/No\nUS: Yes/No\n\nThe company wants to set up a busniess in some new region, we have the information of that region and want to predict what the sales would be. To make the model we will use this data as training data source.\nWe create a variable, called High, which takes on a value of Yes if the Sales variable exceeds \\(8\\), and takes on a value of No otherwise.\n\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\nMounted at /content/drive\n\n\n\nCarseats = pd.read_csv('/content/drive/MyDrive/Lab/Data/Carseats.csv')\n\nprint(Carseats.shape)\n# Check for missing values\nassert Carseats.isnull().sum().sum() == 0\n\n# Create binary variable High 1 if Sales &gt; 8\nCarseats['High'] = (Carseats['Sales'] &gt; 8).astype(np.float64)\n\nCarseats.head()\n\n(400, 11)\n\n\n\n  \n    \n      \n\n\n\n\n\n\nSales\nCompPrice\nIncome\nAdvertising\nPopulation\nPrice\nShelveLoc\nAge\nEducation\nUrban\nUS\nHigh\n\n\n\n\n0\n9.50\n138\n73\n11\n276\n120\nBad\n42\n17\nYes\nYes\n1.0\n\n\n1\n11.22\n111\n48\n16\n260\n83\nGood\n65\n10\nYes\nYes\n1.0\n\n\n2\n10.06\n113\n35\n10\n269\n80\nMedium\n59\n12\nYes\nYes\n1.0\n\n\n3\n7.40\n117\n100\n4\n466\n97\nMedium\n55\n14\nYes\nYes\n0.0\n\n\n4\n4.15\n141\n64\n3\n340\n128\nBad\n38\n13\nYes\nNo\n0.0\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\npredictors = Carseats.drop([\"Sales\",\"High\"], axis=1).columns\nX = pd.get_dummies(Carseats[predictors], drop_first=True)\ny = Carseats[\"High\"]\n\n\nX\n\n\n  \n    \n      \n\n\n\n\n\n\nCompPrice\nIncome\nAdvertising\nPopulation\nPrice\nAge\nEducation\nShelveLoc_Good\nShelveLoc_Medium\nUrban_Yes\nUS_Yes\n\n\n\n\n0\n138\n73\n11\n276\n120\n42\n17\n0\n0\n1\n1\n\n\n1\n111\n48\n16\n260\n83\n65\n10\n1\n0\n1\n1\n\n\n2\n113\n35\n10\n269\n80\n59\n12\n0\n1\n1\n1\n\n\n3\n117\n100\n4\n466\n97\n55\n14\n0\n1\n1\n1\n\n\n4\n141\n64\n3\n340\n128\n38\n13\n0\n0\n1\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n395\n138\n108\n17\n203\n128\n33\n14\n1\n0\n1\n1\n\n\n396\n139\n23\n3\n37\n120\n55\n11\n0\n1\n0\n1\n\n\n397\n162\n26\n12\n368\n159\n40\n18\n0\n1\n1\n1\n\n\n398\n100\n79\n7\n284\n95\n50\n12\n0\n0\n1\n1\n\n\n399\n134\n37\n0\n27\n120\n49\n16\n1\n0\n1\n1\n\n\n\n\n\n400 rows × 11 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nThe best found split may vary across different runs, even if max_features=n_features. That is the case, if the improvement of the criterion is identical for several splits and one split has to be selected at random. To obtain a deterministic behaviour during fitting, random_state\nhttps://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n\nclf_tree = DecisionTreeClassifier(max_depth=3, min_samples_leaf=5, random_state=1, criterion='entropy')\nclf_tree.fit(X, y)\n\nDecisionTreeClassifier(criterion='entropy', max_depth=3, min_samples_leaf=5,\n                       random_state=1)\n\n\nOne of the most attractive properties of trees is that they can be graphically displayed. We use the plot_tree() function to display the tree structure, and the export_text function to display the node labels. The argument feature_names instructs Python to include the feature names for any predictors, rather than simply displaying a number.\n\nplt.figure(figsize=(40,24))\ntree.plot_tree(clf_tree, feature_names=X.columns,  \n                    class_names=['No','Yes'],\n                    filled=True)\nplt.show()\n\n\n\n\n\ndot_data = tree.export_graphviz(clf_tree, feature_names=X.columns,  \n                    class_names=['No','Yes'],\n                    filled=True, out_file=None) \ngraph = graphviz.Source(dot_data) \ngraph.render(\"tree1\")\ngraph \n\n\n\n\n\nr = tree.export_text(clf_tree, feature_names=list(X.columns))\nprint(r)\n\n|--- ShelveLoc_Good &lt;= 0.50\n|   |--- Price &lt;= 92.50\n|   |   |--- Income &lt;= 57.00\n|   |   |   |--- class: 0.0\n|   |   |--- Income &gt;  57.00\n|   |   |   |--- class: 1.0\n|   |--- Price &gt;  92.50\n|   |   |--- Advertising &lt;= 13.50\n|   |   |   |--- class: 0.0\n|   |   |--- Advertising &gt;  13.50\n|   |   |   |--- class: 1.0\n|--- ShelveLoc_Good &gt;  0.50\n|   |--- Price &lt;= 135.00\n|   |   |--- US_Yes &lt;= 0.50\n|   |   |   |--- class: 1.0\n|   |   |--- US_Yes &gt;  0.50\n|   |   |   |--- class: 1.0\n|   |--- Price &gt;  135.00\n|   |   |--- Income &lt;= 46.00\n|   |   |   |--- class: 0.0\n|   |   |--- Income &gt;  46.00\n|   |   |   |--- class: 1.0\n\n\n\nThe most important indicator of Sales appears to be shelving location, since the first branch differentiates Good locations from Bad and Medium locations.\nFor more information about understanding the structure and decision path https://scikit-learn.org/stable/auto_examples/tree/plot_unveil_tree_structure.html#sphx-glr-auto-examples-tree-plot-unveil-tree-structure-py.\n\n#clf_tree = DecisionTreeClassifier(max_depth=8, min_samples_leaf=5, random_state=1, criterion='entropy')\nclf_tree = DecisionTreeClassifier(max_leaf_nodes=30, random_state=1, criterion='entropy')\nclf_tree.fit(X, y)\nclf_tree.score(X, y)\n\n0.9375\n\n\nWe see that the training error rate is \\(6.25\\%\\) when we set the max_leaf_nodes to 30. In order to properly evaluate the performance of a classification tree on these data, we must estimate the test error rather than simply computing the training error. We split the observations into a training set and a test set, build the tree using the training set, and evaluate its performance on the test data. The predict() function can be used for this purpose. This approach leads to correct predictions for around \\(73\\,\\%\\) of the locations in the test data set.\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.5, random_state=1)\nclf_tree = DecisionTreeClassifier(max_leaf_nodes=30, random_state=1, criterion='entropy')\nclf_tree.fit(X_train, y_train)\n\ny_pred = clf_tree.predict(X_test)\ny_pred_prob = clf_tree.predict_proba(X_test)\n\nconf_mat = confusion_matrix(y_test, y_pred)     \nprint(conf_mat)\nTP = conf_mat[1,1]\nTN = conf_mat[0,0]\nFP = conf_mat[0,1]\nFN = conf_mat[1,0]\n\nprint(\"Accuracy: \", (TP+TN) / (TP+TN+FP+FN) )\nprint(\"Sensitivity: \",  TP / (FN + TP) )\nprint(\"Precision: \",  TP / (FP + TP) )\nprint(\"False Positive Rate: \",  FP / (FP + TN) )\nprint(\"AUC Score: \", roc_auc_score(y_test, y_pred_prob[:,1]) )\n\n[[100  19]\n [ 35  46]]\nAccuracy:  0.73\nSensitivity:  0.5679012345679012\nPrecision:  0.7076923076923077\nFalse Positive Rate:  0.15966386554621848\nAUC Score:  0.7327523602033406\n\n\nAgain, If you re-run the fit() function then you might get slightly different results, due to “ties”: for instance, this can happen when the training observations corresponding to a terminal node are evenly split between Yes and No response values. So remember to set the random_state\nNext, we consider whether pruning the tree might lead to improved results. We use cross-validation in order to determine the optimal size of tree complexity.\nThe tree now consider the number of terminal nodes.\n\ntree_sizes = range(2, 50)\n\nk = 5\nkf5 = KFold(n_splits=k, shuffle=True, random_state=1)\n\nACC_scores = []\nAUC_scores = []\n\n# 10-Fold CV for each tree size \nfor size in tree_sizes:\n    clf_tree = DecisionTreeClassifier(max_leaf_nodes=size, random_state=1, criterion='entropy')\n    ACC_s = cross_val_score(clf_tree, X, y, cv=kf5, scoring='accuracy')\n    AUC_s = cross_val_score(clf_tree, X, y, cv=kf5, scoring='roc_auc')\n    ACC_scores.append(np.mean(ACC_s))\n    AUC_scores.append(np.mean(AUC_s))\n    \n# plot CV-Accuracy and AUC socres w.r.t tree sizes \nfig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(12, 5))          \nfig.subplots_adjust(wspace=0.5)\n\nax1.plot(tree_sizes, ACC_scores)\nax1.set_title('5-CV ACC')\nax1.set_xlabel('Tree Size')\nax1.set_ylabel('Accuracy')\nax1.grid(True)\n\nax2.plot(tree_sizes, AUC_scores)\nax2.set_title('5-CV AUC scores')\nax2.set_xlabel('Tree Size')\nax2.set_ylabel('AUC scores')\nax2.grid(True)\n\n\n\n\n\nnp.array(ACC_scores).argmax()\n\n16\n\n\nHow well does this pruned tree perform on the test data set? Once again, we apply the predict() function.\n\nclf_tree = DecisionTreeClassifier(max_leaf_nodes=18, random_state=1, criterion='entropy')\nclf_tree.fit(X_train, y_train)\n\ny_pred = clf_tree.predict(X_test)\ny_pred_prob = clf_tree.predict_proba(X_test)\n\nconf_mat = confusion_matrix(y_test, y_pred)     \nprint(conf_mat)\nTP = conf_mat[1,1]\nTN = conf_mat[0,0]\nFP = conf_mat[0,1]\nFN = conf_mat[1,0]\n\nprint(\"Accuracy: \", (TP+TN) / (TP+TN+FP+FN) )\nprint(\"Sensitivity: \",  TP / (FN + TP) )\nprint(\"Precision: \",  TP / (FP + TP) )\nprint(\"False Positive Rate: \",  FP / (FP + TN) )\nprint(\"AUC Score: \", roc_auc_score(y_test, y_pred_prob[:,1]) )\n\n[[100  19]\n [ 32  49]]\nAccuracy:  0.745\nSensitivity:  0.6049382716049383\nPrecision:  0.7205882352941176\nFalse Positive Rate:  0.15966386554621848\nAUC Score:  0.772849880693018\n\n\nNow \\(74.5\\,\\%\\) of the test observations are correctly classified, so not only has the pruning process produced a more interpretable tree, but it has also slightly improved the classification accuracy.\n\n# Plot feature by importance in this model\n\nplot_df = pd.DataFrame({'feature': X.columns, 'importance': clf_tree.feature_importances_})\n\nplt.figure(figsize=(10,10))\nsns.barplot(x='importance', y='feature', data=plot_df.sort_values('importance', ascending=False),\n            color='b')\nplt.xticks(rotation=90);\n\n\n\n\ncost complexity pruning can also be used in order to select a sequence of trees for consideration.\n\nclf_tree = DecisionTreeClassifier(random_state=1, criterion='entropy')\npath = clf_tree.cost_complexity_pruning_path(X_train, y_train)\nccp_alphas, impurities = path.ccp_alphas, path.impurities\n\n\nfig, ax = plt.subplots()\nax.plot(ccp_alphas[:-1], impurities[:-1], marker=\"o\", drawstyle=\"steps-post\")\nax.set_xlabel(\"effective alpha\")\nax.set_ylabel(\"total impurity of leaves\")\nax.set_title(\"Total Impurity vs effective alpha for training set\")\n\nText(0.5, 1.0, 'Total Impurity vs effective alpha for training set')\n\n\n\n\n\n\nclfs = []\nfor ccp_alpha in ccp_alphas:\n    clf = DecisionTreeClassifier(random_state=1, criterion='entropy', ccp_alpha=ccp_alpha)\n    clf.fit(X_train, y_train)\n    clfs.append(clf)\nprint(\n    \"Number of nodes in the last tree is: {} with ccp_alpha: {}\".format(\n        clfs[-1].tree_.node_count, ccp_alphas[-1]\n    )\n)\n\nNumber of nodes in the last tree is: 1 with ccp_alpha: 0.0974240009819235\n\n\n\nclfs = clfs[:-1]\nccp_alphas = ccp_alphas[:-1]\n\nnode_counts = [clf.tree_.node_count for clf in clfs]\ndepth = [clf.tree_.max_depth for clf in clfs]\nfig, ax = plt.subplots(2, 1)\nax[0].plot(ccp_alphas, node_counts, marker=\"o\", drawstyle=\"steps-post\")\nax[0].set_xlabel(\"alpha\")\nax[0].set_ylabel(\"number of nodes\")\nax[0].set_title(\"Number of nodes vs alpha\")\nax[1].plot(ccp_alphas, depth, marker=\"o\", drawstyle=\"steps-post\")\nax[1].set_xlabel(\"alpha\")\nax[1].set_ylabel(\"depth of tree\")\nax[1].set_title(\"Depth vs alpha\")\nfig.tight_layout()\n\n\n\n\nCheck https://scikit-learn.org/stable/auto_examples/tree/plot_cost_complexity_pruning.html and https://scikit-learn.org/stable/modules/tree.html for more details\n\ntrain_scores = [clf.score(X_train, y_train) for clf in clfs]\ntest_scores = [clf.score(X_test, y_test) for clf in clfs]\n\nfig, ax = plt.subplots()\nax.set_xlabel(\"alpha\")\nax.set_ylabel(\"accuracy\")\nax.set_title(\"Accuracy vs alpha for training and testing sets\")\nax.plot(ccp_alphas, train_scores, marker=\"o\", label=\"train\", drawstyle=\"steps-post\")\nax.plot(ccp_alphas, test_scores, marker=\"o\", label=\"test\", drawstyle=\"steps-post\")\nax.legend()\nplt.show()\n\n\n\n\nOther metrics min_samples_leaf and max_depth can also be used to prevent a tree from overfiting"
  },
  {
    "objectID": "Chapter_8_Lab.html#fitting-regression-trees",
    "href": "Chapter_8_Lab.html#fitting-regression-trees",
    "title": "8  Decision Trees",
    "section": "8.3 Fitting Regression Trees",
    "text": "8.3 Fitting Regression Trees\nHere we fit a regression tree to the Boston data set. First, we create a training set, and fit the tree to the training data.\n\n# Load 'Boston' data\nBoston = pd.read_csv('/content/drive/MyDrive/Lab/Data/Boston.csv')\nBoston = Boston.drop('Unnamed: 0', axis=1)\nBoston.info()    # all numeric.  no Null,  Nice \nBoston.head()   \n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 506 entries, 0 to 505\nData columns (total 13 columns):\n #   Column   Non-Null Count  Dtype  \n---  ------   --------------  -----  \n 0   crim     506 non-null    float64\n 1   zn       506 non-null    float64\n 2   indus    506 non-null    float64\n 3   chas     506 non-null    int64  \n 4   nox      506 non-null    float64\n 5   rm       506 non-null    float64\n 6   age      506 non-null    float64\n 7   dis      506 non-null    float64\n 8   rad      506 non-null    int64  \n 9   tax      506 non-null    int64  \n 10  ptratio  506 non-null    float64\n 11  lstat    506 non-null    float64\n 12  medv     506 non-null    float64\ndtypes: float64(10), int64(3)\nmemory usage: 51.5 KB\n\n\n\n  \n    \n      \n\n\n\n\n\n\ncrim\nzn\nindus\nchas\nnox\nrm\nage\ndis\nrad\ntax\nptratio\nlstat\nmedv\n\n\n\n\n0\n0.00632\n18.0\n2.31\n0\n0.538\n6.575\n65.2\n4.0900\n1\n296\n15.3\n4.98\n24.0\n\n\n1\n0.02731\n0.0\n7.07\n0\n0.469\n6.421\n78.9\n4.9671\n2\n242\n17.8\n9.14\n21.6\n\n\n2\n0.02729\n0.0\n7.07\n0\n0.469\n7.185\n61.1\n4.9671\n2\n242\n17.8\n4.03\n34.7\n\n\n3\n0.03237\n0.0\n2.18\n0\n0.458\n6.998\n45.8\n6.0622\n3\n222\n18.7\n2.94\n33.4\n\n\n4\n0.06905\n0.0\n2.18\n0\n0.458\n7.147\n54.2\n6.0622\n3\n222\n18.7\n5.33\n36.2\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\nX = Boston.drop('medv', axis=1)\ny = Boston.medv\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.5, random_state=1)\n\n\nreg_tree = DecisionTreeRegressor(max_depth=3, min_samples_leaf=5, random_state=1)\nreg_tree.fit(X_train, y_train)\n\nDecisionTreeRegressor(max_depth=3, min_samples_leaf=5, random_state=1)\n\n\n\nplt.figure(figsize=(40,24))\ntree.plot_tree(reg_tree, feature_names=X.columns,  \n                    class_names=['No','Yes'],\n                    filled=True)\nplt.show()\n\n\n\n\nThe variable lstat measures the percentage of individuals with {lower socioeconomic status}, while the variable rm corresponds to the average number of rooms. The tree indicates that larger values of rm, or lower values of lstat, correspond to more expensive houses.\nNow we use the cross validation to see whether pruning the tree will improve performance.\n\ntree_sizes = range(2, 50)\n\nk = 5\nkf5 = KFold(n_splits=k, shuffle=True, random_state=1)\n\nMSE_scores = []\n\n# 7-Fold CV for each tree size \nfor size in tree_sizes:\n    reg_tree = DecisionTreeRegressor(max_leaf_nodes=size, random_state=1)\n    MSE_s = -cross_val_score(reg_tree, X, y, cv=kf5, scoring='neg_mean_squared_error')\n    MSE_scores.append(np.mean(MSE_s))\n    \n# plot CV-MSE as tree size gets bigger \nplt.plot(tree_sizes, MSE_scores)\nplt.title('5-CV MSE')\nplt.xlabel('Tree Size')\nplt.ylabel('MSE')\nplt.grid(True)\n\nprint(\"best tree size = \", tree_sizes[np.argmin(MSE_scores)]) #np.argmin(MSE_scores)=5\n\nbest tree size =  7\n\n\n\n\n\n\nreg_tree = DecisionTreeRegressor(max_leaf_nodes=7, random_state=1)\nreg_tree.fit(X_train, y_train)\npred = reg_tree.predict(X_test)\nmse = mean_squared_error(y_test, pred)\nrmse = np.sqrt(mse)\nprint(mse, rmse)\n\n17.924645209941158 4.2337507260042075\n\n\n\nplt.scatter(pred, y_test, label='medv')\nplt.plot([0, 1], [0, 1], '--k', transform=plt.gca().transAxes)\nplt.xlabel('pred')\nplt.ylabel('y_test')\n\nText(0, 0.5, 'y_test')\n\n\n\n\n\nIn other words, the test set MSE associated with the regression tree is \\(17.92\\). The square root of the MSE is therefore around \\(4.234\\), indicating that this model leads to test predictions that are (on average) within approximately \\(4,234\\) of the true median home value for the census tract."
  },
  {
    "objectID": "Chapter_8_Lab.html#bagging-and-random-forests",
    "href": "Chapter_8_Lab.html#bagging-and-random-forests",
    "title": "8  Decision Trees",
    "section": "8.4 Bagging and Random Forests",
    "text": "8.4 Bagging and Random Forests\nHere we apply bagging and random forests to the Boston data. Recall that bagging is simply a special case of a random forest with \\(m = p\\). Therefore, the RandomForestRegressor() function can be used to perform both random forests and bagging. Let’s start with bagging:\n\n# Bagging: using all features\nmax_features = X.shape[1]\ntree_count   = 100\nregr1 = RandomForestRegressor(max_features=max_features, random_state=2, n_estimators=tree_count)\nregr1.fit(X_train, y_train)\n\nRandomForestRegressor(max_features=12, random_state=2)\n\n\nThe argument max_features=12 indicates that all 12 predictors should be considered for each split of the tree – in other words, that bagging should be done. How well does this bagged model perform on the test set?\n\npred = regr1.predict(X_test)\nplt.scatter(pred, y_test, label='medv')\nplt.plot([0, 1], [0, 1], '--k', transform=plt.gca().transAxes)\nplt.xlabel('pred')\nplt.ylabel('y_test')\nmse = mean_squared_error(y_test, pred)\nrmse = np.sqrt(mse)\nprint(mse, rmse)\n\n11.56611499209486 3.4008991446520227\n\n\n\n\n\nWe can grow a random forest in exactly the same way, except that we’ll use a smaller value of the max_features argument. Here we’ll use max_features = 6:\n\n# Random forests: using 6 features\ntree_count   = 100\nregr2 = RandomForestRegressor(max_features=6, random_state=2, n_estimators=tree_count)\nregr2.fit(X_train, y_train)\n\npred = regr2.predict(X_test)\nmse = mean_squared_error(y_test, pred)\nrmse = np.sqrt(mse)\nprint(mse, rmse)\n\n11.210173703557313 3.3481597488108767\n\n\nThe test set MSE is even lower; this indicates that random forests yielded an improvement over bagging in this case.\nUsing the feature_importances attribute of the RandomForestRegressor, we can view the importance of each variable:\n\nImportance = pd.DataFrame({'Importance':regr2.feature_importances_}, index=X.columns)\nImportance.sort_values(by='Importance', axis=0, ascending=True).plot(kind='barh', color='r', )\nplt.xlabel('Variable Importance')\nplt.gca().legend_ = None\n\n\n\n\nThe results indicate that across all of the trees considered in the random forest, the wealth of the community (lstat) and the house size (rm) are by far the two most important variables."
  },
  {
    "objectID": "Chapter_8_Lab.html#boosting",
    "href": "Chapter_8_Lab.html#boosting",
    "title": "8  Decision Trees",
    "section": "8.5 Boosting",
    "text": "8.5 Boosting\nHere we use the GradientBoostingRegressor() function, to fit boosted regression trees to the Boston data set.\n\n# Gradient boosting\n\nparams = {'n_estimators': 500, 'max_depth': 4, 'min_samples_split': 2, 'max_features': 'sqrt', 'subsample' : 0.9,\n          'learning_rate': 0.01, 'loss': 'squared_error', 'random_state': 1}\n\nregr3   = GradientBoostingRegressor(**params)\nregr3.fit(X_train, y_train)\n\npred = regr3.predict(X_test)\nmse = mean_squared_error(y_test, pred)\nrmse = np.sqrt(mse)\nprint(mse, rmse)\n\n10.585712502747185 3.2535691943997724\n\n\n\n# Gradient boosting\n\nparams = {'n_estimators': 100, 'max_features': 'sqrt', \n          'learning_rate': 0.1, 'random_state': 10}\n\nregr3   = GradientBoostingRegressor(**params)\nregr3.fit(X_train, y_train)\n\npred = regr3.predict(X_test)\nmse = mean_squared_error(y_test, pred)\nrmse = np.sqrt(mse)\nprint(mse, rmse)\n\n10.377996557875113 3.2214898040929936\n\n\nThe test MSE obtained is \\(10.38\\): this is superior to the test MSE of random forests and bagging.\n\ntest_score = np.zeros((params['n_estimators'],), dtype=np.float64)\n\n# compute test set deviance\nfor i, y_pred in enumerate(regr3.staged_predict(X_test)):   \n    test_score[i] = regr3.loss_(y_test, y_pred)             \n\nplt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nplt.title('Deviance (MSE) w.r.t. no. of estimators')\nplt.plot(np.arange(params['n_estimators']) + 1, regr3.train_score_, 'b-',\n         label='Training Set Deviance')\nplt.plot(np.arange(params['n_estimators']) + 1, test_score, 'r-',\n         label='Test Set Deviance')\nplt.legend(loc='upper right')\nplt.xlabel('Boosting Iterations')\nplt.ylabel('Deviance (MSE)')\nprint(\"Train/Test Split GBR with 500 estimators: %.4f\" % test_score[-1])\n\nTrain/Test Split GBR with 500 estimators: 10.3780\n\n\n\n\n\n\nfeature_importance = regr3.feature_importances_\nrel_imp = pd.Series(feature_importance, index=X.columns).sort_values(inplace=False)\nprint(rel_imp)\nrel_imp.T.plot(kind='barh', color='r', )\nplt.xlabel('Variable Importance')\nplt.gca().legend_ = None\n\nchas       0.001323\nrad        0.002557\nzn         0.020765\nage        0.023923\ntax        0.051350\ncrim       0.061762\nptratio    0.075320\ndis        0.096075\nindus      0.098711\nnox        0.105943\nlstat      0.219331\nrm         0.242939\ndtype: float64\n\n\n\n\n\nWe see that lstat and rm are by far the most important variables. We can also produce partial dependence plots for these two variables. These plots illustrate the marginal effect of the selected variables on the response after integrating out the other variables. In this case, as we might expect, median house prices are increasing with rm and decreasing with lstat.\n\n# Partial Dependence Plots\ndisp1 = plot_partial_dependence(regr3,  X_train, [5, 11], feature_names=X.columns)  \n\n/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function plot_partial_dependence is deprecated; Function `plot_partial_dependence` is deprecated in 1.0 and will be removed in 1.2. Use PartialDependenceDisplay.from_estimator instead\n  warnings.warn(msg, category=FutureWarning)\n\n\n\n\n\n\n8.5.1 AdaBoost\n\nada_regr = AdaBoostRegressor(n_estimators=100, random_state=1)\n\nada_regr.fit(X_train, y_train)\n\npred = ada_regr.predict(X_test)\nmse = mean_squared_error(y_test, pred)\nrmse = np.sqrt(mse)\nprint(mse, rmse)\n\n15.134274042481255 3.8902794298714913\n\n\nCheck https://scikit-learn.org/stable/modules/ensemble.html#adaboost for more detail"
  },
  {
    "objectID": "Chapter_8_Lab.html#bayesian-additive-regression-trees",
    "href": "Chapter_8_Lab.html#bayesian-additive-regression-trees",
    "title": "8  Decision Trees",
    "section": "8.6 Bayesian Additive Regression Trees",
    "text": "8.6 Bayesian Additive Regression Trees\nIn this section we use the bartpy package, and within it the SklearnModel() function, to fit a Bayesian additive regression tree model to the Boston housing data set. On colab the following code may takes 5~10 minutes to run with default value.\n\nmodel = SklearnModel() # Use default parameters\nmodel.fit(X_train, y_train) # Fit the model\n\npred = model.predict(X_test)\nmse = mean_squared_error(y_test, pred)\nrmse = np.sqrt(mse)\nprint(mse, rmse)\n\n13.771499662519378 3.710997125102548\n\n\nChcek https://jakecoltman.github.io/bartpy/ for more details"
  },
  {
    "objectID": "Chapter_8_Lab.html#xgboost",
    "href": "Chapter_8_Lab.html#xgboost",
    "title": "8  Decision Trees",
    "section": "8.7 XGBoost",
    "text": "8.7 XGBoost\n\n8.7.1 Classification task\nHere are the essential steps to build an XGBoost classification model in scikit-learn using cross-validation.\n\niris = datasets.load_iris()\ndf = pd.DataFrame(data= np.c_[iris['data'], iris['target']],columns= iris['feature_names'] + ['target'])\ndf.head()\n\n\n  \n    \n      \n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\ntarget\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\n0.0\n\n\n1\n4.9\n3.0\n1.4\n0.2\n0.0\n\n\n2\n4.7\n3.2\n1.3\n0.2\n0.0\n\n\n3\n4.6\n3.1\n1.5\n0.2\n0.0\n\n\n4\n5.0\n3.6\n1.4\n0.2\n0.0\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\nX_train, X_test, y_train, y_test = train_test_split(iris['data'], iris['target'], random_state=42)\n\nThe following template is for building an XGBoost classifier\n\nxgb = XGBClassifier(booster='gbtree', objective='multi:softprob', \n                    learning_rate=0.1, n_estimators=100, random_state=42, n_jobs=-1)\n\n\nbooster='gbtree': The booster is the base learner. It’s the machine learning model that is constructed during every round of boosting. You may have guessed that ‘gbtree’ stands for gradient boosted tree, the XGBoost default base learner. It’s uncommon but possible to work with other base learners,\nobjective='multi:softprob': Standard options for the objective can be viewed in the XGBoost official documentation, https://xgboost.readthedocs.io/en/latest/parameter.html, under Learning Task Parameters. The multi:softprob objective is a standard alternative to binary:logistic when the dataset includes multiple classes. It computes the probabilities of classification and chooses the highest one. If not explicitly stated, XGBoost will often find the right objective for you.\nmax_depth=6: The max_depth of a tree determines the number of branches each tree has. It’s one of the most important hyperparameters in making balanced predictions. XGBoost uses a default of 6, unlike random forests, which don’t provide a value unless explicitly programmed.\nlearning_rate=0.1: Within XGBoost, this hyperparameter is often referred to as eta. This hyperparameter limits the variance by reducing the weight of each tree to the given percentage.\nn_estimators=100: Popular among ensemble methods, n_estimators is the number of boosted trees in the model. Increasing this number while decreasing learning_rate can lead to more robust results.\n\n\nxgb.fit(X_train, y_train)\ny_pred = xgb.predict(X_test)\nscore = accuracy_score(y_pred, y_test)\nprint('Score: ' + str(score))\n\nScore: 1.0\n\n\n\nxgb.get_params()\n\n{'objective': 'multi:softprob',\n 'use_label_encoder': False,\n 'base_score': 0.5,\n 'booster': 'gbtree',\n 'callbacks': None,\n 'colsample_bylevel': 1,\n 'colsample_bynode': 1,\n 'colsample_bytree': 1,\n 'early_stopping_rounds': None,\n 'enable_categorical': False,\n 'eval_metric': None,\n 'gamma': 0,\n 'gpu_id': -1,\n 'grow_policy': 'depthwise',\n 'importance_type': None,\n 'interaction_constraints': '',\n 'learning_rate': 0.1,\n 'max_bin': 256,\n 'max_cat_to_onehot': 4,\n 'max_delta_step': 0,\n 'max_depth': 6,\n 'max_leaves': 0,\n 'min_child_weight': 1,\n 'missing': nan,\n 'monotone_constraints': '()',\n 'n_estimators': 100,\n 'n_jobs': -1,\n 'num_parallel_tree': 1,\n 'predictor': 'auto',\n 'random_state': 42,\n 'reg_alpha': 0,\n 'reg_lambda': 1,\n 'sampling_method': 'uniform',\n 'scale_pos_weight': None,\n 'subsample': 1,\n 'tree_method': 'exact',\n 'validate_parameters': 1,\n 'verbosity': None}\n\n\n\n\n8.7.2 Regression task\nHere are the essential steps to build an XGBoost regression model in scikit-learn using cross-validation.\n\nX,y = datasets.load_diabetes(return_X_y=True)\n\nxgb = XGBRegressor(booster='gbtree', objective='reg:squarederror', \n                    learning_rate=0.1, n_estimators=100, random_state=42, n_jobs=-1)\n\nscores = cross_val_score(xgb, X, y, scoring='neg_mean_squared_error', cv=5)\n\n# Take square root of the scores\nrmse = np.sqrt(-scores)\n\n# Display accuracy\nprint('RMSE:', np.round(rmse, 3))\n\n# Display mean score\nprint('RMSE mean: %0.3f' % (rmse.mean()))\n\nRMSE: [63.033 59.689 64.538 63.699 64.661]\nRMSE mean: 63.124\n\n\n\nxgb.fit(X,y)\nxgb.get_params()\n\n{'objective': 'reg:squarederror',\n 'base_score': 0.5,\n 'booster': 'gbtree',\n 'callbacks': None,\n 'colsample_bylevel': 1,\n 'colsample_bynode': 1,\n 'colsample_bytree': 1,\n 'early_stopping_rounds': None,\n 'enable_categorical': False,\n 'eval_metric': None,\n 'gamma': 0,\n 'gpu_id': -1,\n 'grow_policy': 'depthwise',\n 'importance_type': None,\n 'interaction_constraints': '',\n 'learning_rate': 0.1,\n 'max_bin': 256,\n 'max_cat_to_onehot': 4,\n 'max_delta_step': 0,\n 'max_depth': 6,\n 'max_leaves': 0,\n 'min_child_weight': 1,\n 'missing': nan,\n 'monotone_constraints': '()',\n 'n_estimators': 100,\n 'n_jobs': -1,\n 'num_parallel_tree': 1,\n 'predictor': 'auto',\n 'random_state': 42,\n 'reg_alpha': 0,\n 'reg_lambda': 1,\n 'sampling_method': 'uniform',\n 'scale_pos_weight': 1,\n 'subsample': 1,\n 'tree_method': 'exact',\n 'validate_parameters': 1,\n 'verbosity': None}\n\n\nWithout a baseline of comparison, we have no idea what that score means. Converting the target column, y, into a pandas DataFrame with the .describe() method will give the quartiles and the general statistics of the predictor column, as follows:\n\npd.DataFrame(y).describe()\n\n\n  \n    \n      \n\n\n\n\n\n\n0\n\n\n\n\ncount\n442.000000\n\n\nmean\n152.133484\n\n\nstd\n77.093005\n\n\nmin\n25.000000\n\n\n25%\n87.000000\n\n\n50%\n140.500000\n\n\n75%\n211.500000\n\n\nmax\n346.000000\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nA score of 63.124 is less than 1 standard deviation, a respectable result.\n\n\n8.7.3 Speed comparsion\nLet’s now compare GradientBoostingClassifier and XGBoostClassifier with the exoplanet dataset for its speed\n\n!wget https://github.com/PacktPublishing/Hands-On-Gradient-Boosting-with-XGBoost-and-Scikit-learn/raw/master/Chapter04/exoplanets.csv\n\n--2022-11-08 09:14:53--  https://github.com/PacktPublishing/Hands-On-Gradient-Boosting-with-XGBoost-and-Scikit-learn/raw/master/Chapter04/exoplanets.csv\nResolving github.com (github.com)... 140.82.112.3\nConnecting to github.com (github.com)|140.82.112.3|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://media.githubusercontent.com/media/PacktPublishing/Hands-On-Gradient-Boosting-with-XGBoost-and-Scikit-learn/master/Chapter04/exoplanets.csv [following]\n--2022-11-08 09:14:53--  https://media.githubusercontent.com/media/PacktPublishing/Hands-On-Gradient-Boosting-with-XGBoost-and-Scikit-learn/master/Chapter04/exoplanets.csv\nResolving media.githubusercontent.com (media.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.109.133, ...\nConnecting to media.githubusercontent.com (media.githubusercontent.com)|185.199.110.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 262223348 (250M) [text/plain]\nSaving to: ‘exoplanets.csv’\n\nexoplanets.csv      100%[===================&gt;] 250.08M   170MB/s    in 1.5s    \n\n2022-11-08 09:14:59 (170 MB/s) - ‘exoplanets.csv’ saved [262223348/262223348]\n\n\n\n\ndf = pd.read_csv('exoplanets.csv')\ndf.head()\n\n\n  \n    \n      \n\n\n\n\n\n\nLABEL\nFLUX.1\nFLUX.2\nFLUX.3\nFLUX.4\nFLUX.5\nFLUX.6\nFLUX.7\nFLUX.8\nFLUX.9\n...\nFLUX.3188\nFLUX.3189\nFLUX.3190\nFLUX.3191\nFLUX.3192\nFLUX.3193\nFLUX.3194\nFLUX.3195\nFLUX.3196\nFLUX.3197\n\n\n\n\n0\n2\n93.85\n83.81\n20.10\n-26.98\n-39.56\n-124.71\n-135.18\n-96.27\n-79.89\n...\n-78.07\n-102.15\n-102.15\n25.13\n48.57\n92.54\n39.32\n61.42\n5.08\n-39.54\n\n\n1\n2\n-38.88\n-33.83\n-58.54\n-40.09\n-79.31\n-72.81\n-86.55\n-85.33\n-83.97\n...\n-3.28\n-32.21\n-32.21\n-24.89\n-4.86\n0.76\n-11.70\n6.46\n16.00\n19.93\n\n\n2\n2\n532.64\n535.92\n513.73\n496.92\n456.45\n466.00\n464.50\n486.39\n436.56\n...\n-71.69\n13.31\n13.31\n-29.89\n-20.88\n5.06\n-11.80\n-28.91\n-70.02\n-96.67\n\n\n3\n2\n326.52\n347.39\n302.35\n298.13\n317.74\n312.70\n322.33\n311.31\n312.42\n...\n5.71\n-3.73\n-3.73\n30.05\n20.03\n-12.67\n-8.77\n-17.31\n-17.35\n13.98\n\n\n4\n2\n-1107.21\n-1112.59\n-1118.95\n-1095.10\n-1057.55\n-1034.48\n-998.34\n-1022.71\n-989.57\n...\n-594.37\n-401.66\n-401.66\n-357.24\n-443.76\n-438.54\n-399.71\n-384.65\n-411.79\n-510.54\n\n\n\n\n\n5 rows × 3198 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n# Split data into X and y\nX = df.iloc[:,1:]\ny = df.iloc[:,0]\n\n# Split data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=2)\n\n\nstart = time.time()\n\ngbr = GradientBoostingClassifier(n_estimators=100, max_depth=2, random_state=42)\ngbr.fit(X_train, y_train)\ny_pred = gbr.predict(X_test)\nscore = accuracy_score(y_pred, y_test)\nprint('Score: ' + str(score))\n\nend = time.time()\nelapsed = end - start\n\nprint('Run Time: ' + str(elapsed) + ' seconds')\n\nScore: 0.9874213836477987\nRun Time: 318.0388216972351 seconds\n\n\n\nstart = time.time()\n\n# Instantiate the XGBRegressor, xg_reg\nxg_reg = XGBClassifier(n_estimators=100, max_depth=2, random_state=42)\n\n# class column has to start from 0 (as required since version 1.3.2).\nle = LabelEncoder() # https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html\ny_train = le.fit_transform(y_train)\ny_test = le.fit_transform(y_test)\n# Fit xg_reg to training set\nxg_reg.fit(X_train, y_train)\n\n# Predict labels of test set, y_pred\ny_pred = xg_reg.predict(X_test)\n\nscore = accuracy_score(y_pred, y_test)\n\nprint('Score: ' + str(score))\n\nend = time.time()\nelapsed = end - start\n\nprint('Run Time: ' + str(elapsed) + ' seconds')\n\nScore: 0.9913522012578616\nRun Time: 71.92710447311401 seconds\n\n\nWhen it comes to big data, an algorithm five as fast can save weeks or months of computational time and resources! This advantage is huge in the world of big data. In the world of boosting, XGBoost is the model of choice due to its unparalleled speed and impressive accuracy.\n\n\n8.7.4 Hyperparameter\nXGBoost has many hyperparameters. XGBoost base learner hyperparameters incorporate all decision tree hyperparameters as a starting point. There are gradient boosting hyperparameters, since XGBoost is an enhanced version of gradient boosting.\n\n!wget https://github.com/PacktPublishing/Hands-On-Gradient-Boosting-with-XGBoost-and-Scikit-learn/raw/master/Chapter06/heart_disease.csv\n\n--2022-11-08 09:22:39--  https://github.com/PacktPublishing/Hands-On-Gradient-Boosting-with-XGBoost-and-Scikit-learn/raw/master/Chapter06/heart_disease.csv\nResolving github.com (github.com)... 140.82.114.4\nConnecting to github.com (github.com)|140.82.114.4|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://media.githubusercontent.com/media/PacktPublishing/Hands-On-Gradient-Boosting-with-XGBoost-and-Scikit-learn/master/Chapter06/heart_disease.csv [following]\n--2022-11-08 09:22:39--  https://media.githubusercontent.com/media/PacktPublishing/Hands-On-Gradient-Boosting-with-XGBoost-and-Scikit-learn/master/Chapter06/heart_disease.csv\nResolving media.githubusercontent.com (media.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\nConnecting to media.githubusercontent.com (media.githubusercontent.com)|185.199.108.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 11328 (11K) [text/plain]\nSaving to: ‘heart_disease.csv’\n\nheart_disease.csv   100%[===================&gt;]  11.06K  --.-KB/s    in 0s      \n\n2022-11-08 09:22:39 (67.5 MB/s) - ‘heart_disease.csv’ saved [11328/11328]\n\n\n\n\ndf = pd.read_csv('heart_disease.csv')\ndf.head()\n\n\n  \n    \n      \n\n\n\n\n\n\nage\nsex\ncp\ntrestbps\nchol\nfbs\nrestecg\nthalach\nexang\noldpeak\nslope\nca\nthal\ntarget\n\n\n\n\n0\n63\n1\n3\n145\n233\n1\n0\n150\n0\n2.3\n0\n0\n1\n1\n\n\n1\n37\n1\n2\n130\n250\n0\n1\n187\n0\n3.5\n0\n0\n2\n1\n\n\n2\n41\n0\n1\n130\n204\n0\n0\n172\n0\n1.4\n2\n0\n2\n1\n\n\n3\n56\n1\n1\n120\n236\n0\n1\n178\n0\n0.8\n2\n0\n2\n1\n\n\n4\n57\n0\n0\n120\n354\n0\n1\n163\n1\n0.6\n2\n0\n2\n1\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n# Split data into X and y\nX = df.iloc[:, :-1]\ny = df.iloc[:, -1]\n\nBefore tuning hyperparameters, let’s build a classifier so that we can obtain a baseline score as a starting point.\nWhen fine-tuning hyperparameters, GridSearchCV and RandomizedSearchCV are the standard options. However, cross_val_score and GridSearchCV/RandomizedSearchCV do not split data the same way. One solution is to use StratifiedKFold whenever cross-validation is used.\nA stratified fold includes the same percentage of target values in each fold. If a dataset contains 60% 1s and 40% 0s in the target column, each stratified test set contains 60% 1s and 40% 0s. When folds are random, it’s possible that one test set contains a 70-30 split while another contains a 50-50 split of target values.\n\nkfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\n\n# The 'binary:logistic' objective is standard for binary classification in determining the loss function\nmodel = XGBClassifier(booster='gbtree', objective='binary:logistic', random_state=42)\n# Obtain scores of cross-validation\nscores = cross_val_score(model, X, y, cv=kfold)\n\n# Display accuracy\nprint('Accuracy:', np.round(scores, 2))\n\n# Display mean accuracy\nprint('Accuracy mean: %0.2f' % (scores.mean()))\n\nAccuracy: [0.85 0.72 0.74 0.82 0.78]\nAccuracy mean: 0.78\n\n\nThe point here is to use the same folds to obtain new scores when fine-tuning hyperparameters with GridSearchCV and RandomizedSearchCV so that the comparison of scores is fair.\n\ndef grid_search(params, random=False): \n    \n    xgb = XGBClassifier(booster='gbtree', objective='binary:logistic', random_state=42)\n    \n    kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n    \n    if random:\n        grid = RandomizedSearchCV(xgb, params, cv=kfold, n_iter=20, n_jobs=-1, random_state=42)\n    else:\n        # Instantiate GridSearchCV as grid_reg\n        grid = GridSearchCV(xgb, params, cv=kfold, n_jobs=-1)\n    \n    # Fit grid_reg on X_train and y_train\n    grid.fit(X, y)\n\n    # Extract best params\n    best_params = grid.best_params_\n\n    # Print best params\n    print(\"Best params:\", best_params)\n    \n    # Compute best score\n    best_score = grid.best_score_\n\n    # Print best score\n    print(\"Best score: {:.5f}\".format(best_score))\n\nThe XGBoost hyperparameters presented here are not meant to be exhaustive, but they are meant to be comprehensive. For a complete list of hyperparameters, read the official documentation, XGBoost Parameters, at https://xgboost.readthedocs.io/en/latest/parameter.html.\n\n8.7.4.1 learning_rate\nlearning_rate shrinks the weights of trees for each round of boosting. By lowering learning_rate, more trees are required to produce better scores. Lowering learning_rate prevents overfitting because the size of the weights carried forward is smaller.\nA default value of 0.3 is used. Here is a starting range for learning_rate as placed inside our grid_search function:\n\ngrid_search(params={'learning_rate':[0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5]})\n\nBest params: {'learning_rate': 0.5}\nBest score: 0.80525\n\n\nlowering learning_rate may be advantageous when n_estimators goes up.\n\n\n8.7.4.2 max_depth\nmax_depth determines the length of the tree, equivalent to the number of rounds of splitting. Limiting max_depth prevents overfitting because the individual trees can only grow as far as max_depth allows. XGBoost provides a default max_depth value of six:\n\ngrid_search(params={'max_depth':[2, 3, 5, 6, 8]})\n\nBest params: {'max_depth': 2}\nBest score: 0.79552\n\n\nChanging max_depth from 6 to 2 gave a better score. The lower value for max_depth means variance has been reduced.\n\n\n8.7.4.3 gamma\nKnown as a Lagrange multiplier, gamma provides a threshold that nodes must surpass before making further splits according to the loss function. There is no upper limit to the value of gamma. The default is 0, and anything over 10 is considered very high. Increasing gamma results in a more conservative model:\n\ngrid_search(params={'gamma':[0, 0.01, 0.1, 0.5, 1, 2]})\n\nBest params: {'gamma': 1}\nBest score: 0.79880\n\n\nChanging gamma from 0 to 1 has resulted in a slight improvement.\n\n\n8.7.4.4 min_child_weight\nmin_child_weight refers to the minimum sum of weights required for a node to split into a child. If the sum of the weights is less than the value of min_child_weight, no further splits are made. min_child_weight reduces overfitting by increasing its value:\n\ngrid_search(params={'min_child_weight':[1, 2, 3, 4, 5]})\n\nBest params: {'min_child_weight': 5}\nBest score: 0.81202\n\n\nA slight adjustment to min_child_weight form 1 to 5 gives the best results yet.\n\n\n8.7.4.5 subsample\nThe subsample hyperparameter limits the percentage of training instances (rows) for each boosting round. Decreasing subsample from 100% reduces overfitting:\n\ngrid_search(params={'subsample':[0.5, 0.7, 0.8, 0.9, 1]})\n\nBest params: {'subsample': 0.5}\nBest score: 0.82525\n\n\nThe score has improved by a slight amount once again, indicating a small presence of overfitting.\n\n\n8.7.4.6 colsample_bytree\nSimilar to subsample, colsample_bytree randomly selects particular columns according to the given percentage. colsample_bytree is useful for limiting the influence of columns and reducing variance. Note that colsample_bytree takes a percentage as input, not the number of columns:\n\ngrid_search(params={'colsample_bytree':[0.5, 0.7, 0.8, 0.9, 1]})\n\nBest params: {'colsample_bytree': 0.5}\nBest score: 0.79874\n\n\nYou are encouraged to try colsample_bylevel and colsample_bynode on your own. colsample_bylevel randomly selects columns for each tree depth, and colsample_bynode randomly selects columns when evaluating each tree split.\n\n\n8.7.4.7 n_estimators\nRecall that n_estimators provides the number of trees in the ensemble. In the case of XGBoost, n_estimators is the number of trees trained on the residuals. Initialize a grid search of n_estimators with the default of 100, then double the number of trees through 800 as follows:\n\ngrid_search(params={'n_estimators':[100, 200, 400, 800]})\n\nBest params: {'n_estimators': 200}\nBest score: 0.79219\n\n\nSince our dataset is small, increasing n_estimators did not produce better results.\n\n\n8.7.4.8 Applying early stopping\nearly_stopping_rounds is not a hyperparameter, but a strategy for optimizing the n_estimators hyperparameter.\nNormally when choosing hyperparameters, a test score is given after all boosting rounds are complete. To use early stopping, we need a test score after each round. eval_metric and eval_set may be used as parameters for .fit to generate test scores for each training round. eval_metric provides the scoring method, commonly ‘error’ for classification, and ‘rmse’ for regression. eval_set provides the test to be evaluated, commonly X_test and y_test.\nThe following steps display an evaluation metric for each round of training with the default n_estimators=100:\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\nmodel = XGBClassifier(booster='gbtree', objective='binary:logistic', random_state=42)\neval_set = [(X_test, y_test)]\neval_metric='error'\nmodel.fit(X_train, y_train, eval_metric=eval_metric, eval_set=eval_set)\n# make predictions for test data\ny_pred = model.predict(X_test)\n# evaluate predictions\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n\n[0] validation_0-error:0.23684\n[1] validation_0-error:0.22368\n[2] validation_0-error:0.22368\n[3] validation_0-error:0.21053\n[4] validation_0-error:0.22368\n[5] validation_0-error:0.18421\n[6] validation_0-error:0.21053\n[7] validation_0-error:0.22368\n[8] validation_0-error:0.19737\n[9] validation_0-error:0.19737\n[10]    validation_0-error:0.18421\n[11]    validation_0-error:0.18421\n[12]    validation_0-error:0.19737\n\n\n/usr/local/lib/python3.7/dist-packages/xgboost/sklearn.py:797: UserWarning: `eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n  UserWarning,\n\n\n[13]    validation_0-error:0.17105\n[14]    validation_0-error:0.18421\n[15]    validation_0-error:0.18421\n[16]    validation_0-error:0.18421\n[17]    validation_0-error:0.18421\n[18]    validation_0-error:0.18421\n[19]    validation_0-error:0.19737\n[20]    validation_0-error:0.18421\n[21]    validation_0-error:0.18421\n[22]    validation_0-error:0.18421\n[23]    validation_0-error:0.18421\n[24]    validation_0-error:0.18421\n[25]    validation_0-error:0.18421\n[26]    validation_0-error:0.18421\n[27]    validation_0-error:0.18421\n[28]    validation_0-error:0.18421\n[29]    validation_0-error:0.18421\n[30]    validation_0-error:0.18421\n[31]    validation_0-error:0.17105\n[32]    validation_0-error:0.18421\n[33]    validation_0-error:0.18421\n[34]    validation_0-error:0.18421\n[35]    validation_0-error:0.17105\n[36]    validation_0-error:0.17105\n[37]    validation_0-error:0.17105\n[38]    validation_0-error:0.18421\n[39]    validation_0-error:0.17105\n[40]    validation_0-error:0.18421\n[41]    validation_0-error:0.18421\n[42]    validation_0-error:0.18421\n[43]    validation_0-error:0.18421\n[44]    validation_0-error:0.18421\n[45]    validation_0-error:0.18421\n[46]    validation_0-error:0.18421\n[47]    validation_0-error:0.18421\n[48]    validation_0-error:0.18421\n[49]    validation_0-error:0.18421\n[50]    validation_0-error:0.18421\n[51]    validation_0-error:0.18421\n[52]    validation_0-error:0.18421\n[53]    validation_0-error:0.18421\n[54]    validation_0-error:0.18421\n[55]    validation_0-error:0.18421\n[56]    validation_0-error:0.18421\n[57]    validation_0-error:0.18421\n[58]    validation_0-error:0.18421\n[59]    validation_0-error:0.18421\n[60]    validation_0-error:0.18421\n[61]    validation_0-error:0.18421\n[62]    validation_0-error:0.18421\n[63]    validation_0-error:0.18421\n[64]    validation_0-error:0.18421\n[65]    validation_0-error:0.18421\n[66]    validation_0-error:0.18421\n[67]    validation_0-error:0.18421\n[68]    validation_0-error:0.18421\n[69]    validation_0-error:0.18421\n[70]    validation_0-error:0.18421\n[71]    validation_0-error:0.18421\n[72]    validation_0-error:0.18421\n[73]    validation_0-error:0.18421\n[74]    validation_0-error:0.18421\n[75]    validation_0-error:0.18421\n[76]    validation_0-error:0.18421\n[77]    validation_0-error:0.18421\n[78]    validation_0-error:0.18421\n[79]    validation_0-error:0.18421\n[80]    validation_0-error:0.18421\n[81]    validation_0-error:0.18421\n[82]    validation_0-error:0.18421\n[83]    validation_0-error:0.18421\n[84]    validation_0-error:0.18421\n[85]    validation_0-error:0.18421\n[86]    validation_0-error:0.18421\n[87]    validation_0-error:0.18421\n[88]    validation_0-error:0.18421\n[89]    validation_0-error:0.18421\n[90]    validation_0-error:0.18421\n[91]    validation_0-error:0.18421\n[92]    validation_0-error:0.18421\n[93]    validation_0-error:0.18421\n[94]    validation_0-error:0.18421\n[95]    validation_0-error:0.18421\n[96]    validation_0-error:0.18421\n[97]    validation_0-error:0.18421\n[98]    validation_0-error:0.18421\n[99]    validation_0-error:0.18421\nAccuracy: 81.58%\n\n\nWe know that StratifiedKFold cross-validation gives a mean accuracy of 78% when n_estimators=100. The disparity in scores comes from the difference in test sets.\n\n\n8.7.4.9 early_stopping_rounds\nearly_stopping_rounds is an optional parameter to include with eval_metric and eval_set when fitting a model. Let’s try early_stopping_rounds=10. The previous code is repeated with early_stopping_rounds=10 added in:\n\nmodel = XGBClassifier(booster='gbtree', objective='binary:logistic', random_state=42)\neval_set = [(X_test, y_test)]\neval_metric=\"error\"\nmodel.fit(X_train, y_train, eval_metric=eval_metric, eval_set=eval_set, early_stopping_rounds=10, verbose=True)\ny_pred = model.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n\n[0] validation_0-error:0.23684\n[1] validation_0-error:0.22368\n[2] validation_0-error:0.22368\n[3] validation_0-error:0.21053\n[4] validation_0-error:0.22368\n[5] validation_0-error:0.18421\n[6] validation_0-error:0.21053\n[7] validation_0-error:0.22368\n[8] validation_0-error:0.19737\n\n\n/usr/local/lib/python3.7/dist-packages/xgboost/sklearn.py:797: UserWarning: `eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n  UserWarning,\n/usr/local/lib/python3.7/dist-packages/xgboost/sklearn.py:797: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  UserWarning,\n\n\n[9] validation_0-error:0.19737\n[10]    validation_0-error:0.18421\n[11]    validation_0-error:0.18421\n[12]    validation_0-error:0.19737\n[13]    validation_0-error:0.17105\n[14]    validation_0-error:0.18421\n[15]    validation_0-error:0.18421\n[16]    validation_0-error:0.18421\n[17]    validation_0-error:0.18421\n[18]    validation_0-error:0.18421\n[19]    validation_0-error:0.19737\n[20]    validation_0-error:0.18421\n[21]    validation_0-error:0.18421\n[22]    validation_0-error:0.18421\n[23]    validation_0-error:0.18421\nAccuracy: 82.89%\n\n\nA more thorough approach is to use larger values, say, n_estimators = 5000 and early_stopping_rounds=100. By setting early_stopping_rounds=100, you are guaranteed to reach the default of 100 boosted trees presented by XGBoost. Here is the code that gives a maximum of 5,000 trees and that will stop after 100 consecutive rounds fail to find any improvement:\n\nmodel = XGBClassifier(random_state=42, n_estimators=5000)\neval_set = [(X_test, y_test)]\neval_metric=\"error\"\nmodel.fit(X_train, y_train, eval_metric=eval_metric, eval_set=eval_set, early_stopping_rounds=100)\ny_pred = model.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n\n[0] validation_0-error:0.23684\n[1] validation_0-error:0.22368\n[2] validation_0-error:0.22368\n[3] validation_0-error:0.21053\n[4] validation_0-error:0.22368\n[5] validation_0-error:0.18421\n[6] validation_0-error:0.21053\n[7] validation_0-error:0.22368\n[8] validation_0-error:0.19737\n[9] validation_0-error:0.19737\n[10]    validation_0-error:0.18421\n[11]    validation_0-error:0.18421\n\n\n/usr/local/lib/python3.7/dist-packages/xgboost/sklearn.py:797: UserWarning: `eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n  UserWarning,\n/usr/local/lib/python3.7/dist-packages/xgboost/sklearn.py:797: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  UserWarning,\n\n\n[12]    validation_0-error:0.19737\n[13]    validation_0-error:0.17105\n[14]    validation_0-error:0.18421\n[15]    validation_0-error:0.18421\n[16]    validation_0-error:0.18421\n[17]    validation_0-error:0.18421\n[18]    validation_0-error:0.18421\n[19]    validation_0-error:0.19737\n[20]    validation_0-error:0.18421\n[21]    validation_0-error:0.18421\n[22]    validation_0-error:0.18421\n[23]    validation_0-error:0.18421\n[24]    validation_0-error:0.18421\n[25]    validation_0-error:0.18421\n[26]    validation_0-error:0.18421\n[27]    validation_0-error:0.18421\n[28]    validation_0-error:0.18421\n[29]    validation_0-error:0.18421\n[30]    validation_0-error:0.18421\n[31]    validation_0-error:0.17105\n[32]    validation_0-error:0.18421\n[33]    validation_0-error:0.18421\n[34]    validation_0-error:0.18421\n[35]    validation_0-error:0.17105\n[36]    validation_0-error:0.17105\n[37]    validation_0-error:0.17105\n[38]    validation_0-error:0.18421\n[39]    validation_0-error:0.17105\n[40]    validation_0-error:0.18421\n[41]    validation_0-error:0.18421\n[42]    validation_0-error:0.18421\n[43]    validation_0-error:0.18421\n[44]    validation_0-error:0.18421\n[45]    validation_0-error:0.18421\n[46]    validation_0-error:0.18421\n[47]    validation_0-error:0.18421\n[48]    validation_0-error:0.18421\n[49]    validation_0-error:0.18421\n[50]    validation_0-error:0.18421\n[51]    validation_0-error:0.18421\n[52]    validation_0-error:0.18421\n[53]    validation_0-error:0.18421\n[54]    validation_0-error:0.18421\n[55]    validation_0-error:0.18421\n[56]    validation_0-error:0.18421\n[57]    validation_0-error:0.18421\n[58]    validation_0-error:0.18421\n[59]    validation_0-error:0.18421\n[60]    validation_0-error:0.18421\n[61]    validation_0-error:0.18421\n[62]    validation_0-error:0.18421\n[63]    validation_0-error:0.18421\n[64]    validation_0-error:0.18421\n[65]    validation_0-error:0.18421\n[66]    validation_0-error:0.18421\n[67]    validation_0-error:0.18421\n[68]    validation_0-error:0.18421\n[69]    validation_0-error:0.18421\n[70]    validation_0-error:0.18421\n[71]    validation_0-error:0.18421\n[72]    validation_0-error:0.18421\n[73]    validation_0-error:0.18421\n[74]    validation_0-error:0.18421\n[75]    validation_0-error:0.18421\n[76]    validation_0-error:0.18421\n[77]    validation_0-error:0.18421\n[78]    validation_0-error:0.18421\n[79]    validation_0-error:0.18421\n[80]    validation_0-error:0.18421\n[81]    validation_0-error:0.18421\n[82]    validation_0-error:0.18421\n[83]    validation_0-error:0.18421\n[84]    validation_0-error:0.18421\n[85]    validation_0-error:0.18421\n[86]    validation_0-error:0.18421\n[87]    validation_0-error:0.18421\n[88]    validation_0-error:0.18421\n[89]    validation_0-error:0.18421\n[90]    validation_0-error:0.18421\n[91]    validation_0-error:0.18421\n[92]    validation_0-error:0.18421\n[93]    validation_0-error:0.18421\n[94]    validation_0-error:0.18421\n[95]    validation_0-error:0.18421\n[96]    validation_0-error:0.18421\n[97]    validation_0-error:0.18421\n[98]    validation_0-error:0.18421\n[99]    validation_0-error:0.18421\n[100]   validation_0-error:0.18421\n[101]   validation_0-error:0.18421\n[102]   validation_0-error:0.18421\n[103]   validation_0-error:0.18421\n[104]   validation_0-error:0.18421\n[105]   validation_0-error:0.18421\n[106]   validation_0-error:0.18421\n[107]   validation_0-error:0.18421\n[108]   validation_0-error:0.18421\n[109]   validation_0-error:0.18421\n[110]   validation_0-error:0.18421\n[111]   validation_0-error:0.18421\n[112]   validation_0-error:0.18421\n[113]   validation_0-error:0.18421\nAccuracy: 82.89%\n\n\nAfter 100 rounds of boosting, the score provided by 13 trees is the best.\n\n\n8.7.4.10 Automatically hyperparamter tuning\nYou are encourage to try https://github.com/optuna/optuna-examples/blob/main/xgboost/xgboost_simple.py for hyperparameter tuning.\n\n\n\n8.7.5 For categorical variable and missing value\nXGBoost has experiment support for categorical variable, you can check out here: https://xgboost.readthedocs.io/en/latest/tutorials/categorical.html. But it only works for a few tree methods, it is still recommend to encode your data https://www.kaggle.com/code/shahules/an-overview-of-encoding-techniques/notebook. Missing value, on the other hand can be handled by XGBoost as described at https://xgboost.readthedocs.io/en/stable/faq.html#how-to-deal-with-missing-values.\n\n!gdown 1WkuxuToarMFAHYIQ85SW20YSXrfJsMEH\n\nDownloading...\nFrom: https://drive.google.com/uc?id=1WkuxuToarMFAHYIQ85SW20YSXrfJsMEH\nTo: /content/melb_data.csv\n  0% 0.00/2.09M [00:00&lt;?, ?B/s]100% 2.09M/2.09M [00:00&lt;00:00, 36.3MB/s]\n\n\n\n# Select target\ndata = pd.read_csv('melb_data.csv')\ny = data.Price\n\n# To keep things simple, we'll split the columns into numerical can categorical features\nmelb_predictors = data.drop(['Price', 'Date', 'Address'], axis=1)\ncat_col = melb_predictors.select_dtypes(exclude=['int64','float64'])\n\n# Divide data into training and validation subsets\nX, X_v, y_train, y_valid = train_test_split(melb_predictors, y, train_size=0.8, test_size=0.2, random_state=0)\nX_train = X.select_dtypes(exclude=['object'])\nX_valid = X_v.select_dtypes(exclude=['object'])\nX_train_cat = X.select_dtypes(exclude=['int64','float64'])\nX_valid_cat = X_v.select_dtypes(exclude=['int64','float64'])\n\n\nfor col in X_train_cat.columns:\n  X[col] = X[col].astype('category')\n  X_v[col] = X_v[col].astype('category')\n\n\nxgb = XGBRegressor(booster='gbtree', objective='reg:squarederror', \n                    random_state=42, n_jobs=-1) # You can either specify missing=-9999 or leave it as it is\nxgb.fit(X_train, y_train)\n\nXGBRegressor(base_score=0.5, booster='gbtree', callbacks=None,\n             colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n             early_stopping_rounds=None, enable_categorical=False,\n             eval_metric=None, gamma=0, gpu_id=-1, grow_policy='depthwise',\n             importance_type=None, interaction_constraints='',\n             learning_rate=0.300000012, max_bin=256, max_cat_to_onehot=4,\n             max_delta_step=0, max_depth=6, max_leaves=0, min_child_weight=1,\n             missing=nan, monotone_constraints='()', n_estimators=100,\n             n_jobs=-1, num_parallel_tree=1, predictor='auto', random_state=42,\n             reg_alpha=0, reg_lambda=1, ...)\n\n\n\npreds = xgb.predict(X_valid)\n\n\nmean_absolute_error(y_valid, preds)\n\n173639.76325478646"
  },
  {
    "objectID": "Chapter_8_Lab.html#lightgbm",
    "href": "Chapter_8_Lab.html#lightgbm",
    "title": "8  Decision Trees",
    "section": "8.8 Lightgbm",
    "text": "8.8 Lightgbm\n\n8.8.1 Classification task\n\niris = datasets.load_iris()\ndf = pd.DataFrame(data= np.c_[iris['data'], iris['target']],columns= iris['feature_names'] + ['target'])\nX_train, X_test, y_train, y_test = train_test_split(iris['data'], iris['target'], random_state=42)\n\n\nclf = lgb.LGBMClassifier(boosting_type='gbdt', learning_rate=0.1, n_estimators=100, random_state=42, n_jobs=-1)\nclf.fit(X_train, y_train)\n\nLGBMClassifier(random_state=42)\n\n\n\nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)\nscore = accuracy_score(y_pred, y_test)\nprint('Score: ' + str(score))\n\nScore: 1.0\n\n\n\nclf.get_params()\n\n{'boosting_type': 'gbdt',\n 'class_weight': None,\n 'colsample_bytree': 1.0,\n 'importance_type': 'split',\n 'learning_rate': 0.1,\n 'max_depth': -1,\n 'min_child_samples': 20,\n 'min_child_weight': 0.001,\n 'min_split_gain': 0.0,\n 'n_estimators': 100,\n 'n_jobs': -1,\n 'num_leaves': 31,\n 'objective': None,\n 'random_state': 42,\n 'reg_alpha': 0.0,\n 'reg_lambda': 0.0,\n 'silent': 'warn',\n 'subsample': 1.0,\n 'subsample_for_bin': 200000,\n 'subsample_freq': 0}\n\n\n\n\n8.8.2 Regression task\n\nX,y = datasets.load_diabetes(return_X_y=True)\n\nlgbr = lgb.LGBMRegressor(boosting_type='gbdt', learning_rate=0.1, n_estimators=100, random_state=42, n_jobs=-1)\n\nscores = cross_val_score(lgbr, X, y, scoring='neg_mean_squared_error', cv=5)\n\n# Take square root of the scores\nrmse = np.sqrt(-scores)\n\n# Display accuracy\nprint('RMSE:', np.round(rmse, 3))\n\n# Display mean score\nprint('RMSE mean: %0.3f' % (rmse.mean()))\n\nRMSE: [56.081 59.172 63.191 61.833 60.542]\nRMSE mean: 60.164\n\n\n\nlgbr.fit(X,y)\nlgbr.get_params()\n\n{'boosting_type': 'gbdt',\n 'class_weight': None,\n 'colsample_bytree': 1.0,\n 'importance_type': 'split',\n 'learning_rate': 0.1,\n 'max_depth': -1,\n 'min_child_samples': 20,\n 'min_child_weight': 0.001,\n 'min_split_gain': 0.0,\n 'n_estimators': 100,\n 'n_jobs': -1,\n 'num_leaves': 31,\n 'objective': None,\n 'random_state': 42,\n 'reg_alpha': 0.0,\n 'reg_lambda': 0.0,\n 'silent': 'warn',\n 'subsample': 1.0,\n 'subsample_for_bin': 200000,\n 'subsample_freq': 0}\n\n\n\n\n8.8.3 Speed\n\ndf = pd.read_csv('exoplanets.csv')\n# Split data into X and y\nX = df.iloc[:,1:]\ny = df.iloc[:,0]\n\n# Split data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=2)\n\n\nstart = time.time()\n\n# Instantiate the XGBRegressor, xg_reg\nlg_reg = lgb.LGBMClassifier(n_estimators=100, max_depth=2, random_state=42)\n\n# Fit xg_reg to training set\nlg_reg.fit(X_train, y_train)\n\n# Predict labels of test set, y_pred\ny_pred = lg_reg.predict(X_test)\n\nscore = accuracy_score(y_pred, y_test)\n\nprint('Score: ' + str(score))\n\nend = time.time()\nelapsed = end - start\n\nprint('Run Time: ' + str(elapsed) + ' seconds')\n\nScore: 0.9913522012578616\nRun Time: 10.266663074493408 seconds\n\n\n\n\n8.8.4 Hyperparameter\nFollowing set of practices can be used to improve your model efficiency.\n\nnum_leaves : This is the main parameter to control the complexity of the tree model. Ideally, the value of num_leaves should be less than or equal to 2^(max_depth). Value more than this will result in overfitting.\nmin_data_in_leaf : Setting it to a large value can avoid growing too deep a tree, but may cause under-fitting. In practice, setting it to hundreds or thousands is enough for a large dataset.\nmax_depth : We also can use max_depth to limit the tree depth explicitly.\n\n\nFor Faster Speed\n\n\nUse bagging by setting bagging_fraction and bagging_freq.\nUse feature sub-sampling by setting feature_fraction.\nUse small max_bin.\nUse save_binary to speed up data loading in future learning.\n\n\nFor better accuracy\n\n\nUse large max_bin (may be slower).\nUse small learning_rate with large num_iterations\nUse large num_leaves(may cause over-fitting)\nTry to use categorical feature directly.\n\n\nTo deal with over-fitting\n\n\nUse min_data_in_leaf and min_sum_hessian_in_leaf\nTry lambda_l1, lambda_l2 and min_gain_to_split to regularization\nTry max_depth to avoid growing deep tree\nTry dart\n\nCheck https://lightgbm.readthedocs.io/en/latest/Parameters-Tuning.html for hyperparamter tuning.\n\n\n8.8.5 For categorical variable and missing value\nLightGBM enables the missing value handle by default. See https://lightgbm.readthedocs.io/en/latest/Advanced-Topics.html#missing-value-handle. It also deal with categorical variables as described here https://lightgbm.readthedocs.io/en/latest/Advanced-Topics.html#categorical-feature-support\n\n# Select target\ndata = pd.read_csv('melb_data.csv')\ny = data.Price\n\n# To keep things simple, we'll split the columns into numerical can categorical features\nmelb_predictors = data.drop(['Price', 'Date', 'Address'], axis=1)\ncat_col = melb_predictors.select_dtypes(exclude=['int64','float64'])\n\n# Divide data into training and validation subsets\nX, X_v, y_train, y_valid = train_test_split(melb_predictors, y, train_size=0.8, test_size=0.2, random_state=0)\nX_train = X.select_dtypes(exclude=['object'])\nX_valid = X_v.select_dtypes(exclude=['object'])\nX_train_cat = X.select_dtypes(exclude=['int64','float64'])\nX_valid_cat = X_v.select_dtypes(exclude=['int64','float64'])\n\n\nfor col in X_train_cat.columns:\n  X[col] = X[col].astype('category')\n  X_v[col] = X_v[col].astype('category')\n\n\nlgbr = lgb.LGBMRegressor(boosting_type='gbdt', random_state=42, n_jobs=-1)\nlgbr.fit(X, y_train)\n\nLGBMRegressor(random_state=42)\n\n\n\npreds = lgbr.predict(X_v)\n\n\nmean_absolute_error(y_valid, preds)\n\n160267.37406568974"
  },
  {
    "objectID": "Chapter_8_Lab.html#catboost",
    "href": "Chapter_8_Lab.html#catboost",
    "title": "8  Decision Trees",
    "section": "8.9 CatBoost",
    "text": "8.9 CatBoost\nIn this section, we would explore some base cases of using catboost, such as model training, cross-validation and predicting\n\n8.9.1 Classification task\n\niris = datasets.load_iris()\ndf = pd.DataFrame(data= np.c_[iris['data'], iris['target']],columns= iris['feature_names'] + ['target'])\nX_train, X_test, y_train, y_test = train_test_split(iris['data'], iris['target'], random_state=42)\n\n\nclf = CatBoostClassifier(boosting_type='Plain', learning_rate=0.1, n_estimators=100, random_state=42, thread_count=-1, logging_level = 'Silent')\nclf.fit(X_train, y_train)\n\n&lt;catboost.core.CatBoostClassifier at 0x7f6006b63e50&gt;\n\n\n\nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)\nscore = accuracy_score(y_pred, y_test)\nprint('Score: ' + str(score))\n\nScore: 1.0\n\n\n\nclf.get_all_params()\n\n{'nan_mode': 'Min',\n 'eval_metric': 'MultiClass',\n 'iterations': 100,\n 'sampling_frequency': 'PerTree',\n 'leaf_estimation_method': 'Newton',\n 'grow_policy': 'SymmetricTree',\n 'penalties_coefficient': 1,\n 'boosting_type': 'Plain',\n 'model_shrink_mode': 'Constant',\n 'feature_border_type': 'GreedyLogSum',\n 'bayesian_matrix_reg': 0.10000000149011612,\n 'eval_fraction': 0,\n 'force_unit_auto_pair_weights': False,\n 'l2_leaf_reg': 3,\n 'random_strength': 1,\n 'rsm': 1,\n 'boost_from_average': False,\n 'model_size_reg': 0.5,\n 'pool_metainfo_options': {'tags': {}},\n 'use_best_model': False,\n 'class_names': [0, 1, 2],\n 'random_seed': 42,\n 'depth': 6,\n 'posterior_sampling': False,\n 'border_count': 254,\n 'bagging_temperature': 1,\n 'classes_count': 0,\n 'auto_class_weights': 'None',\n 'sparse_features_conflict_fraction': 0,\n 'leaf_estimation_backtracking': 'AnyImprovement',\n 'best_model_min_trees': 1,\n 'model_shrink_rate': 0,\n 'min_data_in_leaf': 1,\n 'loss_function': 'MultiClass',\n 'learning_rate': 0.10000000149011612,\n 'score_function': 'Cosine',\n 'task_type': 'CPU',\n 'leaf_estimation_iterations': 1,\n 'bootstrap_type': 'Bayesian',\n 'max_leaves': 64}\n\n\n\n\n8.9.2 Regression\n\nX,y = datasets.load_diabetes(return_X_y=True)\n\ncatb = CatBoostRegressor(boosting_type='Plain', learning_rate=0.1, random_state=42, n_estimators=100, thread_count=-1, logging_level = 'Silent')\n\nscores = cross_val_score(catb, X, y, scoring='neg_mean_squared_error', cv=5)\n\n# Take square root of the scores\nrmse = np.sqrt(-scores)\n\n# Display accuracy\nprint('RMSE:', np.round(rmse, 3))\n\n# Display mean score\nprint('RMSE mean: %0.3f' % (rmse.mean()))\n\nRMSE: [54.744 56.125 59.595 56.908 56.583]\nRMSE mean: 56.791\n\n\n\ncatb.fit(X,y)\ncatb.get_all_params()\n\n{'nan_mode': 'Min',\n 'eval_metric': 'RMSE',\n 'iterations': 100,\n 'sampling_frequency': 'PerTree',\n 'leaf_estimation_method': 'Newton',\n 'grow_policy': 'SymmetricTree',\n 'penalties_coefficient': 1,\n 'boosting_type': 'Plain',\n 'model_shrink_mode': 'Constant',\n 'feature_border_type': 'GreedyLogSum',\n 'bayesian_matrix_reg': 0.10000000149011612,\n 'eval_fraction': 0,\n 'force_unit_auto_pair_weights': False,\n 'l2_leaf_reg': 3,\n 'random_strength': 1,\n 'rsm': 1,\n 'boost_from_average': True,\n 'model_size_reg': 0.5,\n 'pool_metainfo_options': {'tags': {}},\n 'subsample': 0.800000011920929,\n 'use_best_model': False,\n 'random_seed': 42,\n 'depth': 6,\n 'posterior_sampling': False,\n 'border_count': 254,\n 'classes_count': 0,\n 'auto_class_weights': 'None',\n 'sparse_features_conflict_fraction': 0,\n 'leaf_estimation_backtracking': 'AnyImprovement',\n 'best_model_min_trees': 1,\n 'model_shrink_rate': 0,\n 'min_data_in_leaf': 1,\n 'loss_function': 'RMSE',\n 'learning_rate': 0.10000000149011612,\n 'score_function': 'Cosine',\n 'task_type': 'CPU',\n 'leaf_estimation_iterations': 1,\n 'bootstrap_type': 'MVS',\n 'max_leaves': 64}\n\n\n\n\n8.9.3 Speed\n\ndf = pd.read_csv('exoplanets.csv')\n# Split data into X and y\nX = df.iloc[:,1:]\ny = df.iloc[:,0]\n\n# Split data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=2)\n\n\nstart = time.time()\n\n# Instantiate the XGBRegressor, xg_reg\nca_reg = CatBoostClassifier(n_estimators=100, max_depth=2, random_state=42, logging_level = 'Silent')\n\n# Fit xg_reg to training set\nca_reg.fit(X_train, y_train)\n\n# Predict labels of test set, y_pred\ny_pred = ca_reg.predict(X_test)\n\nscore = accuracy_score(y_pred, y_test)\n\nprint('Score: ' + str(score))\n\nend = time.time()\nelapsed = end - start\n\nprint('Run Time: ' + str(elapsed) + ' seconds')\n\nScore: 0.9913522012578616\nRun Time: 16.69258213043213 seconds\n\n\n\n\n8.9.4 Hyperparameter\nYou can check more details at https://catboost.ai/en/docs/references/training-parameters/. There are also tutorials about hyperparameter tuning https://github.com/catboost/tutorials/blob/master/hyperparameters_tuning/hyperparameters_tuning_using_optuna_and_hyperopt.ipynb\n\n\n8.9.5 For categorical variable and missing value\nOne of the differences between CatBoost and other gradient boosting libraries is its advanced processing of the categorical features (in fact “Cat” in the package name stands for “CATegorical”).\nCatBoost deals with the categorical data quite well out-of-the-box. However, it also has a huge number of training parameters, which provide fine control over the categorical features preprocessing.\nThe amount of parameters related to categorical features processing in CatBoost is overwhelming. Here is a hopefully the full list:\n\none_hot_max_size (int) - use one-hot encoding for all categorical features with a number of different values less than or equal to the given parameter value. No complex encoding is performed for such features.\nmodel_size_reg (float from 0 to inf) - The model size regularization coefficient. The larger the value, the smaller the model size. This regularization is needed only for models with categorical features (other models are small). Models with categorical features might weight tens of gigabytes or more if categorical features have a lot of values. If the value of the regularizer differs from zero, then the usage of categorical features or feature combinations with a lot of values has a penalty, so fewer of them are used in the resulting model. Default value is 0.5\nmax_ctr_complexity - The maximum number of features that can be combined. Each resulting combination consists of one or more categorical features and can optionally contain binary features in the following form: “numeric feature &gt; value”. For regression task on CPU the default value is 4.\nhas_time (bool) - if true, the 1-st step of categorical features processing, permutation, is not performed. Useful when the objects in your dataset are ordered by time. For our dataset, we don’t need it. Default value is False\nsimple_ctr - Quantization settings for simple categorical features. combinations_ctr - Quantization settings for combinations of categorical features.\nper_feature_ctr - Per-feature quantization settings for categorical features.\ncounter_calc_method determines whether to use validation dataset (provided through parameter eval_set of fit method) to estimate categories frequencies with Counter. By default, it is Full and the objects from validation dataset are used; Pass SkipTest value to ignore the objects from the validation set ctr_target_border_count - The maximum number of borders to use in target quantization for categorical features that need it. Default for regression task is 1.\nctr_leaf_count_limit - The maximum number of leaves with categorical features. Default value is None i.e. no limit.\nstore_all_simple_ctr- If the previous parameter ctr_leaf_count_limit at some point gradient boosting tree can no longer make splits by categorical features. With Default value False the limitation applies both to original categorical features and the features, that CatBoost creates by combining different features. If this parameter is set to True only the number of splits made on combination features is limited.\n\nThe three parameters simple_ctr, combinations_ctr, and per_feature_ctr are complex parameters that control the second and the third steps of categorical features processing.\n\n# Select target\ndata = pd.read_csv('melb_data.csv')\ny = data.Price\n\n# To keep things simple, we'll split the columns into numerical can categorical features\nmelb_predictors = data.drop(['Price', 'Date', 'Address'], axis=1)\ncat_col = melb_predictors.select_dtypes(exclude=['int64','float64'])\n\n# Divide data into training and validation subsets\nX, X_v, y_train, y_valid = train_test_split(melb_predictors, y, train_size=0.8, test_size=0.2, random_state=0)\nX_train = X.select_dtypes(exclude=['object'])\nX_valid = X_v.select_dtypes(exclude=['object'])\nX_train_cat = X.select_dtypes(exclude=['int64','float64'])\nX_valid_cat = X_v.select_dtypes(exclude=['int64','float64'])\n\n\ncategorical_features_names = list(X_train_cat.columns)\n\nfor col in categorical_features_names:\n  X[col] = X.loc[:,col].fillna(value='nan')\n  X_v[col] = X_v.loc[:,col].fillna(value='nan')\n  X[col] = X[col].astype('category')\n  X_v[col] = X_v[col].astype('category')\n\n\ncatbr = CatBoostRegressor(n_estimators=100, random_state=42, thread_count=-1, logging_level = 'Silent', cat_features=categorical_features_names)\ncatbr.fit(X, y_train)\n\n&lt;catboost.core.CatBoostRegressor at 0x7f6006532dd0&gt;\n\n\n\npreds = catbr.predict(X_v)\n\n\nmean_absolute_error(y_valid, preds)\n\n164721.46472522244\n\n\nThe first thing we try is to make CatBoost use one-hot encoding for all our categorical features. The documentation says, that for the features for which one-hot encoding is used no other encodings are computed.\n\ncatbr = CatBoostRegressor(n_estimators=100, random_state=42, thread_count=-1, logging_level = 'Silent', cat_features=categorical_features_names, one_hot_max_size=500)\ncatbr.fit(X, y_train)\npreds = catbr.predict(X_v)\nmean_absolute_error(y_valid, preds)\n\n163165.1377008556\n\n\nLet us try to set model size regularization coefficient to 0 - thus we allow our model to use as many categorical features and its combinations as it wants.\n\ncatbr = CatBoostRegressor(n_estimators=100, random_state=42, thread_count=-1, logging_level = 'Silent', cat_features=categorical_features_names, model_size_reg=0)\ncatbr.fit(X, y_train)\npreds = catbr.predict(X_v)\nmean_absolute_error(y_valid, preds)\n\n165007.46639148306\n\n\n\ncatbr = CatBoostRegressor(n_estimators=100, random_state=42, thread_count=-1, logging_level = 'Silent', cat_features=categorical_features_names, model_size_reg=1)\ncatbr.fit(X, y_train)\npreds = catbr.predict(X_v)\nmean_absolute_error(y_valid, preds)\n\n160827.54657606702\n\n\nNote that any combination of several categorical features could be considered as a new one. Although it is not mentioned in the documentation, this parameter value has to be smaller than 15.\n\ncatbr = CatBoostRegressor(n_estimators=100, random_state=42, thread_count=-1, logging_level = 'Silent', cat_features=categorical_features_names, max_ctr_complexity=6)\ncatbr.fit(X, y_train)\npreds = catbr.predict(X_v)\nmean_absolute_error(y_valid, preds)\n\n163529.3517553931\n\n\n\ncatbr = CatBoostRegressor(n_estimators=100, random_state=42, thread_count=-1, logging_level = 'Silent', cat_features=categorical_features_names, max_ctr_complexity=0)\ncatbr.fit(X, y_train)\npreds = catbr.predict(X_v)\nmean_absolute_error(y_valid, preds)\n\n164721.46472522244\n\n\nCounter method is very similar to the traditional Frequency Encoding\n\ncatbr = CatBoostRegressor(n_estimators=100, random_state=42, thread_count=-1, logging_level = 'Silent', cat_features=categorical_features_names, simple_ctr='Counter', combinations_ctr='Counter')\ncatbr.fit(X, y_train)\npreds = catbr.predict(X_v)\nmean_absolute_error(y_valid, preds)\n\n164993.47954656056\n\n\nNow we proceed to the settings of the encodings methods that require target quantization. The first choice is Borders vs. Buckets\n\ncatbr = CatBoostRegressor(n_estimators=100, random_state=42, thread_count=-1, logging_level = 'Silent', cat_features=categorical_features_names, simple_ctr='Borders', combinations_ctr='Borders')\ncatbr.fit(X, y_train)\npreds = catbr.predict(X_v)\nmean_absolute_error(y_valid, preds)\n\n166920.94141120723\n\n\n\ncatbr = CatBoostRegressor(n_estimators=100, random_state=42, thread_count=-1, logging_level = 'Silent', cat_features=categorical_features_names, simple_ctr='Buckets', combinations_ctr='Buckets')\ncatbr.fit(X, y_train)\npreds = catbr.predict(X_v)\nmean_absolute_error(y_valid, preds)\n\n168168.9386958123\n\n\nIt is quite common to use several encodings for a categorical feature. For instance, CatBoost creates 4 different encodings for each categorical feature by default. By default, CatBoost uses several encoding techniques to encode each categorical feature.\n\nFirst it uses Borders method with one target border TargetBorderCount=1 (in our example for each categorical feature we just want to see if it makes the car more expensive). The obtained float encodings are further discretized into CtrBorderCount=15 different values. Three values of Prior parameter are used to create 3 three different encodings: Prior=0/1:Prior=0.5/1:Prior=1/1\nAlso for each categorical feature, we create an encoding with Counter method. The number of categorical encoding value borders CtrBorderCount is also equal to 15, and only one value of Prior=0/1 is used.\n\n\ncatbr = CatBoostRegressor(n_estimators=100, random_state=42, thread_count=-1, logging_level = 'Silent', cat_features=categorical_features_names, ctr_target_border_count=10)\ncatbr.fit(X, y_train)\npreds = catbr.predict(X_v)\nmean_absolute_error(y_valid, preds)\n\n166281.68070428775\n\n\n\ncatbr = CatBoostRegressor(n_estimators=100, random_state=42, thread_count=-1, logging_level = 'Silent', cat_features=categorical_features_names)\ncatbr.fit(X, y_train)\n\n&lt;catboost.core.CatBoostRegressor at 0x7f6007ab7a10&gt;"
  },
  {
    "objectID": "Chapter_8_Lab.html#voting-and-stacking",
    "href": "Chapter_8_Lab.html#voting-and-stacking",
    "title": "8  Decision Trees",
    "section": "8.10 Voting and Stacking",
    "text": "8.10 Voting and Stacking\n\nCarseats = pd.read_csv('/content/drive/MyDrive/Lab/Data/Carseats.csv')\n\nprint(Carseats.shape)\n# Check for missing values\nassert Carseats.isnull().sum().sum() == 0\n\n# Create binary variable High 1 if Sales &gt; 8\nCarseats['High'] = (Carseats['Sales'] &gt; 8).astype(np.float64)\n\npredictors = Carseats.drop([\"Sales\",\"High\"], axis=1).columns\nX = pd.get_dummies(Carseats[predictors], drop_first=True) #sklearn does not have built-in support for categorical variable \ny = Carseats[\"High\"]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.5, random_state=1)\n\n(400, 11)\n\n\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB \nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline\n\nclf1 = make_pipeline(StandardScaler(),KNeighborsClassifier(n_neighbors=10))\nclf2 = DecisionTreeClassifier(random_state=1)\nclf3 = GaussianNB()\n\nestimators = [\n    (\"knn\", clf1),   \n    (\"dc\", clf2),\n    (\"nb\", clf3)\n]\n\nvclf = VotingClassifier(estimators=estimators, voting='hard')\n\nk = 5\nkf5 = KFold(n_splits=k, shuffle=True, random_state=1)\n\nprint('5-fold cross validation:\\n')\n\n\nfor clf, label in zip([clf1, clf2, clf3, vclf], \n                      ['KNN', \n                       'DecisionTreeClassifier', \n                       'Naive Bayes',\n                       'StackingClassifier']):\n\n    scores = cross_val_score(clf, X_train, y_train, cv=kf5, scoring='accuracy')\n    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" \n          % (scores.mean(), scores.std(), label))\n\n5-fold cross validation:\n\nAccuracy: 0.78 (+/- 0.06) [KNN]\nAccuracy: 0.73 (+/- 0.07) [DecisionTreeClassifier]\nAccuracy: 0.79 (+/- 0.07) [Naive Bayes]\nAccuracy: 0.82 (+/- 0.04) [StackingClassifier]\n\n\nCheck https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html for more details\n\nsclf = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression())\n\nprint('5-fold cross validation:\\n')\n\nfor clf, label in zip([clf1, clf2, clf3, sclf], \n                      ['KNN', \n                       'DecisionTreeClassifier', \n                       'Naive Bayes',\n                       'StackingClassifier']):\n\n    scores = cross_val_score(clf, X_train, y_train, cv=kf5, scoring='accuracy')\n    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" \n          % (scores.mean(), scores.std(), label))\n\n5-fold cross validation:\n\nAccuracy: 0.78 (+/- 0.06) [KNN]\nAccuracy: 0.73 (+/- 0.07) [DecisionTreeClassifier]\nAccuracy: 0.79 (+/- 0.07) [Naive Bayes]\nAccuracy: 0.81 (+/- 0.05) [StackingClassifier]\n\n\nCheck https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.StackingClassifier.html or http://rasbt.github.io/mlxtend/user_guide/classifier/StackingCVClassifier/#stackingcvclassifier for more details."
  },
  {
    "objectID": "Chapter_9_Lab.html#setup",
    "href": "Chapter_9_Lab.html#setup",
    "title": "9  Support Vector Machines",
    "section": "9.1 Setup",
    "text": "9.1 Setup\nSVC, NuSVC and LinearSVC are classes capable of performing binary and multi-class classification on a dataset. SVC and NuSVC are similar methods, but accept slightly different sets of parameters and have different mathematical formulations. On the other hand, LinearSVC is another (faster) implementation of Support Vector Classification for the case of a linear kernel. Note that LinearSVC does not accept parameter kernel, as this is assumed to be linear. It also lacks some of the attributes of SVC and NuSVC, like support_.\nA C argument allows us to specify the cost of a violation to the margin. When the C argument is small, then the margins will be wide and many support vectors will be on the margin or will violate the margin. When the C argument is large, then the margins will be narrow and there will be few support vectors on the margin or violating the margin.\n\nimport pandas as pd\nimport numpy as np\n\nfrom sklearn.utils import resample\nfrom sklearn.svm import SVC, NuSVC, LinearSVC\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.model_selection import train_test_split, KFold, cross_val_score\nfrom sklearn.metrics import confusion_matrix, roc_auc_score, mean_squared_error, roc_curve, accuracy_score\nfrom sklearn.model_selection import GridSearchCV\n\nfrom mlxtend.plotting import plot_decision_regions\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\n#plt.style.use('seaborn-white')\nplt.style.use('seaborn') # pretty matplotlib plots\nsns.set_context(\"notebook\", font_scale=1.5, rc={\"lines.linewidth\": 2.5})"
  },
  {
    "objectID": "Chapter_9_Lab.html#support-vector-classifier",
    "href": "Chapter_9_Lab.html#support-vector-classifier",
    "title": "9  Support Vector Machines",
    "section": "9.2 Support Vector Classifier",
    "text": "9.2 Support Vector Classifier\nWe now use the function to fit the support vector classifier for a given value of the C parameter. Here we demonstrate the use of this function on a two-dimensional example so that we can plot the resulting decision boundary. We begin by generating the observations, which belong to two classes, and checking whether the classes are linearly separable.\n\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\nMounted at /content/drive\n\n\n\n# Generating random data: 20 observations of 2 features and divide into two classes.\nnp.random.seed(5)\nX = np.random.randn(20,2)\ny = np.repeat([-1,1], 10)\n\nX[y == 1, :] += 1\nplt.scatter(X[:,0], X[:,1], s=70, c=y, cmap=plt.cm.Paired)\nplt.xlabel('X1')\nplt.ylabel('X2')\n\nText(0, 0.5, 'X2')\n\n\n\n\n\n\nsvc_clf = SVC(kernel='linear', C=10)\nsvc_clf.fit(X, y)\n\nSVC(C=10, kernel='linear')\n\n\nThe region of feature space that will be assigned to the −1 class is shown in light blue, and the region that will be assigned to the +1 class is shown in brown. The decision boundary between the two classes is linear (because we used the argument kernel=\"linear).\n\nplot_decision_regions(X, y, clf=svc_clf, X_highlight=svc_clf.support_vectors_, legend=2)\nprint('Number of support vectors: ', svc_clf.support_.shape[0])\n\n/usr/local/lib/python3.7/dist-packages/mlxtend/plotting/decision_regions.py:244: MatplotlibDeprecationWarning: Passing unsupported keyword arguments to axis() will raise a TypeError in 3.3.\n  ax.axis(xmin=xx.min(), xmax=xx.max(), y_min=yy.min(), y_max=yy.max())\n/usr/local/lib/python3.7/dist-packages/mlxtend/plotting/decision_regions.py:313: MatplotlibDeprecationWarning: Using a string of single character colors as a color sequence is deprecated. Use an explicit list instead.\n  **scatter_highlight_kwargs)\n\n\nNumber of support vectors:  13\n\n\n\n\n\nWe see here that there are 13 support vectors. We can determine their identities as follows:\n\n# get support vectors\npd.DataFrame(svc_clf.support_vectors_, index=svc_clf.support_)\n\n\n  \n    \n      \n\n\n\n\n\n\n0\n1\n\n\n\n\n0\n0.441227\n-0.330870\n\n\n1\n2.430771\n-0.252092\n\n\n2\n0.109610\n1.582481\n\n\n4\n0.187603\n-0.329870\n\n\n6\n-0.358829\n0.603472\n\n\n8\n1.151391\n1.857331\n\n\n10\n0.019392\n0.143147\n\n\n11\n0.128121\n0.577492\n\n\n13\n1.059144\n0.636689\n\n\n14\n1.003289\n0.894070\n\n\n15\n1.793053\n0.368428\n\n\n16\n0.993805\n0.898932\n\n\n17\n0.947692\n1.249218\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n# get indices of support vectors\nsvc_clf.support_\n\narray([ 0,  1,  2,  4,  6,  8, 10, 11, 13, 14, 15, 16, 17], dtype=int32)\n\n\n\n# get number of support vectors for each class\nsvc_clf.n_support_\n\narray([6, 7], dtype=int32)\n\n\nWhat if we instead used a smaller value of the C parameter?\n\nsvc_clf2 = SVC(kernel='linear', C=0.1)\nsvc_clf2.fit(X, y)\n\nplot_decision_regions(X, y, clf=svc_clf2, X_highlight=svc_clf2.support_vectors_)\nprint('Number of support vectors: ', svc_clf2.support_.shape[0])\n\nNumber of support vectors:  16\n\n\n/usr/local/lib/python3.7/dist-packages/mlxtend/plotting/decision_regions.py:244: MatplotlibDeprecationWarning: Passing unsupported keyword arguments to axis() will raise a TypeError in 3.3.\n  ax.axis(xmin=xx.min(), xmax=xx.max(), y_min=yy.min(), y_max=yy.max())\n/usr/local/lib/python3.7/dist-packages/mlxtend/plotting/decision_regions.py:313: MatplotlibDeprecationWarning: Using a string of single character colors as a color sequence is deprecated. Use an explicit list instead.\n  **scatter_highlight_kwargs)\n\n\n\n\n\nNow that a smaller value of the C parameter is being used, we obtain a larger number of support vectors, because the margin is now wider.\nWe now perform CV to determine the optimal value of C:\n\nk = 5\nkf5 = KFold(n_splits=k, shuffle=True, random_state=1)\n\nsvc_clf3 = SVC(kernel='linear')\nc_space = np.array([0.001, 0.01, 0.1, 1, 5, 10, 100])\nparam_grid = {'C': c_space}\n\ntune = GridSearchCV(svc_clf3, param_grid, cv=kf5)\ntune.fit(X, y)\n\nprint(tune.best_params_)\npd.DataFrame(tune.cv_results_)\n\n{'C': 0.1}\n\n\n\n  \n    \n      \n\n\n\n\n\n\nmean_fit_time\nstd_fit_time\nmean_score_time\nstd_score_time\nparam_C\nparams\nsplit0_test_score\nsplit1_test_score\nsplit2_test_score\nsplit3_test_score\nsplit4_test_score\nmean_test_score\nstd_test_score\nrank_test_score\n\n\n\n\n0\n0.000982\n0.000299\n0.000763\n0.000544\n0.001\n{'C': 0.001}\n0.75\n0.75\n0.25\n0.25\n0.75\n0.55\n0.244949\n6\n\n\n1\n0.000736\n0.000125\n0.000371\n0.000064\n0.01\n{'C': 0.01}\n0.75\n0.75\n0.25\n0.25\n0.75\n0.55\n0.244949\n6\n\n\n2\n0.000595\n0.000018\n0.000331\n0.000020\n0.1\n{'C': 0.1}\n0.75\n0.75\n0.75\n1.00\n0.50\n0.75\n0.158114\n1\n\n\n3\n0.000583\n0.000026\n0.000328\n0.000016\n1.0\n{'C': 1.0}\n0.75\n0.75\n0.50\n1.00\n0.50\n0.70\n0.187083\n2\n\n\n4\n0.000604\n0.000034\n0.000342\n0.000028\n5.0\n{'C': 5.0}\n0.75\n0.75\n0.50\n1.00\n0.50\n0.70\n0.187083\n2\n\n\n5\n0.000690\n0.000087\n0.000394\n0.000139\n10.0\n{'C': 10.0}\n0.75\n0.75\n0.50\n1.00\n0.50\n0.70\n0.187083\n2\n\n\n6\n0.000700\n0.000021\n0.000340\n0.000011\n100.0\n{'C': 100.0}\n0.75\n0.75\n0.50\n0.50\n0.50\n0.60\n0.122474\n5\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nC=0.1 is best according to GridSearchCV.\nAs usual, the predict() function can be used to predict the class label on a set of test observations, at any given value of the C parameter. Let’s generate a test data set:\n\nnp.random.seed(1)\nX_test = np.random.randn(20,2)\ny_test = resample(np.concatenate((np.ones(10, dtype=np.int64)*-1, np.ones(10, dtype=np.int64))),\n                  replace=True,\n                  n_samples=20, random_state=1)\nX_test[y_test == 1, :] += 1\n\n\ny_pred = tune.predict(X_test)\npd.DataFrame(confusion_matrix(y_test, y_pred), index=tune.classes_, columns=tune.classes_)\n\n\n  \n    \n      \n\n\n\n\n\n\n-1\n1\n\n\n\n\n-1\n7\n2\n\n\n1\n1\n10\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nThus, with this value of C, 17 of the test observations are correctly classified. What if we had instead used C = 10?\n\nsvc_clf4 = SVC(C=10, kernel='linear')\nsvc_clf4.fit(X, y)\ny_pred = svc_clf4.predict(X_test)\npd.DataFrame(confusion_matrix(y_test, y_pred), index=svc_clf4.classes_, columns=svc_clf4.classes_)\n\n\n  \n    \n      \n\n\n\n\n\n\n-1\n1\n\n\n\n\n-1\n7\n2\n\n\n1\n4\n7\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nIn this case three additional observations are misclassified.\nNow consider a situation in which the two classes are linearly separable. Then we can find a separating hyperplane using the SVC() function. We first further separate the two classes in our simulated data so that they are linearly separable:\n\nX[y == 1, :] += 1.6\nplt.scatter(X[:, 0], X[:, 1], c=(y+5)/2, cmap='Spectral')\n\n&lt;matplotlib.collections.PathCollection at 0x7fdd359cafd0&gt;\n\n\n\n\n\nNow the observations are just barely linearly separable. We fit the support vector classifier and plot the resulting hyperplane, using a very large value of C so that no observations are misclassified.\n\nsvc_clf5 = SVC(kernel='linear', C=1e5)\nsvc_clf5 .fit(X, y)\n\nplot_decision_regions(X, y, clf=svc_clf5, X_highlight=svc_clf5.support_vectors_)\nprint('Number of support vectors: ', svc_clf5.support_.shape[0])\n\n/usr/local/lib/python3.7/dist-packages/mlxtend/plotting/decision_regions.py:244: MatplotlibDeprecationWarning: Passing unsupported keyword arguments to axis() will raise a TypeError in 3.3.\n  ax.axis(xmin=xx.min(), xmax=xx.max(), y_min=yy.min(), y_max=yy.max())\n/usr/local/lib/python3.7/dist-packages/mlxtend/plotting/decision_regions.py:313: MatplotlibDeprecationWarning: Using a string of single character colors as a color sequence is deprecated. Use an explicit list instead.\n  **scatter_highlight_kwargs)\n\n\nNumber of support vectors:  3\n\n\n\n\n\nNo training errors were made and only three support vectors were used. However, we can see from the figure that the margin is very narrow (because the observations that are not support vectors, indicated as circles, are very close to the decision boundary). It seems likely that this model will perform poorly on test data. Let’s try a smaller value of C:\n\nsvc_clf6 = SVC(kernel='linear', C=1)\nsvc_clf6.fit(X, y)\n\nplot_decision_regions(X, y, clf=svc_clf6, X_highlight=svc_clf6.support_vectors_)\n\n/usr/local/lib/python3.7/dist-packages/mlxtend/plotting/decision_regions.py:244: MatplotlibDeprecationWarning: Passing unsupported keyword arguments to axis() will raise a TypeError in 3.3.\n  ax.axis(xmin=xx.min(), xmax=xx.max(), y_min=yy.min(), y_max=yy.max())\n/usr/local/lib/python3.7/dist-packages/mlxtend/plotting/decision_regions.py:313: MatplotlibDeprecationWarning: Using a string of single character colors as a color sequence is deprecated. Use an explicit list instead.\n  **scatter_highlight_kwargs)\n\n\n&lt;matplotlib.axes._subplots.AxesSubplot at 0x7fdd3583ef10&gt;\n\n\n\n\n\nUsing C=1, we misclassify a training observation, but we also obtain a much wider margin and make use of five support vectors. It seems likely that this model will perform better on test data than the model with C."
  },
  {
    "objectID": "Chapter_9_Lab.html#the-kernel-method",
    "href": "Chapter_9_Lab.html#the-kernel-method",
    "title": "9  Support Vector Machines",
    "section": "9.3 The Kernel Method",
    "text": "9.3 The Kernel Method\n\nfrom sklearn.datasets import make_circles\n\ndef feature_map_1(X):  \n    return np.asarray((np.sqrt(2)*X[:,0]*X[:,1], X[:,0]**2, X[:,1]**2)).T\n\ndef my_kernel_1(X,Y):\n    return np.dot(feature_map_1(X),feature_map_1(Y).T )\n\n\n## Reference https://xavierbourretsicotte.github.io/Kernel_feature_map.html\n\nX, y = make_circles(100, factor=.1, noise=.1, random_state = 0)\nZ = feature_map_1(X)\n\n#2D scatter plot\nfig = plt.figure(figsize = (16,8))\nax = fig.add_subplot(1, 2, 1)\nax.scatter(X[:,0], X[:,1], c = y, cmap = 'viridis')\nax.set_xlabel('$x_1$')\nax.set_ylabel('$x_2$')\nax.set_title('Original data')\n\n#3D scatter plot\nax = fig.add_subplot(1, 2, 2, projection='3d')\nax.scatter3D(Z[:,0],Z[:,1], Z[:,2],c = y, cmap = 'viridis' ) #,rstride = 5, cstride = 5, cmap = 'jet', alpha = .4, edgecolor = 'none' )\nax.set_xlabel('$z_1$')\nax.set_ylabel('$z_2$')\nax.set_zlabel('$z_3$')\nax.set_title('Transformed data: ')\n\n#SVM using linear kernel with feature map 1\nclf = SVC(C = 1, kernel = 'linear')\nclf.fit(Z, y) \n\nw = clf.coef_.flatten()\nb = clf.intercept_.flatten()\nprint('w=',w,'b=',b)\n\n# create x,y\nxx, yy = np.meshgrid(np.linspace(-1,1), np.linspace(0,1))\n\n# calculate corresponding z\n# The equation of the separating plane is given by all x in R^3 such that:\n# np.dot(w, x) + b = 0. We should solve for the last coordinate\nboundary = (-w[0] * xx - w[1] * yy - b) * 1. /w[2]\n\n\n# plot the surface\n\nax.plot_surface(xx, yy, boundary, alpha = .3)\nax.set_ylim(.2,1.2)\nax.set_zlim(-.9,1.1)\n#ax.view_init(0, 260)\n\nplt.show()\n\nw= [-0.05481854 -2.53191791 -2.52028513] b= [1.14976292]\n\n\n\n\n\n\n#SVM using kernel 1 - feature map 1\nclf2 = SVC(kernel=my_kernel_1)\nclf2.fit(X, y) \n\n# predict on training examples - print accuracy score\nprint('Accuracy score using feature map ',accuracy_score(y, clf2.predict(X)))\nprint('Accuracy score using feature map ',accuracy_score(y, clf.predict(Z)))\n\nAccuracy score using feature map  1.0\nAccuracy score using feature map  1.0\n\n\n\n#Initialize data\nh = .01 #Stepsize in the mesh\nx_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\ny_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n\n# Predict on meshgrid\nZ = clf2.predict(np.c_[xx.ravel(), yy.ravel()])\n\n# Put the result into a color plot\nZ = Z.reshape(xx.shape)\nplt.figure(figsize = (7,7))\nplt.contourf(xx, yy, Z, 1, colors = ['darkblue','yellow'], alpha = .1) # plot coutour surface\nplt.contour(xx, yy, Z, cmap = 'viridis') # plot coutour line\n\n# Plot also the training points\nplt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', edgecolors = 'k')\nplt.title('Support Vector Machine with polynomial'\n          ' kernel')\nplt.xlabel('$x_1$')\nplt.ylabel('$x_2$')\n\nText(0, 0.5, '$x_2$')"
  },
  {
    "objectID": "Chapter_9_Lab.html#support-vector-machine",
    "href": "Chapter_9_Lab.html#support-vector-machine",
    "title": "9  Support Vector Machines",
    "section": "9.4 Support Vector Machine",
    "text": "9.4 Support Vector Machine\nIn order to fit an SVM using a non-linear kernel, we once again use the SVC() function. However, now we use a different value of the parameter kernel. To fit an SVM with a polynomial kernel we use kernel=\"poly\", and to fit an SVM with a radial kernel we use kernel=\"rbf\". In the former case we also use the degree argument to specify a degree for the polynomial kernel, and in the latter case we use \\(\\gamma\\) to specify a value of \\(\\gamma\\) for the radial basis kernel.\nLet’s generate some data with a non-linear class boundary:\n\nnp.random.seed(42)\n\nX = np.random.normal(size=400).reshape(200, 2)\nX[0:100, :] += 2\nX[100:150, :] -= 2\ny = np.concatenate((np.full(150, -1, dtype=np.int64), np.full(50, 1, dtype=np.int64)))\n\nplt.scatter(X[:, 0], X[:, 1], c=y, cmap='Spectral')\n\n&lt;matplotlib.collections.PathCollection at 0x7fdd32464b50&gt;\n\n\n\n\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.5, test_size=0.5, random_state=1)\n\nSee how one class is kind of stuck in the middle of another class? This suggests that we might want to use a radial kernel in our SVM. Now let’s fit the training data using the SVC() function with a radial kernel and \\(\\gamma = 1\\):\n\nsvm = SVC(kernel='rbf', gamma=1, C=1)\nsvm.fit(X_train, y_train)\n\nplot_decision_regions(X_train, y_train, clf=svm, X_highlight=svm.support_vectors_);\n\n/usr/local/lib/python3.7/dist-packages/mlxtend/plotting/decision_regions.py:244: MatplotlibDeprecationWarning: Passing unsupported keyword arguments to axis() will raise a TypeError in 3.3.\n  ax.axis(xmin=xx.min(), xmax=xx.max(), y_min=yy.min(), y_max=yy.max())\n/usr/local/lib/python3.7/dist-packages/mlxtend/plotting/decision_regions.py:313: MatplotlibDeprecationWarning: Using a string of single character colors as a color sequence is deprecated. Use an explicit list instead.\n  **scatter_highlight_kwargs)\n\n\n\n\n\nNot too shabby! The plot shows that the resulting SVM has a decidedly non-linear boundary. We can see from the figure that there are a fair number of training errors in this SVM fit. If we increase the value of C, we can reduce the number of training errors:\n\nsvm2 = SVC(kernel='rbf', gamma=1, C=1e5)\nsvm2.fit(X_train, y_train)\n\nplot_decision_regions(X_train, y_train, clf=svm2, X_highlight=svm2.support_vectors_)\n\n/usr/local/lib/python3.7/dist-packages/mlxtend/plotting/decision_regions.py:244: MatplotlibDeprecationWarning: Passing unsupported keyword arguments to axis() will raise a TypeError in 3.3.\n  ax.axis(xmin=xx.min(), xmax=xx.max(), y_min=yy.min(), y_max=yy.max())\n/usr/local/lib/python3.7/dist-packages/mlxtend/plotting/decision_regions.py:313: MatplotlibDeprecationWarning: Using a string of single character colors as a color sequence is deprecated. Use an explicit list instead.\n  **scatter_highlight_kwargs)\n\n\n&lt;matplotlib.axes._subplots.AxesSubplot at 0x7fdd32506a90&gt;\n\n\n\n\n\nHowever, this comes at the price of a more irregular decision boundary that seems to be at risk of overfitting the data. We can perform cross-validation using GridSearchCV() to select the best choice of \\(\\gamma\\) and cost for an SVM with a radial kernel:\n\nsvm3 = SVC(kernel='rbf')\nc_space = np.array([0.1, 1, 10, 100, 1000])\ng_space = np.array([0.5, 1, 2, 3, 4])\nparam_grid = {'C': c_space, 'gamma': g_space}\n\ntune = GridSearchCV(svm3, param_grid, cv=10)\ntune.fit(X_train, y_train)\n\ntune.cv_results_\ntune.best_params_\n\n{'C': 1.0, 'gamma': 0.5}\n\n\nTherefore, the best choice of parameters involves C=1 and gamma=0.5. We can view the test set predictions for this model by applying the predict() function to the test data:\n\ny_pred = tune.predict(X_test)\npd.DataFrame(confusion_matrix(y_test, y_pred), index=tune.classes_, columns=tune.classes_)\n\n\n  \n    \n      \n\n\n\n\n\n\n-1\n1\n\n\n\n\n-1\n67\n3\n\n\n1\n9\n21\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\\(12\\,\\%\\) of test observations are misclassified by this SVM."
  },
  {
    "objectID": "Chapter_9_Lab.html#roc-curves",
    "href": "Chapter_9_Lab.html#roc-curves",
    "title": "9  Support Vector Machines",
    "section": "9.5 ROC Curves",
    "text": "9.5 ROC Curves\nThe roc_curve and auc() function from the sklearn.metrics package can be used to produce ROC curves such as those we saw in lecture:\n\ndef plot_roc(cls,X,y,title):\n    probs = cls.predict_proba(X)[:,1]\n    fpr, tpr, _ = roc_curve(y,probs)\n    auc = roc_auc_score(y,probs)\n    \n    plt.figure(figsize = (6,6))\n    plt.plot(fpr,tpr,label=\"auc=\"+str(auc)[0:4],c = 'r')\n    plt.plot([0,1],[0,1],alpha = 0.1,c = 'b')\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title(title)\n    plt.legend(loc=\"lower right\")\n\nSVMs and support vector classifiers output class labels for each observation. However, it is also possible to obtain fitted values for each observation, which are the numerical scores used to obtain the class labels. For instance, in the case of a support vector classifier, the fitted value for an observation \\(X= (X_1, X_2, \\ldots, X_p)^T\\) takes the form \\(\\hat{\\beta}_0 + \\hat{\\beta}_1 X_1 + \\hat{\\beta}_2 X_2 + \\cdots + \\hat{\\beta}_p X_p\\). For an SVM with a non-linear kernel, the equation that yields the fitted value is given in (9.23). In essence, the sign of the fitted value determines on which side of the decision boundary the observation lies. Therefore, the relationship between the fitted value and the class prediction for a given observation is simple: if the fitted value exceeds zero then the observation is assigned to one class, and if it is less than zero then it is assigned to the other. Then the predict() function will output the fitted values.\nLet’s start by fitting two models, one more flexible than the other:\n\n# More constrained model\nsvm4 = SVC(kernel='rbf', gamma=2, C=1, probability=True)\nsvm4.fit(X_train, y_train)\n\n# More flexible model\nsvm5 = SVC(kernel='rbf', gamma=50, C=1, probability=True)\nsvm5.fit(X_train, y_train)\n\nSVC(C=1, gamma=50, probability=True)\n\n\nSVM appears to be producing accurate predictions. By increasing \\(\\gamma\\) we can produce a more flexible fit and generate further improvements in accuracy.\n\nplot_roc(svm4,X_train,y_train,'training roc, gamma = 2')\nplot_roc(svm5,X_train,y_train,'training roc, gamma = 50')\n\n\n\n\n\n\n\nHowever, these ROC curves are all on the training data. We are really more interested in the level of prediction accuracy on the test data. When we compute the ROC curves on the test data, the model with \\(\\gamma=2\\) appears to provide the most accurate results.\n\nplot_roc(svm4,X_test,y_test,'training roc, gamma = 2')\nplot_roc(svm5,X_test,y_test,'training roc, gamma = 50')"
  },
  {
    "objectID": "Chapter_9_Lab.html#svm-with-multiple-classes",
    "href": "Chapter_9_Lab.html#svm-with-multiple-classes",
    "title": "9  Support Vector Machines",
    "section": "9.6 SVM with Multiple Classes",
    "text": "9.6 SVM with Multiple Classes\nIf the response is a factor containing more than two levels, then the svm() function will perform multi-class classification using the one-versus-all approach. We explore that setting here by generating a third class of observations.\n\nnp.random.seed(1)\n\nX = np.random.normal(size=400).reshape(200, 2)\nX[0:100, :] += 2\nX[100:150, :] -= 2\ny = np.concatenate((np.full(150, 1, dtype=np.int64), np.full(50, 2, dtype=np.int64)))\n\nX = np.concatenate((X, np.random.normal(size=100).reshape(50, 2)))\ny = np.concatenate((y, np.full(50, 0, dtype=np.int64)))\n\nX[y == 0, 1] += 2\n\nplt.scatter(X[:, 0], X[:, 1], c=y+1, cmap='Spectral');\n\n\n\n\nWe now fit an SVM to the data:\n\nsvm_m = SVC(kernel='rbf', C=10, gamma=1, decision_function_shape='ovr')\nsvm_m.fit(X, y)\nprint(svm_m.score(X, y))\nplot_decision_regions(X, y, clf=svm_m, X_highlight=svm_m.support_vectors_)\n\n0.868\n\n\n/usr/local/lib/python3.7/dist-packages/mlxtend/plotting/decision_regions.py:244: MatplotlibDeprecationWarning: Passing unsupported keyword arguments to axis() will raise a TypeError in 3.3.\n  ax.axis(xmin=xx.min(), xmax=xx.max(), y_min=yy.min(), y_max=yy.max())\n/usr/local/lib/python3.7/dist-packages/mlxtend/plotting/decision_regions.py:313: MatplotlibDeprecationWarning: Using a string of single character colors as a color sequence is deprecated. Use an explicit list instead.\n  **scatter_highlight_kwargs)\n\n\n&lt;matplotlib.axes._subplots.AxesSubplot at 0x7fdd30a2e650&gt;"
  },
  {
    "objectID": "Chapter_9_Lab.html#application-to-gene-expression-data",
    "href": "Chapter_9_Lab.html#application-to-gene-expression-data",
    "title": "9  Support Vector Machines",
    "section": "9.7 Application to Gene Expression Data",
    "text": "9.7 Application to Gene Expression Data\nWe now examine the Khan data set, which consists of a number of tissue samples corresponding to four distinct types of small round blue cell tumors. For each tissue sample, gene expression measurements are available. The data set consists of training data, xtrain and ytrain, and testing data, xtest and ytest.\n\nkhan_X_train = pd.read_csv('/content/drive/MyDrive/Lab/Data/Khan_xtrain.csv', index_col=0)\nkhan_y_train = pd.read_csv('/content/drive/MyDrive/Lab/Data/Khan_ytrain.csv', index_col=0)\nkhan_X_test = pd.read_csv('/content/drive/MyDrive/Lab/Data/Khan_xtest.csv', index_col=0)\nkhan_y_test = pd.read_csv('/content/drive/MyDrive/Lab/Data/Khan_ytest.csv', index_col=0)\n\nWe examine the dimension of the data:\n\nkhan_X_train.shape, khan_X_test.shape, len(khan_y_train), len(khan_y_test)\n\n((63, 2308), (20, 2308), 63, 20)\n\n\nThis data set consists of expression measurements for \\(2{,}308\\) genes. The training and test sets consist of \\(63\\) and \\(20\\) observations respectively.\nWe will use a support vector approach to predict cancer subtype using gene expression measurements. In this data set, there are a very large number of features relative to the number of observations. This suggests that we should use a linear kernel, because the additional flexibility that will result from using a polynomial or radial kernel is unnecessary.\n\nkhan_y_train.iloc[:, 0].value_counts(sort=False)\n\n2    23\n4    20\n3    12\n1     8\nName: x, dtype: int64\n\n\n\nkhan_y_test.iloc[:, 0].value_counts(sort=False)\n\n3    6\n2    6\n4    5\n1    3\nName: x, dtype: int64\n\n\n\ngene_clf = SVC(kernel='linear', C=10)\ngene_clf.fit(khan_X_train, khan_y_train.iloc[:, 0])\n\nkhan_y_train_pred = gene_clf.predict(khan_X_train)\n\npd.DataFrame(confusion_matrix(khan_y_train_pred, khan_y_train), index=gene_clf.classes_, columns=gene_clf.classes_)\n\n\n  \n    \n      \n\n\n\n\n\n\n1\n2\n3\n4\n\n\n\n\n1\n8\n0\n0\n0\n\n\n2\n0\n23\n0\n0\n\n\n3\n0\n0\n12\n0\n\n\n4\n0\n0\n0\n20\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nWe see that there are no training errors. In fact, this is not surprising, because the large number of variables relative to the number of observations implies that it is easy to find hyperplanes that fully separate the classes. We are most interested not in the support vector classifier’s performance on the training observations, but rather its performance on the test observations.\n\nkhan_y_test_pred = gene_clf.predict(khan_X_test)\npd.DataFrame(confusion_matrix(khan_y_test_pred, khan_y_test), index=gene_clf.classes_, columns=gene_clf.classes_)\n\n\n  \n    \n      \n\n\n\n\n\n\n1\n2\n3\n4\n\n\n\n\n1\n3\n0\n0\n0\n\n\n2\n0\n6\n2\n0\n\n\n3\n0\n0\n4\n0\n\n\n4\n0\n0\n0\n5\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nWe see that using C = 10 yields two test set errors on this data."
  },
  {
    "objectID": "Chapter_9_Lab.html#sgd-classifier",
    "href": "Chapter_9_Lab.html#sgd-classifier",
    "title": "9  Support Vector Machines",
    "section": "9.8 SGD Classifier",
    "text": "9.8 SGD Classifier\n\nfrom sklearn.datasets import make_blobs\n\n# we create 50 separable points\nX, Y = make_blobs(n_samples=50, centers=2, random_state=0, cluster_std=0.60)\n\n# fit the model\nclf = SGDClassifier(loss=\"hinge\", alpha=0.01, max_iter=200)\n\nclf.fit(X, Y)\n\n# plot the line, the points, and the nearest vectors to the plane\nxx = np.linspace(-1, 5, 10)\nyy = np.linspace(-1, 5, 10)\n\nX1, X2 = np.meshgrid(xx, yy)\nZ = np.empty(X1.shape)\nfor (i, j), val in np.ndenumerate(X1):\n    x1 = val\n    x2 = X2[i, j]\n    p = clf.decision_function([[x1, x2]])\n    Z[i, j] = p[0]\nlevels = [-1.0, 0.0, 1.0]\nlinestyles = [\"dashed\", \"solid\", \"dashed\"]\ncolors = \"k\"\nplt.contour(X1, X2, Z, levels, colors=colors, linestyles=linestyles)\nplt.scatter(X[:, 0], X[:, 1], c=Y, cmap=plt.cm.Paired, edgecolor=\"black\", s=20)\n\nplt.axis(\"tight\")\nplt.show()\n\n\n\n\n\nnp.random.seed(1)\n\nX = np.random.normal(size=400).reshape(200, 2)\nX[0:100, :] += 2\nX[100:150, :] -= 2\ny = np.concatenate((np.full(150, 1, dtype=np.int64), np.full(50, 2, dtype=np.int64)))\n\nX = np.concatenate((X, np.random.normal(size=100).reshape(50, 2)))\ny = np.concatenate((y, np.full(50, 0, dtype=np.int64)))\n\nX[y == 0, 1] += 2\n\nplt.scatter(X[:, 0], X[:, 1], c=y+1, cmap='Spectral');\n\n\n\n\n\nfrom sklearn.kernel_approximation import RBFSampler\n\nrbf_feature = RBFSampler(gamma=1, random_state=1)\nX_features = rbf_feature.fit_transform(X)\nclf = SGDClassifier(loss=\"hinge\", alpha=0.01, max_iter=200)\nclf.fit(X_features, y)\nclf.score(X_features, y)\n\n0.84"
  },
  {
    "objectID": "Chapter_12_Lab.html#dimensional-reduction",
    "href": "Chapter_12_Lab.html#dimensional-reduction",
    "title": "10  Unsupervised Learning",
    "section": "10.1 Dimensional reduction",
    "text": "10.1 Dimensional reduction\n\n10.1.1 Principal Components Analysis\nIn this lab, we perform PCA on the USArrests data set. The rows of the data set contain the 50 states, in alphabetical order.\n\n!pip install fancyimpute -qq\n!pip install opentsne -qq\n!pip install umap-learn -qq\n!pip install git+https://github.com/dmuellner/fastcluster -qq\n!pip install hdbscan -qq\n\n  Installing build dependencies ... done\n  Getting requirements to build wheel ... done\n    Preparing wheel metadata ... done\n\n\n\nimport numpy as np\nimport pandas as pd\nfrom itertools import cycle, islice\n\nfrom sklearn.datasets import load_digits\nfrom sklearn.preprocessing import scale\nfrom sklearn.utils import resample\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\n\nfrom sklearn.manifold import TSNE\nfrom sklearn.datasets import make_blobs\nfrom sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\nfrom sklearn import metrics \n\nfrom scipy import linalg\nfrom scipy.cluster.hierarchy import linkage, dendrogram, cut_tree\n\nfrom fancyimpute import SoftImpute #pip install fancyimpute\nimport fastcluster as fc  #pip install git+https://github.com/dmuellner/fastcluster\nfrom openTSNE import TSNE as oTSNE #pip install opentsne\nimport hdbscan #pip install hdbscan\nimport umap \n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\nplt.style.use('seaborn-white')\nsns.set_context(\"notebook\", font_scale=1.5, rc={\"lines.linewidth\": 2.5})\n\n\n#from https://scikit-learn.org/stable/auto_examples/cluster/plot_agglomerative_dendrogram.html\ndef plot_dendrogram(model, **kwargs):\n    # Create linkage matrix and then plot the dendrogram\n\n    # create the counts of samples under each node\n    counts = np.zeros(model.children_.shape[0])\n    n_samples = len(model.labels_)\n    for i, merge in enumerate(model.children_):\n        current_count = 0\n        for child_idx in merge:\n            if child_idx &lt; n_samples:\n                current_count += 1  # leaf node\n            else:\n                current_count += counts[child_idx - n_samples]\n        counts[i] = current_count\n\n    linkage_matrix = np.column_stack(\n        [model.children_, model.distances_, counts]\n    ).astype(float)\n\n    # Plot the corresponding dendrogram\n    dendrogram(linkage_matrix, **kwargs)\n\n\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\nMounted at /content/drive\n\n\n\n#load the data \nUSArrests = pd.read_csv('/content/drive/MyDrive/Lab/Data/USArrests.csv', index_col=0)\nprint(USArrests.shape)\nUSArrests.head()\n\n(50, 4)\n\n\n\n  \n    \n      \n\n\n\n\n\n\nMurder\nAssault\nUrbanPop\nRape\n\n\n\n\nAlabama\n13.2\n236\n58\n21.2\n\n\nAlaska\n10.0\n263\n48\n44.5\n\n\nArizona\n8.1\n294\n80\n31.0\n\n\nArkansas\n8.8\n190\n50\n19.5\n\n\nCalifornia\n9.0\n276\n91\n40.6\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\nUSArrests.columns\n\nIndex(['Murder', 'Assault', 'UrbanPop', 'Rape'], dtype='object')\n\n\nThis data contains states of US - Name - name of the states - Murder - number of arrests for murder per million pop. - Assault - number of arrests for assaults per million pop. - Urbanpop - percentage of population living in urban area - Rape - number of rape cases per million population\nWe first briefly examine the data. We notice that the variables have vastly different means.\n\nUSArrests.mean()\n\nMurder        7.788\nAssault     170.760\nUrbanPop     65.540\nRape         21.232\ndtype: float64\n\n\nWe see that there are on average three times as many rapes as murders, and more than eight times as many assaults as rapes. We can also examine the variances of the four variables.\n\nUSArrests.var()\n\nMurder        18.970465\nAssault     6945.165714\nUrbanPop     209.518776\nRape          87.729159\ndtype: float64\n\n\nNot surprisingly, the variables also have vastly different variances: the UrbanPop variable measures the percentage of the population in each state living in an urban area, which is not a comparable number to the number of rapes in each state per 100,000 individuals. If we failed to scale the variables before performing PCA, then most of the principal components that we observed would be driven by the Assault variable, since it has by far the largest mean and variance. Thus, it is important to standardize the variables to have mean zero and standard deviation one before performing PCA.\n\n# scaling the data \nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(USArrests)\n\nThe mean_ and var_ components correspond to the means and variance of the variables that were used for scaling prior to implementing PCA.\n\nscaler.mean_, scaler.var_\n\n(array([  7.788, 170.76 ,  65.54 ,  21.232]),\n array([  18.591056, 6806.2624  ,  205.3284  ,   85.974576]))\n\n\nNow, the mean is approximately 0, and std is approximately 1 for all the columns\n\nX_scaled.mean(axis=0), X_scaled.var(axis=0)\n\n(array([-7.10542736e-17,  1.38777878e-16, -4.39648318e-16,  8.59312621e-16]),\n array([1., 1., 1., 1.]))\n\n\n\n# using pca to get principal components \npca = PCA()\npca.fit(X_scaled)\n\nPCA()\n\n\n\n#loading vectors\npd.DataFrame(pca.components_,columns = USArrests.columns,index = ['PC1','PC2','PC3','PC4']).T\n\n\n  \n    \n      \n\n\n\n\n\n\nPC1\nPC2\nPC3\nPC4\n\n\n\n\nMurder\n0.535899\n0.418181\n-0.341233\n0.649228\n\n\nAssault\n0.583184\n0.187986\n-0.268148\n-0.743407\n\n\nUrbanPop\n0.278191\n-0.872806\n-0.378016\n0.133878\n\n\nRape\n0.543432\n-0.167319\n0.817778\n0.089024\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nWe see that there are four distinct principal components. This is to be expected because there are in general \\(\\min(n-1,p)\\) informative principal components in a data set with \\(n\\) observations and \\(p\\) variables.\nUsing the PCA() function, we do not need to explicitly multiply the data by the principal component loading vectors in order to obtain the principal component score vectors. Rather the \\(50 \\times 4\\) matrix x has as its columns the principal component score vectors. That is, the \\(k\\)th column is the \\(k\\)th principal component score vector.\n\n# getting the points in principal components\nX_reduced = pca.transform(X_scaled)\nprint(X_reduced.shape)\ndf = pd.DataFrame(X_reduced ,columns = ['PC1','PC2','PC3','PC4'])\ndf.head()\n\n(50, 4)\n\n\n\n  \n    \n      \n\n\n\n\n\n\nPC1\nPC2\nPC3\nPC4\n\n\n\n\n0\n0.985566\n1.133392\n-0.444269\n0.156267\n\n\n1\n1.950138\n1.073213\n2.040003\n-0.438583\n\n\n2\n1.763164\n-0.745957\n0.054781\n-0.834653\n\n\n3\n-0.141420\n1.119797\n0.114574\n-0.182811\n\n\n4\n2.523980\n-1.542934\n0.598557\n-0.341996\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n# biplot \nxvector = pca.components_[0] \nyvector = -pca.components_[1]\n\nxs = X_reduced[:,0] \nys = -X_reduced[:,1]\n\nfig, ax1 = plt.subplots(nrows=1, ncols=1, figsize=(10, 9))\n\n# First Plot : Principal Component Scores 'DF_USArrests_PCA_scores\" \nx_min, x_max = xs.min() - 0.5, xs.max() + 0.5\ny_min, y_max = xs.min() - 0.5, xs.max() + 0.5\nax1.set_xlim(x_min, x_max)\nax1.set_ylim(y_min, y_max)\n\n\nfor i in range(len(xs)):\n    # circles project documents (ie rows from csv) as points onto PC axes\n    # plt.plot(xs[i], ys[i], 'bo')\n    #plt.text(xs[i], ys[i], USArrests.index[i], color='black', alpha=0.8, size=10)\n    ax1.annotate(USArrests.index[i], (xs[i], ys[i]), ha='center', size=10)\n\nax1.set_xlabel(\"1'st Principal Component Scores\")\nax1.set_ylabel(\"2'nd Principal Component Scores\")\n\n# Plot reference lines\nax1.hlines(0,-3.5,3.5, linestyles='dotted', colors='grey')\nax1.vlines(0,-3.5,3.5, linestyles='dotted', colors='grey')\n\n# Second Plot : Principal Component Loadings 'PCs' \nx_min, x_max = -1, 1          \ny_min, y_max = -1, 1\n\nax2 = ax1.twinx().twiny()\nax2.set_xlim(x_min, x_max)\nax2.set_ylim(y_min, y_max)\n\n\n\nfor i in range(len(xvector)):\n    # arrows project features (ie columns from csv) as vectors onto PC axes\n    #plt.arrow(0, 0, xvector[i]*max(xs), yvector[i]*max(ys),\n    #          color='r', width=0.005, head_width=0.1)\n    #plt.text(xvector[i]*max(xs)*1.1, yvector[i]*max(ys)*1.1,\n    #         USArrests.columns[i], color='r', size=14)\n    ax2.arrow(0, 0, xvector[i],  yvector[i], color='red', width=0.005, head_width=0.02)\n    ax2.annotate(USArrests.columns[i], (xvector[i]*1.05, yvector[i]*1.05), color='red', size=14)  \n\n    \nax2.set_xlabel(\"1'st Principal Component (Loading)\", color='red')\nax2.set_ylabel(\"2'nd Principal Component (Loading)\", color='red')\n\nText(0, 0.5, \"2'nd Principal Component (Loading)\")\n\n\n\n\n\nThe PCA function also outputs the variance of each principal component. For instance, on the USArrests data set, we can access these standard deviations as follows:\n\n# varianc explained by each pc's\npca.explained_variance_\n\narray([2.53085875, 1.00996444, 0.36383998, 0.17696948])\n\n\nTo get the proportion of variance explained by each principal component, we simply use the following attribute:\n\npca.explained_variance_/np.var(X_reduced, ddof=1, axis=0).sum()\n\narray([0.62006039, 0.24744129, 0.0891408 , 0.04335752])\n\n\n\n# variance explained ratio's \npca.explained_variance_ratio_\n\narray([0.62006039, 0.24744129, 0.0891408 , 0.04335752])\n\n\nSo, the first pc explaines 2.53 units of variance, which is around 62% of the total variance, the second pc expains 24.7 % of the variance in the data. We can plot the PVE explained by each component, as well as the cumulative PVE, as follows:\n\n# Percentage of Variance Explained (PVE) by each PC:\nplt.figure(figsize=(10, 8))\nplt.ylim(0, 1.05)\nindex = np.arange(pca.components_.shape[0])\nplt.bar(index, height=pca.explained_variance_ratio_ , width=0.3, align='center', \n        label='percentage of variance explained')\nplt.plot(np.cumsum(pca.explained_variance_ratio_), 'bo-', label='cumulative PVE')\n\nplt.ylabel('percentage of variance explained')\nplt.xlabel('Principal components')\nplt.legend(loc='best')\nplt.tight_layout()\n\n\n\n\nYou can refer to https://scikit-learn.org/stable/modules/decomposition.html#pca for more details\n\n\n10.1.2 Matrix Completion\nWe now re-create the analysis carried out on the USArrests data in Section 12.3. We turn the data frame into a matrix, after centering and scaling each column to have mean zero and variance one.\nWe saw in Section 12.2.2 that solving the optimization problem~(12.6) on a centered data matrix \\(\\bf X\\) is equivalent to computing the first \\(M\\) principal components of the data. The (SVD) is a general algorithm for solving (12.6).\n\nu, d, v = linalg.svd(X_scaled, full_matrices=False)\n\nThe svd() function returns three components, u, d, and v. The matrix v is equivalent to the loading matrix from principal components (up to an unimportant sign flip).\n\nv.T\n\narray([[-0.53589947,  0.41818087, -0.34123273,  0.6492278 ],\n       [-0.58318363,  0.1879856 , -0.26814843, -0.74340748],\n       [-0.27819087, -0.87280619, -0.37801579,  0.13387773],\n       [-0.54343209, -0.16731864,  0.81777791,  0.08902432]])\n\n\nThe matrix u is equivalent to the matrix of standardized scores, and the standard deviations are in the vector d. We can recover the score vectors using the output of svd(). They are identical to the score vectors output by PCA().\n\nm, n = X_scaled.shape\nu[:,:n].dot(np.diag(d))\nX_reduced\n\narray([[ 9.85565885e-01,  1.13339238e+00, -4.44268788e-01,\n         1.56267145e-01],\n       [ 1.95013775e+00,  1.07321326e+00,  2.04000333e+00,\n        -4.38583440e-01],\n       [ 1.76316354e+00, -7.45956781e-01,  5.47808243e-02,\n        -8.34652924e-01],\n       [-1.41420290e-01,  1.11979678e+00,  1.14573692e-01,\n        -1.82810896e-01],\n       [ 2.52398013e+00, -1.54293399e+00,  5.98556799e-01,\n        -3.41996478e-01],\n       [ 1.51456286e+00, -9.87555085e-01,  1.09500699e+00,\n         1.46488703e-03],\n       [-1.35864746e+00, -1.08892789e+00, -6.43257569e-01,\n        -1.18469414e-01],\n       [ 4.77093091e-02, -3.25358925e-01, -7.18632942e-01,\n        -8.81977637e-01],\n       [ 3.01304227e+00,  3.92285133e-02, -5.76829492e-01,\n        -9.62847520e-02],\n       [ 1.63928304e+00,  1.27894240e+00, -3.42460080e-01,\n         1.07679681e+00],\n       [-9.12657146e-01, -1.57046001e+00,  5.07818939e-02,\n         9.02806864e-01],\n       [-1.63979985e+00,  2.10972917e-01,  2.59801342e-01,\n        -4.99104101e-01],\n       [ 1.37891072e+00, -6.81841189e-01, -6.77495641e-01,\n        -1.22021292e-01],\n       [-5.05461361e-01, -1.51562542e-01,  2.28054838e-01,\n         4.24665700e-01],\n       [-2.25364607e+00, -1.04054073e-01,  1.64564315e-01,\n         1.75559157e-02],\n       [-7.96881121e-01, -2.70164705e-01,  2.55533072e-02,\n         2.06496428e-01],\n       [-7.50859074e-01,  9.58440286e-01, -2.83694170e-02,\n         6.70556671e-01],\n       [ 1.56481798e+00,  8.71054655e-01, -7.83480358e-01,\n         4.54728038e-01],\n       [-2.39682949e+00,  3.76391576e-01, -6.56823864e-02,\n        -3.30459817e-01],\n       [ 1.76336939e+00,  4.27655192e-01, -1.57250127e-01,\n        -5.59069521e-01],\n       [-4.86166287e-01, -1.47449650e+00, -6.09497476e-01,\n        -1.79598963e-01],\n       [ 2.10844115e+00, -1.55396819e-01,  3.84868584e-01,\n         1.02372019e-01],\n       [-1.69268181e+00, -6.32261251e-01,  1.53070434e-01,\n         6.73168850e-02],\n       [ 9.96494459e-01,  2.39379599e+00, -7.40808395e-01,\n         2.15508013e-01],\n       [ 6.96787329e-01, -2.63354790e-01,  3.77443827e-01,\n         2.25824461e-01],\n       [-1.18545191e+00,  5.36874372e-01,  2.46889321e-01,\n         1.23742227e-01],\n       [-1.26563654e+00, -1.93953730e-01,  1.75573906e-01,\n         1.58928878e-02],\n       [ 2.87439454e+00, -7.75600196e-01,  1.16338049e+00,\n         3.14515476e-01],\n       [-2.38391541e+00, -1.80822897e-02,  3.68553932e-02,\n        -3.31373376e-02],\n       [ 1.81566110e-01, -1.44950571e+00, -7.64453551e-01,\n         2.43382700e-01],\n       [ 1.98002375e+00,  1.42848780e-01,  1.83692180e-01,\n        -3.39533597e-01],\n       [ 1.68257738e+00, -8.23184142e-01, -6.43075093e-01,\n        -1.34843689e-02],\n       [ 1.12337861e+00,  2.22800338e+00, -8.63571788e-01,\n        -9.54381667e-01],\n       [-2.99222562e+00,  5.99118824e-01,  3.01277285e-01,\n        -2.53987327e-01],\n       [-2.25965422e-01, -7.42238237e-01, -3.11391215e-02,\n         4.73915911e-01],\n       [-3.11782855e-01, -2.87854206e-01, -1.53097922e-02,\n         1.03323208e-02],\n       [ 5.91220768e-02, -5.41411454e-01,  9.39832977e-01,\n        -2.37780688e-01],\n       [-8.88415824e-01, -5.71100352e-01, -4.00628706e-01,\n         3.59061124e-01],\n       [-8.63772064e-01, -1.49197842e+00, -1.36994570e+00,\n        -6.13569430e-01],\n       [ 1.32072380e+00,  1.93340466e+00, -3.00537790e-01,\n        -1.31466685e-01],\n       [-1.98777484e+00,  8.23343241e-01,  3.89293329e-01,\n        -1.09571764e-01],\n       [ 9.99741684e-01,  8.60251305e-01,  1.88082949e-01,\n         6.52864291e-01],\n       [ 1.35513821e+00, -4.12480819e-01, -4.92068858e-01,\n         6.43195491e-01],\n       [-5.50565262e-01, -1.47150461e+00,  2.93728037e-01,\n        -8.23140470e-02],\n       [-2.80141174e+00,  1.40228806e+00,  8.41263094e-01,\n        -1.44889914e-01],\n       [-9.63349112e-02,  1.99735289e-01,  1.17125418e-02,\n         2.11370813e-01],\n       [-2.16903379e-01, -9.70124183e-01,  6.24870938e-01,\n        -2.20847793e-01],\n       [-2.10858541e+00,  1.42484670e+00,  1.04774671e-01,\n         1.31908831e-01],\n       [-2.07971417e+00, -6.11268624e-01, -1.38864998e-01,\n         1.84103743e-01],\n       [-6.29426664e-01,  3.21012967e-01, -2.40659234e-01,\n        -1.66651801e-01]])\n\n\nWe now omit 20 entries in the \\(50\\times 4\\) data matrix at random. We do so by first selecting 20 rows (states) at random, and then selecting one of the four entries in each row at random. This ensures that every row has at least three observed values.\n\nnomit = 20\nnp.random.seed(2)\nina = resample(range(50), n_samples = nomit, replace=False)\ninb = resample(range(4), n_samples = nomit, replace=True)\n\n\nina\n\n[36, 47, 28, 9, 13, 0, 44, 46, 39, 23, 24, 48, 17, 12, 27, 33, 16, 2, 25, 14]\n\n\n\nXna = X_scaled.copy()\nXna[ina, inb] = np.nan\n\nHere, ina contains 20 integers from 1 to 50; this represents the states that are selected to contain missing values. And inb contains 20 integers from 1 to 4, representing the features that contain the missing values for each of the selected states.\nWe now write some code to implement Algorithm 12.1. We first write a function that takes in a matrix, and returns an approximation to the matrix using the svd() function. This will be needed in Step 2 of Algorithm 12.1.\n\ndef SVD_approx(X, M=1):\n    u, d, v = linalg.svd(X, full_matrices=False)\n    #m, n = X.shape\n    return u[:,:M].dot(np.diag(d[:M])).dot(v[:M,:])\n\nTo conduct Step 1 of the algorithm, we initialize Xhat — this is \\(\\tilde{\\bf X}\\) in Algorithm 12.1 — by replacing the missing values with the column means of the non-missing entries.\n\nXhat = Xna.copy()\nxbar = np.nanmean(Xna, axis=0)\nxbar\n\narray([0.07184528, 0.02066806, 0.00883971, 0.01192302])\n\n\n\nXhat[ina, inb] = xbar[inb]\n\nBefore we begin Step 2, we set ourselves up to measure the progress of our iterations:\n\nthresh = 1e-7\nrel_err = 1\nitera = 0\nismiss = np.isnan(Xna)\nXX = Xna-xbar\nmssold = np.mean((XX[~ismiss])**2)\nmss0 = np.mean(Xna[~ismiss]**2)\n\n\nmssold, mss0\n\n(1.0160715803147764, 1.0175207163820559)\n\n\nHere ismiss is a new logical matrix with the same dimensions as Xna; a given element equals True if the corresponding matrix element is missing. This is useful because it allows us to access both the missing and non-missing entries. We store the mean of the squared non-missing elements in mss0. We store the mean squared error of the non-missing elements of the old version of Xhat in mssold. We plan to store the mean squared error of the non-missing elements of the current version of Xhat in mss, and will then iterate Step 2 of Algorithm 12.1 until the relative error, defined as (mssold - mss) / mss0, falls below thresh = 1e-7. ( Algorithm 12.1 tells us to iterate Step 2 until (12.14) is no longer decreasing. Determining whether (12.14) is decreasing requires us only to keep track of mssold - mss. However, in practice, we keep track of (mssold - mss) / mss0 instead: this makes it so that the number of iterations required for Algorithm 12.1 to converge does not depend on whether we multiplied the raw data \\(\\bf X\\) by a constant factor. )\nIn Step 2(a) of Algorithm 12.1, we approximate Xhat using SVD_approx; we call this Xapp. In Step 2(b), we use Xapp to update the estimates for elements in Xhat that are missing in Xna. Finally, in Step 2(c), we compute the relative error. These three steps are contained in this while() loop:\n\nwhile rel_err &gt; thresh:\n    itera = itera + 1\n    # Step 2(a)\n    Xapp  = SVD_approx(Xhat, M=1)\n    # Step 2(b)\n    Xhat[ismiss] = Xapp[ismiss]\n    # Step 2(c)\n    XX = Xna-Xapp\n    mss = np.mean((XX[~ismiss])**2)\n    rel_err = (mssold - mss) / mss0\n    mssold = mss\n    print(\"Iter: %d \"%itera)\n    print(\"MSS: %f \"%mss)\n    print(\"Rel. Err:%f \\n\"%rel_err)\n\nIter: 1 \nMSS: 0.414600 \nRel. Err:0.591115 \n\nIter: 2 \nMSS: 0.398214 \nRel. Err:0.016104 \n\nIter: 3 \nMSS: 0.396375 \nRel. Err:0.001807 \n\nIter: 4 \nMSS: 0.396105 \nRel. Err:0.000265 \n\nIter: 5 \nMSS: 0.396051 \nRel. Err:0.000054 \n\nIter: 6 \nMSS: 0.396037 \nRel. Err:0.000014 \n\nIter: 7 \nMSS: 0.396033 \nRel. Err:0.000004 \n\nIter: 8 \nMSS: 0.396032 \nRel. Err:0.000001 \n\nIter: 9 \nMSS: 0.396032 \nRel. Err:0.000000 \n\nIter: 10 \nMSS: 0.396031 \nRel. Err:0.000000 \n\n\n\nWe see that after eight iterations, the relative error has fallen below thresh = 1e-7, and so the algorithm terminates. When this happens, the mean squared error of the non-missing elements equals \\(0.396\\).\nFinally, we compute the correlation between the 20 imputed values and the actual values:\n\nnp.corrcoef(Xhat[ismiss], X_scaled[ismiss])\n\narray([[1.        , 0.80681102],\n       [0.80681102, 1.        ]])\n\n\nIn this lab, we implemented Algorithm 12.1 ourselves for didactic purposes. In practice, if you wishes to apply matrix completion to your data, you can use the https://github.com/iskandr/fancyimpute package, which provides a very eﬃcient implementation of a generalization of this algorithm.\n\nXhat2 = Xna.copy()\nX_filled = SoftImpute(max_rank=1).fit_transform(Xhat2)\n\n[SoftImpute] Max Singular Value of X_init = 10.088074\n[SoftImpute] Iter 1: observed MAE=0.477063 rank=1\n[SoftImpute] Iter 2: observed MAE=0.463440 rank=1\n[SoftImpute] Iter 3: observed MAE=0.460874 rank=1\n[SoftImpute] Iter 4: observed MAE=0.460157 rank=1\n[SoftImpute] Iter 5: observed MAE=0.459960 rank=1\n[SoftImpute] Iter 6: observed MAE=0.459916 rank=1\n[SoftImpute] Iter 7: observed MAE=0.459900 rank=1\n[SoftImpute] Iter 8: observed MAE=0.459894 rank=1\n[SoftImpute] Iter 9: observed MAE=0.459891 rank=1\n[SoftImpute] Stopped after iteration 9 for lambda=0.201761\n\n\n\nnp.corrcoef(X_filled[ismiss], X_scaled[ismiss])\n\narray([[1.        , 0.80736346],\n       [0.80736346, 1.        ]])\n\n\n\n\n10.1.3 t-SNE\n\ndigits = load_digits()\n\n\nX = digits.images.reshape(-1, digits.images.shape[1]*digits.images.shape[2]) #vectorize\nX.shape\n\n(1797, 64)\n\n\n\nfig, ax_array = plt.subplots(5, 5)\naxes = ax_array.flatten()\nfor i, ax in enumerate(axes):\n    ax.imshow(digits.images[i], cmap='gray_r')\nplt.setp(axes, xticks=[], yticks=[], frame_on=False)\nplt.tight_layout(h_pad=0.5, w_pad=0.01)\n\n\n\n\n\ntsne = TSNE(\n    n_components=2, perplexity=30, init=\"pca\", learning_rate=\"auto\", random_state=0, verbose=True\n)\n\n\nembedding = tsne.fit_transform(X)\n\n[t-SNE] Computing 91 nearest neighbors...\n[t-SNE] Indexed 1797 samples in 0.001s...\n[t-SNE] Computed neighbors for 1797 samples in 0.230s...\n[t-SNE] Computed conditional probabilities for sample 1000 / 1797\n[t-SNE] Computed conditional probabilities for sample 1797 / 1797\n[t-SNE] Mean sigma: 11.585657\n\n\n/usr/local/lib/python3.7/dist-packages/sklearn/manifold/_t_sne.py:986: FutureWarning: The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence.\n  FutureWarning,\n\n\n[t-SNE] KL divergence after 250 iterations with early exaggeration: 54.981926\n[t-SNE] KL divergence after 1000 iterations: 0.780030\n\n\n\nplt.figure(figsize=(10, 8))\nplt.scatter(embedding[:, 0], embedding[:, 1], c=digits.target, cmap='Spectral', s=5)\nplt.gca().set_aspect('equal', 'datalim')\nplt.colorbar(boundaries=np.arange(11)-0.5).set_ticks(np.arange(10))\nplt.title('tSNE of the Digits dataset', fontsize=24)\n\nText(0.5, 1.0, 'tSNE of the Digits dataset')\n\n\n\n\n\nopenTSNE provides fast implementation and more optimization related to t-SNE, see https://opentsne.readthedocs.io/en/latest/ for more details.\n\notsne = oTSNE(\n    n_components=2,\n    perplexity=30,\n    initialization='pca', \n    n_jobs=2,\n    random_state=0,\n    negative_gradient_method='auto', \n    verbose=True,\n)\n\n\nembedding = otsne.fit(X)\n\n--------------------------------------------------------------------------------\nTSNE(n_jobs=2, random_state=0, verbose=True)\n--------------------------------------------------------------------------------\n===&gt; Finding 90 nearest neighbors using Annoy approximate search using euclidean distance...\n   --&gt; Time elapsed: 1.32 seconds\n===&gt; Calculating affinity matrix...\n   --&gt; Time elapsed: 0.29 seconds\n===&gt; Calculating PCA-based initialization...\n   --&gt; Time elapsed: 0.02 seconds\n===&gt; Running optimization with exaggeration=12.00, lr=200.00 for 250 iterations...\nIteration   50, KL divergence 3.1044, 50 iterations in 0.7805 sec\nIteration  100, KL divergence 2.7275, 50 iterations in 0.4949 sec\nIteration  150, KL divergence 2.6611, 50 iterations in 0.6111 sec\nIteration  200, KL divergence 2.6358, 50 iterations in 0.5653 sec\nIteration  250, KL divergence 2.6226, 50 iterations in 0.7305 sec\n   --&gt; Time elapsed: 3.19 seconds\n===&gt; Running optimization with exaggeration=1.00, lr=200.00 for 500 iterations...\nIteration   50, KL divergence 1.2822, 50 iterations in 0.7395 sec\nIteration  100, KL divergence 1.0153, 50 iterations in 1.1167 sec\nIteration  150, KL divergence 0.9093, 50 iterations in 1.7734 sec\nIteration  200, KL divergence 0.8536, 50 iterations in 0.8590 sec\nIteration  250, KL divergence 0.8199, 50 iterations in 1.3595 sec\nIteration  300, KL divergence 0.7989, 50 iterations in 1.3390 sec\nIteration  350, KL divergence 0.7856, 50 iterations in 0.8678 sec\nIteration  400, KL divergence 0.7763, 50 iterations in 0.4503 sec\nIteration  450, KL divergence 0.7706, 50 iterations in 0.6760 sec\nIteration  500, KL divergence 0.7670, 50 iterations in 1.7542 sec\n   --&gt; Time elapsed: 10.96 seconds\n\n\n\nplt.figure(figsize=(10, 8))\nplt.scatter(embedding[:, 0], embedding[:, 1], c=digits.target, cmap='Spectral', s=5)\nplt.gca().set_aspect('equal', 'datalim')\nplt.colorbar(boundaries=np.arange(11)-0.5).set_ticks(np.arange(10))\nplt.title('tSNE of the Digits dataset', fontsize=24)\n\nText(0.5, 1.0, 'tSNE of the Digits dataset')\n\n\n\n\n\n\n\n10.1.4 UMAP\nUMAP is useful for generating visualisations, but if you want to make use of UMAP more generally for machine learning tasks it is important to be be able to train a model and then later pass new data to the model and have it transform that data into the learned space. For example if we use UMAP to learn a latent space and then train a classifier on data transformed into the latent space then the classifier is only useful for prediction if we can transform data for which we want a prediction into the latent space the classifier uses.\n\nX_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target, stratify=digits.target, random_state=42)\n\nNow to get a benchmark idea of what we are looking at let’s train a couple of different classifiers and then see how well they score on the test set. For this example let’s try a support vector classifier and a KNN classifier.\n\nsvc = SVC(gamma='auto').fit(X_train, y_train)\nknn = KNeighborsClassifier().fit(X_train, y_train)\nsvc.score(X_test, y_test), knn.score(X_test, y_test)\n\n(0.62, 0.9844444444444445)\n\n\nThe goal now is to make use of UMAP as a preprocessing step that one could potentially fit into a pipeline.\n\ntrans = umap.UMAP(n_neighbors=5, random_state=42).fit(X_train)\n\n\nplt.figure(figsize=(10, 8))\nplt.scatter(trans.embedding_[:, 0], trans.embedding_[:, 1], c=y_train, cmap='Spectral', s=5)\nplt.gca().set_aspect('equal', 'datalim')\nplt.colorbar(boundaries=np.arange(11)-0.5).set_ticks(np.arange(10))\nplt.title('Umap of the Digits dataset', fontsize=24)\n\nText(0.5, 1.0, 'Umap of the Digits dataset')\n\n\n\n\n\nThis looks very promising! Most of the classes got very cleanly separated, and that gives us some hope that it could help with classifier performance. We can now train some new models (again an SVC and a KNN classifier) on the embedded training data. This looks exactly as before but now we pass it the embedded data.\n\nsvc = SVC(gamma='auto').fit(trans.embedding_, y_train)\nknn = KNeighborsClassifier().fit(trans.embedding_, y_train)\n\n\ntest_embedding = trans.transform(X_test)\n\nThe next important question is what the transform did to our test data. In principle we have a new two dimensional representation of the test-set, and ideally this should be based on the existing embedding of the training set\n\nplt.figure(figsize=(10, 8))\nplt.scatter(test_embedding[:, 0], test_embedding[:, 1], c=y_test, cmap='Spectral', s=5)\nplt.gca().set_aspect('equal', 'datalim')\nplt.colorbar(boundaries=np.arange(11)-0.5).set_ticks(np.arange(10))\nplt.title('Umap of the Digits dataset', fontsize=24)\n\nText(0.5, 1.0, 'Umap of the Digits dataset')\n\n\n\n\n\nThe results look like what we should expect; the test data has been embedded into two dimensions in exactly the locations we should expect (by class) given the embedding of the training data visualised above. This means we can now try out models that were trained on the embedded training data by handing them the newly transformed test set.\n\nsvc.score(trans.transform(X_test), y_test), knn.score(trans.transform(X_test), y_test)\n\n(0.9822222222222222, 0.9822222222222222)\n\n\nThe results are pretty good. While the accuracy of the KNN classifier did not improve there was not a lot of scope for improvement given the data. On the other hand the SVC has improved to have equal accuracy to the KNN classifier!\nFor more interesting datasets the larger dimensional embedding might have been a significant gain – it is certainly worth exploring as one of the parameters in a grid search across a pipeline that includes UMAP."
  },
  {
    "objectID": "Chapter_12_Lab.html#clustering",
    "href": "Chapter_12_Lab.html#clustering",
    "title": "10  Unsupervised Learning",
    "section": "10.2 Clustering",
    "text": "10.2 Clustering\n\n10.2.1 \\(K\\)-Means Clustering\nYou can refer to https://scikit-learn.org/stable/modules/clustering.html for more details\nThe function kmeans() performs \\(K\\)-means clustering in Python. We begin with a simple simulated example in which there truly are two clusters in the data: the first 25 observations have a mean shift relative to the next 25 observations.\n\n# set seed \nnp.random.seed(2)\nX = np.random.normal(size = (50,2))\nX[:25,0] += 3 \nX[:25,1] -= 4\n\n\nplt.scatter(X[:,0],X[:,1])\n\n&lt;matplotlib.collections.PathCollection at 0x7fdb94e66f50&gt;\n\n\n\n\n\nWe now perform \\(K\\)-means clustering with \\(K=2\\).\n\nkmeans = KMeans(n_clusters = 2, n_init=20, random_state=1)\nkmeans.fit(X)\n\nKMeans(n_clusters=2, n_init=20, random_state=1)\n\n\nThe cluster assignments of the 50 observations are listed as below:\n\nkmeans.labels_\n\narray([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 0], dtype=int32)\n\n\nThe \\(K\\)-means clustering nearly perfectly separated the observations into two clusters even though we did not supply any group information to kmeans(). We can plot the data, with each observation colored according to its cluster assignment.\n\nplt.scatter(X[:, 0], X[:, 1], c=kmeans.labels_, s=50, cmap=plt.cm.bwr)\n\ncenters = kmeans.cluster_centers_\nplt.scatter(centers[:, 0], centers[:, 1],marker = '*', c='black', s=200, alpha=1)\nplt.xlabel('X0')\nplt.ylabel('X1')\n\nText(0, 0.5, 'X1')\n\n\n\n\n\n\nnp.bincount(kmeans.labels_)\n# 26 obs are classified in cluster 1, and 24 in cluster 2\n\narray([26, 24])\n\n\nHere the observations can be easily plotted because they are two-dimensional. If there were more than two variables then we could instead perform PCA and plot the first two principal components score vectors.\nIn this example, we knew that there really were two clusters because we generated the data. However, for real data, in general we do not know the true number of clusters. We could instead have performed \\(K\\)-means clustering on this example with \\(K=3\\).\n\nkmean2 = KMeans(n_clusters=3, n_init=20, random_state=1)\nkmean2.fit(X)\n\nplt.figure(figsize=(6,5))\nplt.scatter(X[:,0], X[:,1], s=50, c=kmean2.labels_, cmap=plt.cm.prism) \nplt.scatter(kmean2.cluster_centers_[:, 0], kmean2.cluster_centers_[:, 1], marker='*', s=150,\n            color='blue', label='Centers')\nplt.legend(loc='best')\nplt.xlabel('X0')\nplt.ylabel('X1')\n\nText(0, 0.5, 'X1')\n\n\n\n\n\nWhen \\(K=3\\), \\(K\\)-means clustering splits up the two clusters.\nTo run the kmeans() function in Python with multiple initial cluster assignments, we use the n_init argument. If a value of n_init greater than one is used, then \\(K\\)-means clustering will be performed using multiple random assignments in Step~1 of Algorithm 12.2, and the kmeans() function will report only the best results. Here we compare using n_init = 1 to n_init = 20.\n\nkmeans3 = KMeans(n_clusters=3, n_init=1, random_state=0)\nkmeans3.fit(X)\nkmeans4 = KMeans(n_clusters=3,  n_init=20, random_state=0)\nkmeans4.fit(X)\n\nprint('inertia with n_init=1:', kmeans3.inertia_)\nprint('inertia with n_init=20:', kmeans4.inertia_)\n\ninertia with n_init=1: 69.42547524486459\ninertia with n_init=20: 68.97379200939723\n\n\nNote that kmeans.inertia_ is the sum of squared distances of samples to their closest cluster center which we seek to minimize by performing \\(K\\)-means clustering. We strongly recommend always running \\(K\\)-means clustering with a large value of n_init, such as 20 or 50, since otherwise an undesirable local optimum may be obtained.\nWhen performing \\(K\\)-means clustering, in addition to using multiple initial cluster assignments, it is also important to set a random seed using the random_state. This way, the initial cluster assignments in Step~1 can be replicated, and the \\(K\\)-means output will be fully reproducible.\n\n\n10.2.2 Hierarchical Clustering\nThe AgglomerativeClustering() function implements hierarchical clustering in Python. In the following example we use the data from the previous lab to plot the hierarchical clustering dendrogram using complete, single, and average linkage clustering, with Euclidean distance as the dissimilarity measure. We begin by clustering observations using complete linkage.\n\nhclust = AgglomerativeClustering(n_clusters=2, linkage='complete', compute_distances=True)    \nhclust.fit(X)\nhclust.labels_\n\narray([1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n       1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0])\n\n\nWe could just as easily perform hierarchical clustering with average or single linkage instead:\n\nhclust = AgglomerativeClustering(n_clusters=2, linkage='average', compute_distances=True)    \nhclust.fit(X)\nhclust.labels_\n\narray([1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,\n       1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0])\n\n\n\nhclust = AgglomerativeClustering(n_clusters=2, linkage='single', compute_distances=True)    \nhclust.fit(X)\nhclust.labels_\n\narray([1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n       1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0])\n\n\nWe can now plot the dendrograms obtained using the custom function. The numbers at the bottom of the plot identify each observation.\n\nplt.title(\"Hierarchical Clustering Dendrogram\")\n# plot the top three levels of the dendrogram\nplot_dendrogram(hclust, truncate_mode=\"level\", p=3)\nplt.xlabel(\"Number of points in node (or index of point if no parenthesis).\")\n\nText(0.5, 0, 'Number of points in node (or index of point if no parenthesis).')\n\n\n\n\n\nYou can also perform clustering using Scipy, see https://docs.scipy.org/doc/scipy/reference/cluster.hierarchy.html for more details.\n\nhc_complete = linkage(X, method='complete')\nhc_average = linkage(X, method='average')\nhc_single = linkage(X, method='single')\n\n\nf, axes = plt.subplots(1, 3, sharex=False, sharey=False)\n\nf.set_figheight(8)\nf.set_figwidth(16)\n\ndendrogram(hc_complete,\n           labels=X,\n           leaf_rotation=90,\n           leaf_font_size=6,\n           ax=axes[0])\n\ndendrogram(hc_average,\n           labels=X, # coordinates\n           leaf_rotation=90,\n           leaf_font_size=6,\n           ax=axes[1])\n\ndendrogram(hc_single,\n           labels=X,\n           leaf_rotation=90,\n           leaf_font_size=6,\n           ax=axes[2])\n\naxes[0].set_title('Complete Linkage', size=16)\naxes[1].set_title('Average Linkage', size=16)\naxes[2].set_title('Single Linkage', size=16)\n\n/usr/local/lib/python3.7/dist-packages/matplotlib/text.py:1165: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n  if s != self._text:\n\n\nText(0.5, 1.0, 'Single Linkage')\n\n\n\n\n\nTo determine the cluster labels for each observation associated with a given cut of the dendrogram, we can use the cutree() function: The second argument to cutree() is the number of clusters we wish to obtain. For this data, complete and average linkage generally separate the observations into their correct groups. However, single linkage does not.\n\ncut_tree(hc_complete, n_clusters=2, height=None).ravel()\n\narray([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n       0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1])\n\n\n\ncut_tree(hc_average, 2).ravel()\n\narray([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,\n       0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1])\n\n\n\ncut_tree(hc_single, 2).ravel()\n\narray([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n       0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1])\n\n\n\ncut_tree(hc_single, 3).ravel()\n\narray([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n       0, 0, 0, 2, 2, 2, 2, 1, 2, 2, 2, 1, 2, 2, 2, 2, 2, 1, 2, 2, 1, 2,\n       2, 2, 2, 2, 2, 1])\n\n\nTo scale the variables before performing hierarchical clustering of the observations:\n\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\ndendrogram(linkage(X_scaled, method='complete'),\n           labels=X_scaled,\n           leaf_rotation=90,\n           leaf_font_size=6)\n\nplt.title('Hierarchical Clustering with Scaled Features', size=16)\n\n/usr/local/lib/python3.7/dist-packages/matplotlib/text.py:1165: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n  if s != self._text:\n\n\nText(0.5, 1.0, 'Hierarchical Clustering with Scaled Features')\n\n\n\n\n\nCorrelation-based distance can be computed using the pdist() function, which converts an arbitrary square symmetric matrix into a form that the scipy recognizes as a distance matrix. However, this only makes sense for data with at least three features since the absolute correlation between any two observations with measurements on two features is always 1. Hence, we will cluster a three-dimensional data set. This data set does not contain any true clusters.\n\nX = np.random.normal(size=30*3).reshape(30, 3)\n\n# scipy linkage takes care of the distance function pdist\ndendrogram(linkage(X, method='complete', metric='correlation'),\n           labels=X,\n           leaf_rotation=90,\n           leaf_font_size=6)\n\nplt.title('Complete Linkage with Correlation Based Distance', size=16);\n\n\n\n\nYou can get a more efficient implementation at http://danifold.net/fastcluster.html. The usuage is the same as Scipy.\n\nlk = fc.linkage(X, method='complete', metric='correlation', preserve_input=True)\ndendrogram(lk,\n           labels=X,\n           leaf_rotation=90,\n           leaf_font_size=6)\n\nplt.title('Complete Linkage with Correlation Based Distance', size=16);\n\n\n\n\n\n\n10.2.3 DBSCAN\n\n# Generate sample data\ncenters = [[1, 1], [-1, -1], [1, -1]]\nX, labels_true = make_blobs(\n    n_samples=750, centers=centers, cluster_std=0.4, random_state=0\n)\n\nX = StandardScaler().fit_transform(X)\n\nFor more clustering performance measure see https://scikit-learn.org/stable/modules/clustering.html#clustering-performance-evaluation.\n\n# Compute DBSCAN\ndb = DBSCAN(eps=0.3, min_samples=10).fit(X)\ncore_samples_mask = np.zeros_like(db.labels_, dtype=bool)\ncore_samples_mask[db.core_sample_indices_] = True\nlabels = db.labels_\n\n# Number of clusters in labels, ignoring noise if present.\nn_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\nn_noise_ = list(labels).count(-1)\n\nprint(\"Estimated number of clusters: %d\" % n_clusters_)\nprint(\"Estimated number of noise points: %d\" % n_noise_)\nprint(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels_true, labels))\nprint(\"Completeness: %0.3f\" % metrics.completeness_score(labels_true, labels))\nprint(\"V-measure: %0.3f\" % metrics.v_measure_score(labels_true, labels))\nprint(\"Adjusted Rand Index: %0.3f\" % metrics.adjusted_rand_score(labels_true, labels))\nprint(\n    \"Adjusted Mutual Information: %0.3f\"\n    % metrics.adjusted_mutual_info_score(labels_true, labels)\n)\n\nEstimated number of clusters: 3\nEstimated number of noise points: 18\nHomogeneity: 0.953\nCompleteness: 0.883\nV-measure: 0.917\nAdjusted Rand Index: 0.952\nAdjusted Mutual Information: 0.916\n\n\n\n# Black removed and is used for noise instead.\nunique_labels = set(labels)\ncolors = [plt.cm.Spectral(each) for each in np.linspace(0, 1, len(unique_labels))]\nfor k, col in zip(unique_labels, colors):\n    if k == -1:\n        # Black used for noise.\n        col = [0, 0, 0, 1]\n\n    class_member_mask = labels == k\n\n    xy = X[class_member_mask & core_samples_mask]\n    plt.plot(\n        xy[:, 0],\n        xy[:, 1],\n        \"o\",\n        markerfacecolor=tuple(col),\n        markeredgecolor=\"k\",\n        markersize=14,\n    )\n\n    xy = X[class_member_mask & ~core_samples_mask]\n    plt.plot(\n        xy[:, 0],\n        xy[:, 1],\n        \"o\",\n        markerfacecolor=tuple(col),\n        markeredgecolor=\"k\",\n        markersize=6,\n    )\n\nplt.title(\"Estimated number of clusters: %d\" % n_clusters_)\n\nText(0.5, 1.0, 'Estimated number of clusters: 3')\n\n\n\n\n\nThe DBSCAN, however, is quite sensitive to the hypeparameter\n\n# Generate sample data\ncenters = [[0, 0], [5, 5]]\nX, labels_true = make_blobs(\n    n_samples=500, centers=centers, cluster_std=[2.0,0.5], random_state=0\n)\n\n\nplt.scatter(X[:,0],X[:,1])\n\n&lt;matplotlib.collections.PathCollection at 0x7fdb95825810&gt;\n\n\n\n\n\n\nepss = [2,2,3,3]\nmin_sampless = [50,100,50,100]\n\n\nf, axes = plt.subplots(1, 4, sharex=False, sharey=False, figsize=(20,5))\n\nfor i in range(4):\n  db = DBSCAN(eps=epss[i], min_samples=min_sampless[i]).fit(X)\n  y_pred = db.labels_.astype(int)\n  colors = np.array(\n            list(\n                islice(\n                    cycle(\n                        [\n                            \"#377eb8\",\n                            \"#ff7f00\",\n                            \"#4daf4a\",\n                            \"#f781bf\",\n                            \"#a65628\",\n                            \"#984ea3\",\n                            \"#999999\",\n                            \"#e41a1c\",\n                            \"#dede00\",\n                        ]\n                    ),\n                    int(max(y_pred) + 1),\n                )\n            )\n        )\n  # add black color for outliers (if any)\n  colors = np.append(colors, [\"#000000\"])\n  axes[i].scatter(X[:, 0], X[:, 1], s=10, color=colors[y_pred])\n\n  axes[i].set_title(\"eps=%.1f, min_samples=%d\" % (epss[i],min_sampless[i]))\n\n\n\n\nYou can try to use HDBSCAN instead. For more information, please refer to https://hdbscan.readthedocs.io/en/latest/index.html.\n\nmin_cluster_sizes = [5,10,30,50]\n\n\nf, axes = plt.subplots(1, 4, sharex=False, sharey=False, figsize=(20,5))\n\nfor i in range(4):\n  db = hdbscan.HDBSCAN(min_cluster_size=min_cluster_sizes[i]).fit(X)\n  y_pred = db.labels_.astype(int)\n  colors = np.array(\n            list(\n                islice(\n                    cycle(\n                        [\n                            \"#377eb8\",\n                            \"#ff7f00\",\n                            \"#4daf4a\",\n                            \"#f781bf\",\n                            \"#a65628\",\n                            \"#984ea3\",\n                            \"#999999\",\n                            \"#e41a1c\",\n                            \"#dede00\",\n                        ]\n                    ),\n                    int(max(y_pred) + 1),\n                )\n            )\n        )\n  # add black color for outliers (if any)\n  colors = np.append(colors, [\"#000000\"])\n  axes[i].scatter(X[:, 0], X[:, 1], s=10, color=colors[y_pred])\n  axes[i].set_title(\"min_cluster_size=%d\" % (min_cluster_sizes[i]))"
  },
  {
    "objectID": "Chapter_12_Lab.html#nci60-data-example",
    "href": "Chapter_12_Lab.html#nci60-data-example",
    "title": "10  Unsupervised Learning",
    "section": "10.3 NCI60 Data Example",
    "text": "10.3 NCI60 Data Example\nUnsupervised techniques are often used in the analysis of genomic data. In particular, PCA and hierarchical clustering are popular tools. We illustrate these techniques on the NCI cancer cell line microarray data, which consists of \\(6{,}830\\) gene expression measurements on \\(64\\) cancer cell lines.\n\nNCI60_labs = pd.read_csv(\"/content/drive/MyDrive/Lab/Data/NCI60_labs.csv\", index_col=0)\nNCI60_data = pd.read_csv(\"/content/drive/MyDrive/Lab/Data/NCI60_data.csv\", index_col=0)\n\nNCI60_data.head()\n\n\n  \n    \n      \n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n...\n6821\n6822\n6823\n6824\n6825\n6826\n6827\n6828\n6829\n6830\n\n\n\n\nV1\n0.300000\n1.180000\n0.550000\n1.140000\n-0.265000\n-7.000000e-02\n0.350000\n-0.315000\n-0.450000\n-0.654980\n...\n-0.990020\n0.000000\n0.030000\n-0.175000\n0.629981\n-0.030000\n0.000000\n0.280000\n-0.340000\n-1.930000\n\n\nV2\n0.679961\n1.289961\n0.169961\n0.379961\n0.464961\n5.799610e-01\n0.699961\n0.724961\n-0.040039\n-0.285019\n...\n-0.270058\n-0.300039\n-0.250039\n-0.535039\n0.109941\n-0.860039\n-1.250049\n-0.770039\n-0.390039\n-2.000039\n\n\nV3\n0.940000\n-0.040000\n-0.170000\n-0.040000\n-0.605000\n0.000000e+00\n0.090000\n0.645000\n0.430000\n0.475019\n...\n0.319981\n0.120000\n-0.740000\n-0.595000\n-0.270020\n-0.150000\n0.000000\n-0.120000\n-0.410000\n0.000000\n\n\nV4\n0.280000\n-0.310000\n0.680000\n-0.810000\n0.625000\n-1.387779e-17\n0.170000\n0.245000\n0.020000\n0.095019\n...\n-1.240020\n-0.110000\n-0.160000\n0.095000\n-0.350019\n-0.300000\n-1.150010\n1.090000\n-0.260000\n-1.100000\n\n\nV5\n0.485000\n-0.465000\n0.395000\n0.905000\n0.200000\n-5.000000e-03\n0.085000\n0.110000\n0.235000\n1.490019\n...\n0.554980\n-0.775000\n-0.515000\n-0.320000\n0.634980\n0.605000\n0.000000\n0.745000\n0.425000\n0.145000\n\n\n\n\n\n5 rows × 6830 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nEach cell line is labeled with a cancer type, given in NCI60_labs. We do not make use of the cancer types in performing PCA and clustering, as these are unsupervised techniques. But after performing PCA and clustering, we will check to see the extent to which these cancer types agree with the results of these unsupervised techniques.\nThe data has \\(64\\) rows and \\(6{,}830\\) columns.\n\nNCI60_labs.value_counts()\n\nx          \nNSCLC          9\nRENAL          9\nMELANOMA       8\nBREAST         7\nCOLON          7\nLEUKEMIA       6\nOVARIAN        6\nCNS            5\nPROSTATE       2\nK562A-repro    1\nK562B-repro    1\nMCF7A-repro    1\nMCF7D-repro    1\nUNKNOWN        1\ndtype: int64\n\n\n\n10.3.1 PCA on the NCI60 Data\nWe first perform PCA on the data after scaling the variables (genes) to have standard deviation one, although one could reasonably argue that it is better not to scale the genes.\n\nscaler = StandardScaler()\nNCI60_scaled = scaler.fit_transform(NCI60_data)\n\npca = PCA()\npca.fit(NCI60_scaled)\n\nPCA()\n\n\nWe now plot the first few principal component score vectors, in order to visualize the data. The observations (cell lines) corresponding to a given cancer type will be plotted in the same color, so that we can see to what extent the observations within a cancer type are similar to each other. We first create a simple function that assigns a distinct color to each element of a numeric vector. The function will be used to assign a color to each of the \\(64\\) cell lines, based on the cancer type to which it corresponds.\n\nX_reduced = pca.transform(NCI60_scaled)\n\n\nfig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(17, 8))\n\nZ1 = X_reduced[:, 0]    \nZ2 = X_reduced[:, 1]   \nZ3 = X_reduced[:, 2]    \n\n# print(list(le.fit_transform(nci_labs.x)))\n# list(enumerate(le.classes_))\nle = LabelEncoder()\nax1.scatter(Z1, Z2, s=70, c=le.fit_transform(NCI60_labs), cmap=plt.cm.gist_rainbow)     \nax1.set_xlabel('Z1', size=18)\nax1.set_ylabel('Z2', size=18)\n\nax2.scatter(Z1, Z3, s=70, c=le.fit_transform(NCI60_labs), cmap=plt.cm.gist_rainbow) \nax2.set_xlabel('Z1', size=18)\nax2.set_ylabel('Z3', size=18)\n\n/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n\n\nText(0, 0.5, 'Z3')\n\n\n\n\n\nThe resulting plots are shown in Figure 12.17. On the whole, cell lines corresponding to a single cancer type do tend to have similar values on the first few principal component score vectors. This indicates that cell lines from the same cancer type tend to have pretty similar gene expression levels.\nWe can obtain a summary of the proportion of variance explained (PVE) of the first few principal components:\n\n# Percentage of Variance Explained by each PC:\nplt.figure(figsize=(14, 7))\nplt.ylim(0, 100)\nplt.xlim(-1, 65)\nindex = np.arange(pca.components_.shape[0])\nplt.bar(index,  \n        height=pca.explained_variance_ratio_ * 100 , width=0.3, align='center', \n        label='percentage of variance explained')\nplt.plot(np.cumsum(pca.explained_variance_ratio_) * 100, 'ro-', label='cumulative PVE')\n\nplt.ylabel('percentage of variance explained', size=18)\nplt.xlabel('Principal components', size=18)\nplt.legend(loc='best', fontsize='xx-large')\nplt.tight_layout()\n\n\n\n\nWe see that together, the first seven principal components explain around \\(40\\,\\%\\) of the variance in the data. This is not a huge amount of the variance. However, looking at the scree plot, we see that while each of the first seven principal components explain a substantial amount of variance, there is a marked decrease in the variance explained by further principal components. That is, there is an elbow in the plot after approximately the seventh principal component. This suggests that there may be little benefit to examining more than seven or so principal components (though even examining seven principal components may be difficult).\n\n\n10.3.2 Clustering the Observations of the NCI60 Data\nWe now proceed to hierarchically cluster the cell lines in the NCI data, with the goal of finding out whether or not the observations cluster into distinct types of cancer. To begin, we standardize the variables to have mean zero and standard deviation one. As mentioned earlier, this step is optional and should be performed only if we want each gene to be on the same scale.\n\nNCI60_labs\n\n\n  \n    \n      \n\n\n\n\n\n\nx\n\n\n\n\n1\nCNS\n\n\n2\nCNS\n\n\n3\nCNS\n\n\n4\nRENAL\n\n\n5\nBREAST\n\n\n...\n...\n\n\n60\nMELANOMA\n\n\n61\nMELANOMA\n\n\n62\nMELANOMA\n\n\n63\nMELANOMA\n\n\n64\nMELANOMA\n\n\n\n\n\n64 rows × 1 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\nf, axes = plt.subplots(3, 1, sharex=False, sharey=False)\n\nf.set_figheight(24)\nf.set_figwidth(16)\n\ndendrogram(linkage(NCI60_scaled, method='complete'),\n           labels=NCI60_labs.values,\n           leaf_rotation=90,\n           leaf_font_size=6,\n           ax=axes[0])\n\ndendrogram(linkage(NCI60_scaled, method='average'),\n           labels=NCI60_labs.values,\n           leaf_rotation=90,\n           leaf_font_size=6,\n           ax=axes[1])\n\ndendrogram(linkage(NCI60_scaled, method='single'),\n           labels=NCI60_labs.values,\n           leaf_rotation=90,\n           leaf_font_size=6,\n           ax=axes[2])\n\naxes[0].set_title('Complete Linkage', size=16)\naxes[1].set_title('Average Linkage', size=16)\naxes[2].set_title('Single Linkage', size=16)\n\nText(0.5, 1.0, 'Single Linkage')\n\n\n\n\n\nWe see that the choice of linkage certainly does affect the results obtained. Typically, single linkage will tend to yield trailing clusters: very large clusters onto which individual observations attach one-by-one. On the other hand, complete and average linkage tend to yield more balanced, attractive clusters. For this reason, complete and average linkage are generally preferred to single linkage. Clearly cell lines within a single cancer type do tend to cluster together, although the clustering is not perfect. We will use complete linkage hierarchical clustering for the analysis that follows.\nWe can cut the dendrogram at the height that will yield a particular number of clusters, say four:\n\ncut_tree(linkage(NCI60_scaled, method='complete'), 4).ravel()\n\narray([0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 0, 3, 0,\n       3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n\n\nWe can plot the cut on the dendrogram that produces these four clusters:\n\nplt.figure(figsize=(16, 10))\n\ndendrogram(linkage(NCI60_scaled, method='complete'),\n           labels=NCI60_labs.values,\n           leaf_rotation=90,\n           leaf_font_size=6)\n\nplt.axhline(y=139, c='r')\nplt.title('Complete Linkage', size=16);\n\n\n\n\nWe claimed earlier in Section 12.4.2 that \\(K\\)-means clustering and hierarchical clustering with the dendrogram cut to obtain the same number of clusters can yield very different results. How do these NCI hierarchical clustering results compare to what we get if we perform \\(K\\)-means clustering with \\(K=4\\)?\n\nkmeans = KMeans(n_clusters=4, n_init=20, random_state=0)\nkmeans.fit(NCI60_scaled)\npd.crosstab(kmeans.labels_, cut_tree(linkage(NCI60_scaled, method='complete'), 4).ravel())\n\n\n  \n    \n      \n\n\n\n\n\ncol_0\n0\n1\n2\n3\n\n\nrow_0\n\n\n\n\n\n\n\n\n0\n28\n7\n0\n0\n\n\n1\n3\n0\n0\n9\n\n\n2\n9\n0\n0\n0\n\n\n3\n0\n0\n8\n0\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nWe see that the four clusters obtained using hierarchical clustering and \\(K\\)-means clustering are somewhat different. Cluster \\(4\\) in \\(K\\)-means clustering is identical to cluster \\(3\\) in hierarchical clustering. However, the other clusters differ\nRather than performing hierarchical clustering on the entire data matrix, we can simply perform hierarchical clustering on the first few principal component score vectors, as follows:\n\nhc2 = linkage(X_reduced[:, 0:5], method='complete')\n\nplt.figure(figsize=(16, 10))\n\ndendrogram(hc2,\n           labels=NCI60_labs.values,\n           leaf_rotation=90,\n           leaf_font_size=6)\n\nplt.title('Hierarchical CLustering on First Five Score Vectors', size=16)\n\nText(0.5, 1.0, 'Hierarchical CLustering on First Five Score Vectors')\n\n\n\n\n\nNot surprisingly, these results are different from the ones that we obtained when we performed hierarchical clustering on the full data set. Sometimes performing clustering on the first few principal component score vectors can give better results than performing clustering on the full data. In this situation, we might view the principal component step as one of denoising the data. We could also perform \\(K\\)-means clustering on the first few principal component score vectors rather than the full data set."
  },
  {
    "objectID": "NumPy_tutorial.html#introduction",
    "href": "NumPy_tutorial.html#introduction",
    "title": "Appendix A — Numpy - multidimensional data arrays for python",
    "section": "A.1 Introduction",
    "text": "A.1 Introduction\nPython objects:\n\nHigh-level objects: integers, floating-point\nContainers: lists (costless append), dictionaries (fast lookup)\nPython lists are very general. They can contain any kind of object and are dynamically typed\nHowever, they do not support mathematical functions such as matrix and dot multiplications. Implementing such functions for Python lists would not be very efficient because of the dynamic typing\n\nNumPy provides:\n\nExtension package to Python for multi-dimensional arrays\nNumpy arrays are statically typed and homogeneous. The type of the elements is determined when the array is created\nBecause of the static typing, fast implementation of mathematical functions such as multiplication and addition of numpy arrays can be implemented in a compiled language (C and Fortran is used). Moreover, Numpy arrays are memory efficient\n\nThe numpy package (module) is used in almost all numerical computation using Python. It is a package that provides high-performance vector, matrix and higher-dimensional data structures for Python. It is implemented in C and Fortran so when calculations are vectorized (formulated with vectors and matrices) which provides good performance\nTo use numpy you need to import the module, using for example:\n\nimport numpy as np\n\n\na = range(1000)\n\n\n%%timeit\na1 = []\nfor i in range(1000):\n    a1.append(a[i]**2)\n\n426 µs ± 19.1 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n\n\n\n%%timeit \nglobal a2\na2 = [i**2 for i in a]\n\n255 µs ± 14.9 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n\n\n\nb = np.arange(1000)\n\n\n%%timeit \nb1 = b**2\n\n1.68 µs ± 96.2 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)\n\n\nIn the numpy package the terminology used for vectors, matrices and higher-dimensional data sets is array."
  },
  {
    "objectID": "NumPy_tutorial.html#documentation",
    "href": "NumPy_tutorial.html#documentation",
    "title": "Appendix A — Numpy - multidimensional data arrays for python",
    "section": "A.2 Documentation",
    "text": "A.2 Documentation\n\nhttps://scipy-lectures.org/intro/numpy/index.html\nhttps://numpy.org/doc/\n\n\nnp.array?\n\nDocstring:\narray(object, dtype=None, copy=True, order='K', subok=False, ndmin=0)\n\nCreate an array.\n\nParameters\n----------\nobject : array_like\n    An array, any object exposing the array interface, an object whose\n    __array__ method returns an array, or any (nested) sequence.\ndtype : data-type, optional\n    The desired data-type for the array.  If not given, then the type will\n    be determined as the minimum type required to hold the objects in the\n    sequence.\ncopy : bool, optional\n    If true (default), then the object is copied.  Otherwise, a copy will\n    only be made if __array__ returns a copy, if obj is a nested sequence,\n    or if a copy is needed to satisfy any of the other requirements\n    (`dtype`, `order`, etc.).\norder : {'K', 'A', 'C', 'F'}, optional\n    Specify the memory layout of the array. If object is not an array, the\n    newly created array will be in C order (row major) unless 'F' is\n    specified, in which case it will be in Fortran order (column major).\n    If object is an array the following holds.\n\n    ===== ========= ===================================================\n    order  no copy                     copy=True\n    ===== ========= ===================================================\n    'K'   unchanged F & C order preserved, otherwise most similar order\n    'A'   unchanged F order if input is F and not C, otherwise C order\n    'C'   C order   C order\n    'F'   F order   F order\n    ===== ========= ===================================================\n\n    When ``copy=False`` and a copy is made for other reasons, the result is\n    the same as if ``copy=True``, with some exceptions for `A`, see the\n    Notes section. The default order is 'K'.\nsubok : bool, optional\n    If True, then sub-classes will be passed-through, otherwise\n    the returned array will be forced to be a base-class array (default).\nndmin : int, optional\n    Specifies the minimum number of dimensions that the resulting\n    array should have.  Ones will be pre-pended to the shape as\n    needed to meet this requirement.\n\nReturns\n-------\nout : ndarray\n    An array object satisfying the specified requirements.\n\nSee Also\n--------\nempty_like : Return an empty array with shape and type of input.\nones_like : Return an array of ones with shape and type of input.\nzeros_like : Return an array of zeros with shape and type of input.\nfull_like : Return a new array with shape of input filled with value.\nempty : Return a new uninitialized array.\nones : Return a new array setting values to one.\nzeros : Return a new array setting values to zero.\nfull : Return a new array of given shape filled with value.\n\n\nNotes\n-----\nWhen order is 'A' and `object` is an array in neither 'C' nor 'F' order,\nand a copy is forced by a change in dtype, then the order of the result is\nnot necessarily 'C' as expected. This is likely a bug.\n\nExamples\n--------\n&gt;&gt;&gt; np.array([1, 2, 3])\narray([1, 2, 3])\n\nUpcasting:\n\n&gt;&gt;&gt; np.array([1, 2, 3.0])\narray([ 1.,  2.,  3.])\n\nMore than one dimension:\n\n&gt;&gt;&gt; np.array([[1, 2], [3, 4]])\narray([[1, 2],\n       [3, 4]])\n\nMinimum dimensions 2:\n\n&gt;&gt;&gt; np.array([1, 2, 3], ndmin=2)\narray([[1, 2, 3]])\n\nType provided:\n\n&gt;&gt;&gt; np.array([1, 2, 3], dtype=complex)\narray([ 1.+0.j,  2.+0.j,  3.+0.j])\n\nData-type consisting of more than one element:\n\n&gt;&gt;&gt; x = np.array([(1,2),(3,4)],dtype=[('a','&lt;i4'),('b','&lt;i4')])\n&gt;&gt;&gt; x['a']\narray([1, 3])\n\nCreating an array from sub-classes:\n\n&gt;&gt;&gt; np.array(np.mat('1 2; 3 4'))\narray([[1, 2],\n       [3, 4]])\n\n&gt;&gt;&gt; np.array(np.mat('1 2; 3 4'), subok=True)\nmatrix([[1, 2],\n        [3, 4]])\nType:      builtin_function_or_method"
  },
  {
    "objectID": "NumPy_tutorial.html#creating-numpy-arrays",
    "href": "NumPy_tutorial.html#creating-numpy-arrays",
    "title": "Appendix A — Numpy - multidimensional data arrays for python",
    "section": "A.3 Creating numpy arrays",
    "text": "A.3 Creating numpy arrays\nThere are a number of ways to initialize new numpy arrays, for example, from\n\nA Python list or tuples\nUsing functions that are dedicated to generating numpy arrays, such as arange, linspace, etc.\nReading data from files (npy)\n\n\nA.3.1 From Python list\nFor example, to create new vector and matrix arrays from Python lists, we can use the numpy.array function.\n\n# a vector: the argument to the array function is a Python list\nv = np.array([1,2,3,4])\nv, type(v), v.dtype, v.shape\n\n(array([1, 2, 3, 4]), numpy.ndarray, dtype('int32'), (4,))\n\n\n\n# a matrix: the argument to the array function is a nested Python list\nM = np.array([[1, 2], [3, 4]])\nM, type(M), M.dtype, M.shape\n\n(array([[1, 2],\n        [3, 4]]),\n numpy.ndarray,\n dtype('int32'),\n (2, 2))\n\n\nNote that the v and M objects are both of the type ndarray that the numpy module provides. The difference between the v and M arrays is only their shapes. We can get information about the shape of an array by using the ndarray.shape property.\nSince it is statically typing, we can explicitly define the type of the array data when we create it, using the dtype keyword argument:\n\nM = np.array([[1, 2], [3, 4]], dtype=complex)\nM\n\narray([[1.+0.j, 2.+0.j],\n       [3.+0.j, 4.+0.j]])\n\n\nCommon data types that can be used with dtype are: int, float, complex, bool, etc.\nWe can also explicitly define the bit size of the data types, for example: int64, int16, float128, complex128.\n\n\nA.3.2 Using array-generating functions\nFor larger arrays, it is impractical to initialize the data manually using explicit python lists. Instead, we can use one of the many functions in numpy that generate arrays of different forms. Some of the more common are:\n\nnp.arange(-1, 1, 0.1) # arguments: start, stop, step\n\narray([-1.00000000e+00, -9.00000000e-01, -8.00000000e-01, -7.00000000e-01,\n       -6.00000000e-01, -5.00000000e-01, -4.00000000e-01, -3.00000000e-01,\n       -2.00000000e-01, -1.00000000e-01, -2.22044605e-16,  1.00000000e-01,\n        2.00000000e-01,  3.00000000e-01,  4.00000000e-01,  5.00000000e-01,\n        6.00000000e-01,  7.00000000e-01,  8.00000000e-01,  9.00000000e-01])\n\n\n\n# using linspace, both end points ARE included\nnp.linspace(0, 10, 25) # arguments: start, end, number of samples\n\narray([ 0.        ,  0.41666667,  0.83333333,  1.25      ,  1.66666667,\n        2.08333333,  2.5       ,  2.91666667,  3.33333333,  3.75      ,\n        4.16666667,  4.58333333,  5.        ,  5.41666667,  5.83333333,\n        6.25      ,  6.66666667,  7.08333333,  7.5       ,  7.91666667,\n        8.33333333,  8.75      ,  9.16666667,  9.58333333, 10.        ])\n\n\n\nfrom numpy import random\n\n\n# uniform random numbers in [0,1]\nnp.random.rand(5,5) #argument: shape\n\narray([[0.95856122, 0.46008766, 0.18125959, 0.29118265, 0.12936857],\n       [0.66136799, 0.31069994, 0.02396709, 0.19487356, 0.2781103 ],\n       [0.95491478, 0.39030392, 0.98749426, 0.11391192, 0.71392245],\n       [0.45548694, 0.26654714, 0.39209578, 0.09068336, 0.1440259 ],\n       [0.65795932, 0.07484714, 0.33585994, 0.38683142, 0.25092455]])\n\n\n\n# standard normal distributed random numbers\nnp.random.randn(5,5)\n\narray([[-0.1083494 , -1.4625737 , -1.52901998, -0.19867851, -0.69311333],\n       [ 0.22918277, -0.54191491, -0.11518915, -0.39199225, -0.46892591],\n       [-1.74171355,  0.04522399, -1.30233269, -0.56877774, -0.96248809],\n       [ 0.47210184, -0.67675756,  0.25428361,  0.42873618,  0.94328066],\n       [ 1.04585954, -1.53339424,  1.22914079,  0.83127729, -0.45995271]])\n\n\n\nA.3.2.1 diag\n\n# a diagonal matrix\nnp.diag([1,2,3])\n\narray([[1, 0, 0],\n       [0, 2, 0],\n       [0, 0, 3]])\n\n\n\n\nA.3.2.2 zeros and ones\n\nnp.zeros((3,3))\n\narray([[0., 0., 0.],\n       [0., 0., 0.],\n       [0., 0., 0.]])\n\n\n\nnp.ones((3,3))\n\narray([[1., 1., 1.],\n       [1., 1., 1.],\n       [1., 1., 1.]])\n\n\n\nnp.random.seed(89)\nnp.random.rand(5)\n\narray([0.49969432, 0.25593713, 0.25810063, 0.09692171, 0.56418511])\n\n\n\nnp.random.rand(5)\n\narray([0.01599007, 0.15259523, 0.48024773, 0.09987276, 0.41696389])"
  },
  {
    "objectID": "NumPy_tutorial.html#manipulating-arrays",
    "href": "NumPy_tutorial.html#manipulating-arrays",
    "title": "Appendix A — Numpy - multidimensional data arrays for python",
    "section": "A.4 Manipulating arrays",
    "text": "A.4 Manipulating arrays\n\nA.4.1 Indexing and slicing\n\nNote that the indices begin at 0, like other Python sequences (and C/C++). In contrast, in Fortran or Matlab, indices start at 1.\nIn 2D, the first dimension corresponds to rows, the second to columns.\n\nWe can index elements in an array using square brackets and indices:\n\n# v is a vector, and has only one dimension, taking one index\nv = np.random.rand(5) #Note it starts with zero \nv, v[3]\n\n(array([0.91365081, 0.35071951, 0.11460437, 0.71260839, 0.10188615]),\n 0.7126083905021839)\n\n\n\n# M is a matrix, or a 2 dimensional array, taking two indices \nM = np.random.rand(5,5)\nM, M[2,3]\n\n(array([[0.40570044, 0.66548144, 0.13835937, 0.83043309, 0.12319969],\n        [0.58779155, 0.06309849, 0.49710274, 0.92839462, 0.80603084],\n        [0.19839124, 0.34528354, 0.53473647, 0.97858347, 0.5030445 ],\n        [0.3474475 , 0.21278653, 0.17745402, 0.1040286 , 0.18745545],\n        [0.04031375, 0.23991727, 0.5462427 , 0.20778317, 0.99270398]]),\n 0.9785834687356999)\n\n\nWe can get rows and columns as follows\n\nM[1,:] # row 1\n\narray([0.58779155, 0.06309849, 0.49710274, 0.92839462, 0.80603084])\n\n\n\nM[:,1] # column 1\n\narray([0.66548144, 0.06309849, 0.34528354, 0.21278653, 0.23991727])\n\n\nIndex slicing is the technical name for the syntax M[lower:upper:step] to extract part of an array:\n\nA = np.array([1,2,3,4,5])\nA\n\narray([1, 2, 3, 4, 5])\n\n\n\nA[1:3]\n\narray([2, 3])\n\n\nWe can omit any of the three parameters in M[lower:upper:step]:\n\nA[::2] # step is 2, lower and upper defaults to the beginning and end of the array\n\narray([1, 3, 5])\n\n\n\nA[:3] # first three elements\n\narray([1, 2, 3])\n\n\n\nA[3:] # elements from index 3\n\narray([4, 5])\n\n\nNegative indices counts from the end of the array (positive index from the begining):\n\nA = array([1,2,3,4,5])\n\nNameError: name 'array' is not defined\n\n\n\nA[-1] # the last element in the array\n\n5\n\n\n\nA[-3:] # the last three elements\n\narray([3, 4, 5])\n\n\nIndex slicing works exactly the same way for multidimensional arrays:\n\nA = np.array([[n+m*10 for n in range(5)] for m in range(5)])\nA\n\narray([[ 0,  1,  2,  3,  4],\n       [10, 11, 12, 13, 14],\n       [20, 21, 22, 23, 24],\n       [30, 31, 32, 33, 34],\n       [40, 41, 42, 43, 44]])\n\n\n\n# a block from the original array\nA[1:4, 1:4]\n\narray([[11, 12, 13],\n       [21, 22, 23],\n       [31, 32, 33]])\n\n\n\n\nChcek “Fancy indexing” at https://scipy-lectures.org/intro/numpy/array_object.html#fancy-indexing"
  },
  {
    "objectID": "NumPy_tutorial.html#linear-algebra-on-array",
    "href": "NumPy_tutorial.html#linear-algebra-on-array",
    "title": "Appendix A — Numpy - multidimensional data arrays for python",
    "section": "A.5 Linear algebra on array",
    "text": "A.5 Linear algebra on array\nVectorizing code is the key to writing efficient numerical calculations with Python/Numpy. That means that as much as possible of a program should be formulated in terms of matrix and vector operations, like matrix-matrix multiplication.\n\nA.5.1 Scalar and array operations\nWe can use the usual arithmetic operators to multiply, add, subtract, and divide arrays with scalar numbers.\n\nv = np.arange(0, 5)\n\n\nv * 2, v + 2\n\n(array([0, 2, 4, 6, 8]), array([2, 3, 4, 5, 6]))\n\n\n\nprint(A * 2)\nprint(A + 2)\n\n[[ 0  2  4  6  8]\n [20 22 24 26 28]\n [40 42 44 46 48]\n [60 62 64 66 68]\n [80 82 84 86 88]]\n[[ 2  3  4  5  6]\n [12 13 14 15 16]\n [22 23 24 25 26]\n [32 33 34 35 36]\n [42 43 44 45 46]]\n\n\n\na = np.arange(10000)\n\n\n%timeit a + 1  \n\n6.25 µs ± 186 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n\n\n\nl = range(10000)\n\n\n%timeit [i+1 for i in l] \n\n813 µs ± 65.3 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n\n\n\nA.5.1.1 Element-wise array-array operations\nWhen we add, subtract, multiply and divide arrays with each other, the default behavior is element-wise operations:\n\nA * A # element-wise multiplication\n\narray([[   0,    1,    4,    9,   16],\n       [ 100,  121,  144,  169,  196],\n       [ 400,  441,  484,  529,  576],\n       [ 900,  961, 1024, 1089, 1156],\n       [1600, 1681, 1764, 1849, 1936]])\n\n\n\nv * v\n\narray([ 0,  1,  4,  9, 16])\n\n\nIf we multiply arrays with compatible shapes, we get an element-wise multiplication of each row:\n\nA, v\n\n(array([[ 0,  1,  2,  3,  4],\n        [10, 11, 12, 13, 14],\n        [20, 21, 22, 23, 24],\n        [30, 31, 32, 33, 34],\n        [40, 41, 42, 43, 44]]),\n array([0, 1, 2, 3, 4]))\n\n\n\nA.shape, v.shape\n\n((5, 5), (5,))\n\n\n\nA * v    \n\narray([[  0,   1,   4,   9,  16],\n       [  0,  11,  24,  39,  56],\n       [  0,  21,  44,  69,  96],\n       [  0,  31,  64,  99, 136],\n       [  0,  41,  84, 129, 176]])\n\n\n\n\nSee Broadcasting at https://scipy-lectures.org/intro/numpy/operations.html#broadcasting and https://cs231n.github.io/python-numpy-tutorial/#broadcasting\n\n\n\n\n\nA.5.2 Matrix algebra\nWhat about matrix multiplication? There are two ways. We can either use the dot function, which applies a matrix-matrix, matrix-vector, or inner vector multiplication to its two arguments:\n\nnp.dot(A, A)\n\narray([[ 300,  310,  320,  330,  340],\n       [1300, 1360, 1420, 1480, 1540],\n       [2300, 2410, 2520, 2630, 2740],\n       [3300, 3460, 3620, 3780, 3940],\n       [4300, 4510, 4720, 4930, 5140]])\n\n\n\nnp.dot(A, v)\n\narray([ 30, 130, 230, 330, 430])\n\n\n\nnp.dot(v, v)\n\n30\n\n\n\nA.T #transpose\n\narray([[ 0, 10, 20, 30, 40],\n       [ 1, 11, 21, 31, 41],\n       [ 2, 12, 22, 32, 42],\n       [ 3, 13, 23, 33, 43],\n       [ 4, 14, 24, 34, 44]])\n\n\nAlternatively, we can cast the array objects to the type matrix. This changes the behavior of the standard arithmetic operators +, -, * to use matrix algebra. (Become matrix operation!)\n\nhelp(np.matrix)\n\nHelp on class matrix in module numpy:\n\nclass matrix(ndarray)\n |  matrix(data, dtype=None, copy=True)\n |  \n |  matrix(data, dtype=None, copy=True)\n |  \n |  .. note:: It is no longer recommended to use this class, even for linear\n |            algebra. Instead use regular arrays. The class may be removed\n |            in the future.\n |  \n |  Returns a matrix from an array-like object, or from a string of data.\n |  A matrix is a specialized 2-D array that retains its 2-D nature\n |  through operations.  It has certain special operators, such as ``*``\n |  (matrix multiplication) and ``**`` (matrix power).\n |  \n |  Parameters\n |  ----------\n |  data : array_like or string\n |     If `data` is a string, it is interpreted as a matrix with commas\n |     or spaces separating columns, and semicolons separating rows.\n |  dtype : data-type\n |     Data-type of the output matrix.\n |  copy : bool\n |     If `data` is already an `ndarray`, then this flag determines\n |     whether the data is copied (the default), or whether a view is\n |     constructed.\n |  \n |  See Also\n |  --------\n |  array\n |  \n |  Examples\n |  --------\n |  &gt;&gt;&gt; a = np.matrix('1 2; 3 4')\n |  &gt;&gt;&gt; a\n |  matrix([[1, 2],\n |          [3, 4]])\n |  \n |  &gt;&gt;&gt; np.matrix([[1, 2], [3, 4]])\n |  matrix([[1, 2],\n |          [3, 4]])\n |  \n |  Method resolution order:\n |      matrix\n |      ndarray\n |      builtins.object\n |  \n |  Methods defined here:\n |  \n |  __array_finalize__(self, obj)\n |      None.\n |  \n |  __getitem__(self, index)\n |      Return self[key].\n |  \n |  __imul__(self, other)\n |      Return self*=value.\n |  \n |  __ipow__(self, other)\n |      Return self**=value.\n |  \n |  __mul__(self, other)\n |      Return self*value.\n |  \n |  __pow__(self, other)\n |      Return pow(self, value, mod).\n |  \n |  __rmul__(self, other)\n |      Return value*self.\n |  \n |  __rpow__(self, other)\n |      Return pow(value, self, mod).\n |  \n |  all(self, axis=None, out=None)\n |      Test whether all matrix elements along a given axis evaluate to True.\n |      \n |      Parameters\n |      ----------\n |      See `numpy.all` for complete descriptions\n |      \n |      See Also\n |      --------\n |      numpy.all\n |      \n |      Notes\n |      -----\n |      This is the same as `ndarray.all`, but it returns a `matrix` object.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; x = np.matrix(np.arange(12).reshape((3,4))); x\n |      matrix([[ 0,  1,  2,  3],\n |              [ 4,  5,  6,  7],\n |              [ 8,  9, 10, 11]])\n |      &gt;&gt;&gt; y = x[0]; y\n |      matrix([[0, 1, 2, 3]])\n |      &gt;&gt;&gt; (x == y)\n |      matrix([[ True,  True,  True,  True],\n |              [False, False, False, False],\n |              [False, False, False, False]])\n |      &gt;&gt;&gt; (x == y).all()\n |      False\n |      &gt;&gt;&gt; (x == y).all(0)\n |      matrix([[False, False, False, False]])\n |      &gt;&gt;&gt; (x == y).all(1)\n |      matrix([[ True],\n |              [False],\n |              [False]])\n |  \n |  any(self, axis=None, out=None)\n |      Test whether any array element along a given axis evaluates to True.\n |      \n |      Refer to `numpy.any` for full documentation.\n |      \n |      Parameters\n |      ----------\n |      axis : int, optional\n |          Axis along which logical OR is performed\n |      out : ndarray, optional\n |          Output to existing array instead of creating new one, must have\n |          same shape as expected output\n |      \n |      Returns\n |      -------\n |          any : bool, ndarray\n |              Returns a single bool if `axis` is ``None``; otherwise,\n |              returns `ndarray`\n |  \n |  argmax(self, axis=None, out=None)\n |      Indexes of the maximum values along an axis.\n |      \n |      Return the indexes of the first occurrences of the maximum values\n |      along the specified axis.  If axis is None, the index is for the\n |      flattened matrix.\n |      \n |      Parameters\n |      ----------\n |      See `numpy.argmax` for complete descriptions\n |      \n |      See Also\n |      --------\n |      numpy.argmax\n |      \n |      Notes\n |      -----\n |      This is the same as `ndarray.argmax`, but returns a `matrix` object\n |      where `ndarray.argmax` would return an `ndarray`.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; x = np.matrix(np.arange(12).reshape((3,4))); x\n |      matrix([[ 0,  1,  2,  3],\n |              [ 4,  5,  6,  7],\n |              [ 8,  9, 10, 11]])\n |      &gt;&gt;&gt; x.argmax()\n |      11\n |      &gt;&gt;&gt; x.argmax(0)\n |      matrix([[2, 2, 2, 2]])\n |      &gt;&gt;&gt; x.argmax(1)\n |      matrix([[3],\n |              [3],\n |              [3]])\n |  \n |  argmin(self, axis=None, out=None)\n |      Indexes of the minimum values along an axis.\n |      \n |      Return the indexes of the first occurrences of the minimum values\n |      along the specified axis.  If axis is None, the index is for the\n |      flattened matrix.\n |      \n |      Parameters\n |      ----------\n |      See `numpy.argmin` for complete descriptions.\n |      \n |      See Also\n |      --------\n |      numpy.argmin\n |      \n |      Notes\n |      -----\n |      This is the same as `ndarray.argmin`, but returns a `matrix` object\n |      where `ndarray.argmin` would return an `ndarray`.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; x = -np.matrix(np.arange(12).reshape((3,4))); x\n |      matrix([[  0,  -1,  -2,  -3],\n |              [ -4,  -5,  -6,  -7],\n |              [ -8,  -9, -10, -11]])\n |      &gt;&gt;&gt; x.argmin()\n |      11\n |      &gt;&gt;&gt; x.argmin(0)\n |      matrix([[2, 2, 2, 2]])\n |      &gt;&gt;&gt; x.argmin(1)\n |      matrix([[3],\n |              [3],\n |              [3]])\n |  \n |  flatten(self, order='C')\n |      Return a flattened copy of the matrix.\n |      \n |      All `N` elements of the matrix are placed into a single row.\n |      \n |      Parameters\n |      ----------\n |      order : {'C', 'F', 'A', 'K'}, optional\n |          'C' means to flatten in row-major (C-style) order. 'F' means to\n |          flatten in column-major (Fortran-style) order. 'A' means to\n |          flatten in column-major order if `m` is Fortran *contiguous* in\n |          memory, row-major order otherwise. 'K' means to flatten `m` in\n |          the order the elements occur in memory. The default is 'C'.\n |      \n |      Returns\n |      -------\n |      y : matrix\n |          A copy of the matrix, flattened to a `(1, N)` matrix where `N`\n |          is the number of elements in the original matrix.\n |      \n |      See Also\n |      --------\n |      ravel : Return a flattened array.\n |      flat : A 1-D flat iterator over the matrix.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; m = np.matrix([[1,2], [3,4]])\n |      &gt;&gt;&gt; m.flatten()\n |      matrix([[1, 2, 3, 4]])\n |      &gt;&gt;&gt; m.flatten('F')\n |      matrix([[1, 3, 2, 4]])\n |  \n |  getA = A(self)\n |      Return `self` as an `ndarray` object.\n |      \n |      Equivalent to ``np.asarray(self)``.\n |      \n |      Parameters\n |      ----------\n |      None\n |      \n |      Returns\n |      -------\n |      ret : ndarray\n |          `self` as an `ndarray`\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; x = np.matrix(np.arange(12).reshape((3,4))); x\n |      matrix([[ 0,  1,  2,  3],\n |              [ 4,  5,  6,  7],\n |              [ 8,  9, 10, 11]])\n |      &gt;&gt;&gt; x.getA()\n |      array([[ 0,  1,  2,  3],\n |             [ 4,  5,  6,  7],\n |             [ 8,  9, 10, 11]])\n |  \n |  getA1 = A1(self)\n |      Return `self` as a flattened `ndarray`.\n |      \n |      Equivalent to ``np.asarray(x).ravel()``\n |      \n |      Parameters\n |      ----------\n |      None\n |      \n |      Returns\n |      -------\n |      ret : ndarray\n |          `self`, 1-D, as an `ndarray`\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; x = np.matrix(np.arange(12).reshape((3,4))); x\n |      matrix([[ 0,  1,  2,  3],\n |              [ 4,  5,  6,  7],\n |              [ 8,  9, 10, 11]])\n |      &gt;&gt;&gt; x.getA1()\n |      array([ 0,  1,  2, ...,  9, 10, 11])\n |  \n |  getH = H(self)\n |      Returns the (complex) conjugate transpose of `self`.\n |      \n |      Equivalent to ``np.transpose(self)`` if `self` is real-valued.\n |      \n |      Parameters\n |      ----------\n |      None\n |      \n |      Returns\n |      -------\n |      ret : matrix object\n |          complex conjugate transpose of `self`\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; x = np.matrix(np.arange(12).reshape((3,4)))\n |      &gt;&gt;&gt; z = x - 1j*x; z\n |      matrix([[  0. +0.j,   1. -1.j,   2. -2.j,   3. -3.j],\n |              [  4. -4.j,   5. -5.j,   6. -6.j,   7. -7.j],\n |              [  8. -8.j,   9. -9.j,  10.-10.j,  11.-11.j]])\n |      &gt;&gt;&gt; z.getH()\n |      matrix([[ 0. -0.j,  4. +4.j,  8. +8.j],\n |              [ 1. +1.j,  5. +5.j,  9. +9.j],\n |              [ 2. +2.j,  6. +6.j, 10.+10.j],\n |              [ 3. +3.j,  7. +7.j, 11.+11.j]])\n |  \n |  getI = I(self)\n |      Returns the (multiplicative) inverse of invertible `self`.\n |      \n |      Parameters\n |      ----------\n |      None\n |      \n |      Returns\n |      -------\n |      ret : matrix object\n |          If `self` is non-singular, `ret` is such that ``ret * self`` ==\n |          ``self * ret`` == ``np.matrix(np.eye(self[0,:].size)`` all return\n |          ``True``.\n |      \n |      Raises\n |      ------\n |      numpy.linalg.LinAlgError: Singular matrix\n |          If `self` is singular.\n |      \n |      See Also\n |      --------\n |      linalg.inv\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; m = np.matrix('[1, 2; 3, 4]'); m\n |      matrix([[1, 2],\n |              [3, 4]])\n |      &gt;&gt;&gt; m.getI()\n |      matrix([[-2. ,  1. ],\n |              [ 1.5, -0.5]])\n |      &gt;&gt;&gt; m.getI() * m\n |      matrix([[ 1.,  0.], # may vary\n |              [ 0.,  1.]])\n |  \n |  getT = T(self)\n |      Returns the transpose of the matrix.\n |      \n |      Does *not* conjugate!  For the complex conjugate transpose, use ``.H``.\n |      \n |      Parameters\n |      ----------\n |      None\n |      \n |      Returns\n |      -------\n |      ret : matrix object\n |          The (non-conjugated) transpose of the matrix.\n |      \n |      See Also\n |      --------\n |      transpose, getH\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; m = np.matrix('[1, 2; 3, 4]')\n |      &gt;&gt;&gt; m\n |      matrix([[1, 2],\n |              [3, 4]])\n |      &gt;&gt;&gt; m.getT()\n |      matrix([[1, 3],\n |              [2, 4]])\n |  \n |  max(self, axis=None, out=None)\n |      Return the maximum value along an axis.\n |      \n |      Parameters\n |      ----------\n |      See `amax` for complete descriptions\n |      \n |      See Also\n |      --------\n |      amax, ndarray.max\n |      \n |      Notes\n |      -----\n |      This is the same as `ndarray.max`, but returns a `matrix` object\n |      where `ndarray.max` would return an ndarray.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; x = np.matrix(np.arange(12).reshape((3,4))); x\n |      matrix([[ 0,  1,  2,  3],\n |              [ 4,  5,  6,  7],\n |              [ 8,  9, 10, 11]])\n |      &gt;&gt;&gt; x.max()\n |      11\n |      &gt;&gt;&gt; x.max(0)\n |      matrix([[ 8,  9, 10, 11]])\n |      &gt;&gt;&gt; x.max(1)\n |      matrix([[ 3],\n |              [ 7],\n |              [11]])\n |  \n |  mean(self, axis=None, dtype=None, out=None)\n |      Returns the average of the matrix elements along the given axis.\n |      \n |      Refer to `numpy.mean` for full documentation.\n |      \n |      See Also\n |      --------\n |      numpy.mean\n |      \n |      Notes\n |      -----\n |      Same as `ndarray.mean` except that, where that returns an `ndarray`,\n |      this returns a `matrix` object.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; x = np.matrix(np.arange(12).reshape((3, 4)))\n |      &gt;&gt;&gt; x\n |      matrix([[ 0,  1,  2,  3],\n |              [ 4,  5,  6,  7],\n |              [ 8,  9, 10, 11]])\n |      &gt;&gt;&gt; x.mean()\n |      5.5\n |      &gt;&gt;&gt; x.mean(0)\n |      matrix([[4., 5., 6., 7.]])\n |      &gt;&gt;&gt; x.mean(1)\n |      matrix([[ 1.5],\n |              [ 5.5],\n |              [ 9.5]])\n |  \n |  min(self, axis=None, out=None)\n |      Return the minimum value along an axis.\n |      \n |      Parameters\n |      ----------\n |      See `amin` for complete descriptions.\n |      \n |      See Also\n |      --------\n |      amin, ndarray.min\n |      \n |      Notes\n |      -----\n |      This is the same as `ndarray.min`, but returns a `matrix` object\n |      where `ndarray.min` would return an ndarray.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; x = -np.matrix(np.arange(12).reshape((3,4))); x\n |      matrix([[  0,  -1,  -2,  -3],\n |              [ -4,  -5,  -6,  -7],\n |              [ -8,  -9, -10, -11]])\n |      &gt;&gt;&gt; x.min()\n |      -11\n |      &gt;&gt;&gt; x.min(0)\n |      matrix([[ -8,  -9, -10, -11]])\n |      &gt;&gt;&gt; x.min(1)\n |      matrix([[ -3],\n |              [ -7],\n |              [-11]])\n |  \n |  prod(self, axis=None, dtype=None, out=None)\n |      Return the product of the array elements over the given axis.\n |      \n |      Refer to `prod` for full documentation.\n |      \n |      See Also\n |      --------\n |      prod, ndarray.prod\n |      \n |      Notes\n |      -----\n |      Same as `ndarray.prod`, except, where that returns an `ndarray`, this\n |      returns a `matrix` object instead.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; x = np.matrix(np.arange(12).reshape((3,4))); x\n |      matrix([[ 0,  1,  2,  3],\n |              [ 4,  5,  6,  7],\n |              [ 8,  9, 10, 11]])\n |      &gt;&gt;&gt; x.prod()\n |      0\n |      &gt;&gt;&gt; x.prod(0)\n |      matrix([[  0,  45, 120, 231]])\n |      &gt;&gt;&gt; x.prod(1)\n |      matrix([[   0],\n |              [ 840],\n |              [7920]])\n |  \n |  ptp(self, axis=None, out=None)\n |      Peak-to-peak (maximum - minimum) value along the given axis.\n |      \n |      Refer to `numpy.ptp` for full documentation.\n |      \n |      See Also\n |      --------\n |      numpy.ptp\n |      \n |      Notes\n |      -----\n |      Same as `ndarray.ptp`, except, where that would return an `ndarray` object,\n |      this returns a `matrix` object.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; x = np.matrix(np.arange(12).reshape((3,4))); x\n |      matrix([[ 0,  1,  2,  3],\n |              [ 4,  5,  6,  7],\n |              [ 8,  9, 10, 11]])\n |      &gt;&gt;&gt; x.ptp()\n |      11\n |      &gt;&gt;&gt; x.ptp(0)\n |      matrix([[8, 8, 8, 8]])\n |      &gt;&gt;&gt; x.ptp(1)\n |      matrix([[3],\n |              [3],\n |              [3]])\n |  \n |  ravel(self, order='C')\n |      Return a flattened matrix.\n |      \n |      Refer to `numpy.ravel` for more documentation.\n |      \n |      Parameters\n |      ----------\n |      order : {'C', 'F', 'A', 'K'}, optional\n |          The elements of `m` are read using this index order. 'C' means to\n |          index the elements in C-like order, with the last axis index\n |          changing fastest, back to the first axis index changing slowest.\n |          'F' means to index the elements in Fortran-like index order, with\n |          the first index changing fastest, and the last index changing\n |          slowest. Note that the 'C' and 'F' options take no account of the\n |          memory layout of the underlying array, and only refer to the order\n |          of axis indexing.  'A' means to read the elements in Fortran-like\n |          index order if `m` is Fortran *contiguous* in memory, C-like order\n |          otherwise.  'K' means to read the elements in the order they occur\n |          in memory, except for reversing the data when strides are negative.\n |          By default, 'C' index order is used.\n |      \n |      Returns\n |      -------\n |      ret : matrix\n |          Return the matrix flattened to shape `(1, N)` where `N`\n |          is the number of elements in the original matrix.\n |          A copy is made only if necessary.\n |      \n |      See Also\n |      --------\n |      matrix.flatten : returns a similar output matrix but always a copy\n |      matrix.flat : a flat iterator on the array.\n |      numpy.ravel : related function which returns an ndarray\n |  \n |  squeeze(self, axis=None)\n |      Return a possibly reshaped matrix.\n |      \n |      Refer to `numpy.squeeze` for more documentation.\n |      \n |      Parameters\n |      ----------\n |      axis : None or int or tuple of ints, optional\n |          Selects a subset of the single-dimensional entries in the shape.\n |          If an axis is selected with shape entry greater than one,\n |          an error is raised.\n |      \n |      Returns\n |      -------\n |      squeezed : matrix\n |          The matrix, but as a (1, N) matrix if it had shape (N, 1).\n |      \n |      See Also\n |      --------\n |      numpy.squeeze : related function\n |      \n |      Notes\n |      -----\n |      If `m` has a single column then that column is returned\n |      as the single row of a matrix.  Otherwise `m` is returned.\n |      The returned matrix is always either `m` itself or a view into `m`.\n |      Supplying an axis keyword argument will not affect the returned matrix\n |      but it may cause an error to be raised.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; c = np.matrix([[1], [2]])\n |      &gt;&gt;&gt; c\n |      matrix([[1],\n |              [2]])\n |      &gt;&gt;&gt; c.squeeze()\n |      matrix([[1, 2]])\n |      &gt;&gt;&gt; r = c.T\n |      &gt;&gt;&gt; r\n |      matrix([[1, 2]])\n |      &gt;&gt;&gt; r.squeeze()\n |      matrix([[1, 2]])\n |      &gt;&gt;&gt; m = np.matrix([[1, 2], [3, 4]])\n |      &gt;&gt;&gt; m.squeeze()\n |      matrix([[1, 2],\n |              [3, 4]])\n |  \n |  std(self, axis=None, dtype=None, out=None, ddof=0)\n |      Return the standard deviation of the array elements along the given axis.\n |      \n |      Refer to `numpy.std` for full documentation.\n |      \n |      See Also\n |      --------\n |      numpy.std\n |      \n |      Notes\n |      -----\n |      This is the same as `ndarray.std`, except that where an `ndarray` would\n |      be returned, a `matrix` object is returned instead.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; x = np.matrix(np.arange(12).reshape((3, 4)))\n |      &gt;&gt;&gt; x\n |      matrix([[ 0,  1,  2,  3],\n |              [ 4,  5,  6,  7],\n |              [ 8,  9, 10, 11]])\n |      &gt;&gt;&gt; x.std()\n |      3.4520525295346629 # may vary\n |      &gt;&gt;&gt; x.std(0)\n |      matrix([[ 3.26598632,  3.26598632,  3.26598632,  3.26598632]]) # may vary\n |      &gt;&gt;&gt; x.std(1)\n |      matrix([[ 1.11803399],\n |              [ 1.11803399],\n |              [ 1.11803399]])\n |  \n |  sum(self, axis=None, dtype=None, out=None)\n |      Returns the sum of the matrix elements, along the given axis.\n |      \n |      Refer to `numpy.sum` for full documentation.\n |      \n |      See Also\n |      --------\n |      numpy.sum\n |      \n |      Notes\n |      -----\n |      This is the same as `ndarray.sum`, except that where an `ndarray` would\n |      be returned, a `matrix` object is returned instead.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; x = np.matrix([[1, 2], [4, 3]])\n |      &gt;&gt;&gt; x.sum()\n |      10\n |      &gt;&gt;&gt; x.sum(axis=1)\n |      matrix([[3],\n |              [7]])\n |      &gt;&gt;&gt; x.sum(axis=1, dtype='float')\n |      matrix([[3.],\n |              [7.]])\n |      &gt;&gt;&gt; out = np.zeros((2, 1), dtype='float')\n |      &gt;&gt;&gt; x.sum(axis=1, dtype='float', out=np.asmatrix(out))\n |      matrix([[3.],\n |              [7.]])\n |  \n |  tolist(self)\n |      Return the matrix as a (possibly nested) list.\n |      \n |      See `ndarray.tolist` for full documentation.\n |      \n |      See Also\n |      --------\n |      ndarray.tolist\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; x = np.matrix(np.arange(12).reshape((3,4))); x\n |      matrix([[ 0,  1,  2,  3],\n |              [ 4,  5,  6,  7],\n |              [ 8,  9, 10, 11]])\n |      &gt;&gt;&gt; x.tolist()\n |      [[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11]]\n |  \n |  var(self, axis=None, dtype=None, out=None, ddof=0)\n |      Returns the variance of the matrix elements, along the given axis.\n |      \n |      Refer to `numpy.var` for full documentation.\n |      \n |      See Also\n |      --------\n |      numpy.var\n |      \n |      Notes\n |      -----\n |      This is the same as `ndarray.var`, except that where an `ndarray` would\n |      be returned, a `matrix` object is returned instead.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; x = np.matrix(np.arange(12).reshape((3, 4)))\n |      &gt;&gt;&gt; x\n |      matrix([[ 0,  1,  2,  3],\n |              [ 4,  5,  6,  7],\n |              [ 8,  9, 10, 11]])\n |      &gt;&gt;&gt; x.var()\n |      11.916666666666666\n |      &gt;&gt;&gt; x.var(0)\n |      matrix([[ 10.66666667,  10.66666667,  10.66666667,  10.66666667]]) # may vary\n |      &gt;&gt;&gt; x.var(1)\n |      matrix([[1.25],\n |              [1.25],\n |              [1.25]])\n |  \n |  ----------------------------------------------------------------------\n |  Static methods defined here:\n |  \n |  __new__(subtype, data, dtype=None, copy=True)\n |      Create and return a new object.  See help(type) for accurate signature.\n |  \n |  ----------------------------------------------------------------------\n |  Readonly properties defined here:\n |  \n |  A\n |      Return `self` as an `ndarray` object.\n |      \n |      Equivalent to ``np.asarray(self)``.\n |      \n |      Parameters\n |      ----------\n |      None\n |      \n |      Returns\n |      -------\n |      ret : ndarray\n |          `self` as an `ndarray`\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; x = np.matrix(np.arange(12).reshape((3,4))); x\n |      matrix([[ 0,  1,  2,  3],\n |              [ 4,  5,  6,  7],\n |              [ 8,  9, 10, 11]])\n |      &gt;&gt;&gt; x.getA()\n |      array([[ 0,  1,  2,  3],\n |             [ 4,  5,  6,  7],\n |             [ 8,  9, 10, 11]])\n |  \n |  A1\n |      Return `self` as a flattened `ndarray`.\n |      \n |      Equivalent to ``np.asarray(x).ravel()``\n |      \n |      Parameters\n |      ----------\n |      None\n |      \n |      Returns\n |      -------\n |      ret : ndarray\n |          `self`, 1-D, as an `ndarray`\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; x = np.matrix(np.arange(12).reshape((3,4))); x\n |      matrix([[ 0,  1,  2,  3],\n |              [ 4,  5,  6,  7],\n |              [ 8,  9, 10, 11]])\n |      &gt;&gt;&gt; x.getA1()\n |      array([ 0,  1,  2, ...,  9, 10, 11])\n |  \n |  H\n |      Returns the (complex) conjugate transpose of `self`.\n |      \n |      Equivalent to ``np.transpose(self)`` if `self` is real-valued.\n |      \n |      Parameters\n |      ----------\n |      None\n |      \n |      Returns\n |      -------\n |      ret : matrix object\n |          complex conjugate transpose of `self`\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; x = np.matrix(np.arange(12).reshape((3,4)))\n |      &gt;&gt;&gt; z = x - 1j*x; z\n |      matrix([[  0. +0.j,   1. -1.j,   2. -2.j,   3. -3.j],\n |              [  4. -4.j,   5. -5.j,   6. -6.j,   7. -7.j],\n |              [  8. -8.j,   9. -9.j,  10.-10.j,  11.-11.j]])\n |      &gt;&gt;&gt; z.getH()\n |      matrix([[ 0. -0.j,  4. +4.j,  8. +8.j],\n |              [ 1. +1.j,  5. +5.j,  9. +9.j],\n |              [ 2. +2.j,  6. +6.j, 10.+10.j],\n |              [ 3. +3.j,  7. +7.j, 11.+11.j]])\n |  \n |  I\n |      Returns the (multiplicative) inverse of invertible `self`.\n |      \n |      Parameters\n |      ----------\n |      None\n |      \n |      Returns\n |      -------\n |      ret : matrix object\n |          If `self` is non-singular, `ret` is such that ``ret * self`` ==\n |          ``self * ret`` == ``np.matrix(np.eye(self[0,:].size)`` all return\n |          ``True``.\n |      \n |      Raises\n |      ------\n |      numpy.linalg.LinAlgError: Singular matrix\n |          If `self` is singular.\n |      \n |      See Also\n |      --------\n |      linalg.inv\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; m = np.matrix('[1, 2; 3, 4]'); m\n |      matrix([[1, 2],\n |              [3, 4]])\n |      &gt;&gt;&gt; m.getI()\n |      matrix([[-2. ,  1. ],\n |              [ 1.5, -0.5]])\n |      &gt;&gt;&gt; m.getI() * m\n |      matrix([[ 1.,  0.], # may vary\n |              [ 0.,  1.]])\n |  \n |  T\n |      Returns the transpose of the matrix.\n |      \n |      Does *not* conjugate!  For the complex conjugate transpose, use ``.H``.\n |      \n |      Parameters\n |      ----------\n |      None\n |      \n |      Returns\n |      -------\n |      ret : matrix object\n |          The (non-conjugated) transpose of the matrix.\n |      \n |      See Also\n |      --------\n |      transpose, getH\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; m = np.matrix('[1, 2; 3, 4]')\n |      &gt;&gt;&gt; m\n |      matrix([[1, 2],\n |              [3, 4]])\n |      &gt;&gt;&gt; m.getT()\n |      matrix([[1, 3],\n |              [2, 4]])\n |  \n |  ----------------------------------------------------------------------\n |  Data descriptors defined here:\n |  \n |  __dict__\n |      dictionary for instance variables (if defined)\n |  \n |  ----------------------------------------------------------------------\n |  Data and other attributes defined here:\n |  \n |  __array_priority__ = 10.0\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from ndarray:\n |  \n |  __abs__(self, /)\n |      abs(self)\n |  \n |  __add__(self, value, /)\n |      Return self+value.\n |  \n |  __and__(self, value, /)\n |      Return self&value.\n |  \n |  __array__(...)\n |      a.__array__(|dtype) -&gt; reference if type unchanged, copy otherwise.\n |      \n |      Returns either a new reference to self if dtype is not given or a new array\n |      of provided data type if dtype is different from the current dtype of the\n |      array.\n |  \n |  __array_function__(...)\n |  \n |  __array_prepare__(...)\n |      a.__array_prepare__(obj) -&gt; Object of same type as ndarray object obj.\n |  \n |  __array_ufunc__(...)\n |  \n |  __array_wrap__(...)\n |      a.__array_wrap__(obj) -&gt; Object of same type as ndarray object a.\n |  \n |  __bool__(self, /)\n |      self != 0\n |  \n |  __complex__(...)\n |  \n |  __contains__(self, key, /)\n |      Return key in self.\n |  \n |  __copy__(...)\n |      a.__copy__()\n |      \n |      Used if :func:`copy.copy` is called on an array. Returns a copy of the array.\n |      \n |      Equivalent to ``a.copy(order='K')``.\n |  \n |  __deepcopy__(...)\n |      a.__deepcopy__(memo, /) -&gt; Deep copy of array.\n |      \n |      Used if :func:`copy.deepcopy` is called on an array.\n |  \n |  __delitem__(self, key, /)\n |      Delete self[key].\n |  \n |  __divmod__(self, value, /)\n |      Return divmod(self, value).\n |  \n |  __eq__(self, value, /)\n |      Return self==value.\n |  \n |  __float__(self, /)\n |      float(self)\n |  \n |  __floordiv__(self, value, /)\n |      Return self//value.\n |  \n |  __format__(...)\n |      Default object formatter.\n |  \n |  __ge__(self, value, /)\n |      Return self&gt;=value.\n |  \n |  __gt__(self, value, /)\n |      Return self&gt;value.\n |  \n |  __iadd__(self, value, /)\n |      Return self+=value.\n |  \n |  __iand__(self, value, /)\n |      Return self&=value.\n |  \n |  __ifloordiv__(self, value, /)\n |      Return self//=value.\n |  \n |  __ilshift__(self, value, /)\n |      Return self&lt;&lt;=value.\n |  \n |  __imatmul__(self, value, /)\n |      Return self@=value.\n |  \n |  __imod__(self, value, /)\n |      Return self%=value.\n |  \n |  __index__(self, /)\n |      Return self converted to an integer, if self is suitable for use as an index into a list.\n |  \n |  __int__(self, /)\n |      int(self)\n |  \n |  __invert__(self, /)\n |      ~self\n |  \n |  __ior__(self, value, /)\n |      Return self|=value.\n |  \n |  __irshift__(self, value, /)\n |      Return self&gt;&gt;=value.\n |  \n |  __isub__(self, value, /)\n |      Return self-=value.\n |  \n |  __iter__(self, /)\n |      Implement iter(self).\n |  \n |  __itruediv__(self, value, /)\n |      Return self/=value.\n |  \n |  __ixor__(self, value, /)\n |      Return self^=value.\n |  \n |  __le__(self, value, /)\n |      Return self&lt;=value.\n |  \n |  __len__(self, /)\n |      Return len(self).\n |  \n |  __lshift__(self, value, /)\n |      Return self&lt;&lt;value.\n |  \n |  __lt__(self, value, /)\n |      Return self&lt;value.\n |  \n |  __matmul__(self, value, /)\n |      Return self@value.\n |  \n |  __mod__(self, value, /)\n |      Return self%value.\n |  \n |  __ne__(self, value, /)\n |      Return self!=value.\n |  \n |  __neg__(self, /)\n |      -self\n |  \n |  __or__(self, value, /)\n |      Return self|value.\n |  \n |  __pos__(self, /)\n |      +self\n |  \n |  __radd__(self, value, /)\n |      Return value+self.\n |  \n |  __rand__(self, value, /)\n |      Return value&self.\n |  \n |  __rdivmod__(self, value, /)\n |      Return divmod(value, self).\n |  \n |  __reduce__(...)\n |      a.__reduce__()\n |      \n |      For pickling.\n |  \n |  __reduce_ex__(...)\n |      Helper for pickle.\n |  \n |  __repr__(self, /)\n |      Return repr(self).\n |  \n |  __rfloordiv__(self, value, /)\n |      Return value//self.\n |  \n |  __rlshift__(self, value, /)\n |      Return value&lt;&lt;self.\n |  \n |  __rmatmul__(self, value, /)\n |      Return value@self.\n |  \n |  __rmod__(self, value, /)\n |      Return value%self.\n |  \n |  __ror__(self, value, /)\n |      Return value|self.\n |  \n |  __rrshift__(self, value, /)\n |      Return value&gt;&gt;self.\n |  \n |  __rshift__(self, value, /)\n |      Return self&gt;&gt;value.\n |  \n |  __rsub__(self, value, /)\n |      Return value-self.\n |  \n |  __rtruediv__(self, value, /)\n |      Return value/self.\n |  \n |  __rxor__(self, value, /)\n |      Return value^self.\n |  \n |  __setitem__(self, key, value, /)\n |      Set self[key] to value.\n |  \n |  __setstate__(...)\n |      a.__setstate__(state, /)\n |      \n |      For unpickling.\n |      \n |      The `state` argument must be a sequence that contains the following\n |      elements:\n |      \n |      Parameters\n |      ----------\n |      version : int\n |          optional pickle version. If omitted defaults to 0.\n |      shape : tuple\n |      dtype : data-type\n |      isFortran : bool\n |      rawdata : string or list\n |          a binary string with the data (or a list if 'a' is an object array)\n |  \n |  __sizeof__(...)\n |      Size of object in memory, in bytes.\n |  \n |  __str__(self, /)\n |      Return str(self).\n |  \n |  __sub__(self, value, /)\n |      Return self-value.\n |  \n |  __truediv__(self, value, /)\n |      Return self/value.\n |  \n |  __xor__(self, value, /)\n |      Return self^value.\n |  \n |  argpartition(...)\n |      a.argpartition(kth, axis=-1, kind='introselect', order=None)\n |      \n |      Returns the indices that would partition this array.\n |      \n |      Refer to `numpy.argpartition` for full documentation.\n |      \n |      .. versionadded:: 1.8.0\n |      \n |      See Also\n |      --------\n |      numpy.argpartition : equivalent function\n |  \n |  argsort(...)\n |      a.argsort(axis=-1, kind=None, order=None)\n |      \n |      Returns the indices that would sort this array.\n |      \n |      Refer to `numpy.argsort` for full documentation.\n |      \n |      See Also\n |      --------\n |      numpy.argsort : equivalent function\n |  \n |  astype(...)\n |      a.astype(dtype, order='K', casting='unsafe', subok=True, copy=True)\n |      \n |      Copy of the array, cast to a specified type.\n |      \n |      Parameters\n |      ----------\n |      dtype : str or dtype\n |          Typecode or data-type to which the array is cast.\n |      order : {'C', 'F', 'A', 'K'}, optional\n |          Controls the memory layout order of the result.\n |          'C' means C order, 'F' means Fortran order, 'A'\n |          means 'F' order if all the arrays are Fortran contiguous,\n |          'C' order otherwise, and 'K' means as close to the\n |          order the array elements appear in memory as possible.\n |          Default is 'K'.\n |      casting : {'no', 'equiv', 'safe', 'same_kind', 'unsafe'}, optional\n |          Controls what kind of data casting may occur. Defaults to 'unsafe'\n |          for backwards compatibility.\n |      \n |            * 'no' means the data types should not be cast at all.\n |            * 'equiv' means only byte-order changes are allowed.\n |            * 'safe' means only casts which can preserve values are allowed.\n |            * 'same_kind' means only safe casts or casts within a kind,\n |              like float64 to float32, are allowed.\n |            * 'unsafe' means any data conversions may be done.\n |      subok : bool, optional\n |          If True, then sub-classes will be passed-through (default), otherwise\n |          the returned array will be forced to be a base-class array.\n |      copy : bool, optional\n |          By default, astype always returns a newly allocated array. If this\n |          is set to false, and the `dtype`, `order`, and `subok`\n |          requirements are satisfied, the input array is returned instead\n |          of a copy.\n |      \n |      Returns\n |      -------\n |      arr_t : ndarray\n |          Unless `copy` is False and the other conditions for returning the input\n |          array are satisfied (see description for `copy` input parameter), `arr_t`\n |          is a new array of the same shape as the input array, with dtype, order\n |          given by `dtype`, `order`.\n |      \n |      Notes\n |      -----\n |      .. versionchanged:: 1.17.0\n |         Casting between a simple data type and a structured one is possible only\n |         for \"unsafe\" casting.  Casting to multiple fields is allowed, but\n |         casting from multiple fields is not.\n |      \n |      .. versionchanged:: 1.9.0\n |         Casting from numeric to string types in 'safe' casting mode requires\n |         that the string dtype length is long enough to store the max\n |         integer/float value converted.\n |      \n |      Raises\n |      ------\n |      ComplexWarning\n |          When casting from complex to float or int. To avoid this,\n |          one should use ``a.real.astype(t)``.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; x = np.array([1, 2, 2.5])\n |      &gt;&gt;&gt; x\n |      array([1. ,  2. ,  2.5])\n |      \n |      &gt;&gt;&gt; x.astype(int)\n |      array([1, 2, 2])\n |  \n |  byteswap(...)\n |      a.byteswap(inplace=False)\n |      \n |      Swap the bytes of the array elements\n |      \n |      Toggle between low-endian and big-endian data representation by\n |      returning a byteswapped array, optionally swapped in-place.\n |      Arrays of byte-strings are not swapped. The real and imaginary\n |      parts of a complex number are swapped individually.\n |      \n |      Parameters\n |      ----------\n |      inplace : bool, optional\n |          If ``True``, swap bytes in-place, default is ``False``.\n |      \n |      Returns\n |      -------\n |      out : ndarray\n |          The byteswapped array. If `inplace` is ``True``, this is\n |          a view to self.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; A = np.array([1, 256, 8755], dtype=np.int16)\n |      &gt;&gt;&gt; list(map(hex, A))\n |      ['0x1', '0x100', '0x2233']\n |      &gt;&gt;&gt; A.byteswap(inplace=True)\n |      array([  256,     1, 13090], dtype=int16)\n |      &gt;&gt;&gt; list(map(hex, A))\n |      ['0x100', '0x1', '0x3322']\n |      \n |      Arrays of byte-strings are not swapped\n |      \n |      &gt;&gt;&gt; A = np.array([b'ceg', b'fac'])\n |      &gt;&gt;&gt; A.byteswap()\n |      array([b'ceg', b'fac'], dtype='|S3')\n |      \n |      ``A.newbyteorder().byteswap()`` produces an array with the same values\n |        but different representation in memory\n |      \n |      &gt;&gt;&gt; A = np.array([1, 2, 3])\n |      &gt;&gt;&gt; A.view(np.uint8)\n |      array([1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0,\n |             0, 0], dtype=uint8)\n |      &gt;&gt;&gt; A.newbyteorder().byteswap(inplace=True)\n |      array([1, 2, 3])\n |      &gt;&gt;&gt; A.view(np.uint8)\n |      array([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0,\n |             0, 3], dtype=uint8)\n |  \n |  choose(...)\n |      a.choose(choices, out=None, mode='raise')\n |      \n |      Use an index array to construct a new array from a set of choices.\n |      \n |      Refer to `numpy.choose` for full documentation.\n |      \n |      See Also\n |      --------\n |      numpy.choose : equivalent function\n |  \n |  clip(...)\n |      a.clip(min=None, max=None, out=None, **kwargs)\n |      \n |      Return an array whose values are limited to ``[min, max]``.\n |      One of max or min must be given.\n |      \n |      Refer to `numpy.clip` for full documentation.\n |      \n |      See Also\n |      --------\n |      numpy.clip : equivalent function\n |  \n |  compress(...)\n |      a.compress(condition, axis=None, out=None)\n |      \n |      Return selected slices of this array along given axis.\n |      \n |      Refer to `numpy.compress` for full documentation.\n |      \n |      See Also\n |      --------\n |      numpy.compress : equivalent function\n |  \n |  conj(...)\n |      a.conj()\n |      \n |      Complex-conjugate all elements.\n |      \n |      Refer to `numpy.conjugate` for full documentation.\n |      \n |      See Also\n |      --------\n |      numpy.conjugate : equivalent function\n |  \n |  conjugate(...)\n |      a.conjugate()\n |      \n |      Return the complex conjugate, element-wise.\n |      \n |      Refer to `numpy.conjugate` for full documentation.\n |      \n |      See Also\n |      --------\n |      numpy.conjugate : equivalent function\n |  \n |  copy(...)\n |      a.copy(order='C')\n |      \n |      Return a copy of the array.\n |      \n |      Parameters\n |      ----------\n |      order : {'C', 'F', 'A', 'K'}, optional\n |          Controls the memory layout of the copy. 'C' means C-order,\n |          'F' means F-order, 'A' means 'F' if `a` is Fortran contiguous,\n |          'C' otherwise. 'K' means match the layout of `a` as closely\n |          as possible. (Note that this function and :func:`numpy.copy` are very\n |          similar, but have different default values for their order=\n |          arguments.)\n |      \n |      See also\n |      --------\n |      numpy.copy\n |      numpy.copyto\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; x = np.array([[1,2,3],[4,5,6]], order='F')\n |      \n |      &gt;&gt;&gt; y = x.copy()\n |      \n |      &gt;&gt;&gt; x.fill(0)\n |      \n |      &gt;&gt;&gt; x\n |      array([[0, 0, 0],\n |             [0, 0, 0]])\n |      \n |      &gt;&gt;&gt; y\n |      array([[1, 2, 3],\n |             [4, 5, 6]])\n |      \n |      &gt;&gt;&gt; y.flags['C_CONTIGUOUS']\n |      True\n |  \n |  cumprod(...)\n |      a.cumprod(axis=None, dtype=None, out=None)\n |      \n |      Return the cumulative product of the elements along the given axis.\n |      \n |      Refer to `numpy.cumprod` for full documentation.\n |      \n |      See Also\n |      --------\n |      numpy.cumprod : equivalent function\n |  \n |  cumsum(...)\n |      a.cumsum(axis=None, dtype=None, out=None)\n |      \n |      Return the cumulative sum of the elements along the given axis.\n |      \n |      Refer to `numpy.cumsum` for full documentation.\n |      \n |      See Also\n |      --------\n |      numpy.cumsum : equivalent function\n |  \n |  diagonal(...)\n |      a.diagonal(offset=0, axis1=0, axis2=1)\n |      \n |      Return specified diagonals. In NumPy 1.9 the returned array is a\n |      read-only view instead of a copy as in previous NumPy versions.  In\n |      a future version the read-only restriction will be removed.\n |      \n |      Refer to :func:`numpy.diagonal` for full documentation.\n |      \n |      See Also\n |      --------\n |      numpy.diagonal : equivalent function\n |  \n |  dot(...)\n |      a.dot(b, out=None)\n |      \n |      Dot product of two arrays.\n |      \n |      Refer to `numpy.dot` for full documentation.\n |      \n |      See Also\n |      --------\n |      numpy.dot : equivalent function\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; a = np.eye(2)\n |      &gt;&gt;&gt; b = np.ones((2, 2)) * 2\n |      &gt;&gt;&gt; a.dot(b)\n |      array([[2.,  2.],\n |             [2.,  2.]])\n |      \n |      This array method can be conveniently chained:\n |      \n |      &gt;&gt;&gt; a.dot(b).dot(b)\n |      array([[8.,  8.],\n |             [8.,  8.]])\n |  \n |  dump(...)\n |      a.dump(file)\n |      \n |      Dump a pickle of the array to the specified file.\n |      The array can be read back with pickle.load or numpy.load.\n |      \n |      Parameters\n |      ----------\n |      file : str or Path\n |          A string naming the dump file.\n |      \n |          .. versionchanged:: 1.17.0\n |              `pathlib.Path` objects are now accepted.\n |  \n |  dumps(...)\n |      a.dumps()\n |      \n |      Returns the pickle of the array as a string.\n |      pickle.loads or numpy.loads will convert the string back to an array.\n |      \n |      Parameters\n |      ----------\n |      None\n |  \n |  fill(...)\n |      a.fill(value)\n |      \n |      Fill the array with a scalar value.\n |      \n |      Parameters\n |      ----------\n |      value : scalar\n |          All elements of `a` will be assigned this value.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; a = np.array([1, 2])\n |      &gt;&gt;&gt; a.fill(0)\n |      &gt;&gt;&gt; a\n |      array([0, 0])\n |      &gt;&gt;&gt; a = np.empty(2)\n |      &gt;&gt;&gt; a.fill(1)\n |      &gt;&gt;&gt; a\n |      array([1.,  1.])\n |  \n |  getfield(...)\n |      a.getfield(dtype, offset=0)\n |      \n |      Returns a field of the given array as a certain type.\n |      \n |      A field is a view of the array data with a given data-type. The values in\n |      the view are determined by the given type and the offset into the current\n |      array in bytes. The offset needs to be such that the view dtype fits in the\n |      array dtype; for example an array of dtype complex128 has 16-byte elements.\n |      If taking a view with a 32-bit integer (4 bytes), the offset needs to be\n |      between 0 and 12 bytes.\n |      \n |      Parameters\n |      ----------\n |      dtype : str or dtype\n |          The data type of the view. The dtype size of the view can not be larger\n |          than that of the array itself.\n |      offset : int\n |          Number of bytes to skip before beginning the element view.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; x = np.diag([1.+1.j]*2)\n |      &gt;&gt;&gt; x[1, 1] = 2 + 4.j\n |      &gt;&gt;&gt; x\n |      array([[1.+1.j,  0.+0.j],\n |             [0.+0.j,  2.+4.j]])\n |      &gt;&gt;&gt; x.getfield(np.float64)\n |      array([[1.,  0.],\n |             [0.,  2.]])\n |      \n |      By choosing an offset of 8 bytes we can select the complex part of the\n |      array for our view:\n |      \n |      &gt;&gt;&gt; x.getfield(np.float64, offset=8)\n |      array([[1.,  0.],\n |             [0.,  4.]])\n |  \n |  item(...)\n |      a.item(*args)\n |      \n |      Copy an element of an array to a standard Python scalar and return it.\n |      \n |      Parameters\n |      ----------\n |      \\*args : Arguments (variable number and type)\n |      \n |          * none: in this case, the method only works for arrays\n |            with one element (`a.size == 1`), which element is\n |            copied into a standard Python scalar object and returned.\n |      \n |          * int_type: this argument is interpreted as a flat index into\n |            the array, specifying which element to copy and return.\n |      \n |          * tuple of int_types: functions as does a single int_type argument,\n |            except that the argument is interpreted as an nd-index into the\n |            array.\n |      \n |      Returns\n |      -------\n |      z : Standard Python scalar object\n |          A copy of the specified element of the array as a suitable\n |          Python scalar\n |      \n |      Notes\n |      -----\n |      When the data type of `a` is longdouble or clongdouble, item() returns\n |      a scalar array object because there is no available Python scalar that\n |      would not lose information. Void arrays return a buffer object for item(),\n |      unless fields are defined, in which case a tuple is returned.\n |      \n |      `item` is very similar to a[args], except, instead of an array scalar,\n |      a standard Python scalar is returned. This can be useful for speeding up\n |      access to elements of the array and doing arithmetic on elements of the\n |      array using Python's optimized math.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; np.random.seed(123)\n |      &gt;&gt;&gt; x = np.random.randint(9, size=(3, 3))\n |      &gt;&gt;&gt; x\n |      array([[2, 2, 6],\n |             [1, 3, 6],\n |             [1, 0, 1]])\n |      &gt;&gt;&gt; x.item(3)\n |      1\n |      &gt;&gt;&gt; x.item(7)\n |      0\n |      &gt;&gt;&gt; x.item((0, 1))\n |      2\n |      &gt;&gt;&gt; x.item((2, 2))\n |      1\n |  \n |  itemset(...)\n |      a.itemset(*args)\n |      \n |      Insert scalar into an array (scalar is cast to array's dtype, if possible)\n |      \n |      There must be at least 1 argument, and define the last argument\n |      as *item*.  Then, ``a.itemset(*args)`` is equivalent to but faster\n |      than ``a[args] = item``.  The item should be a scalar value and `args`\n |      must select a single item in the array `a`.\n |      \n |      Parameters\n |      ----------\n |      \\*args : Arguments\n |          If one argument: a scalar, only used in case `a` is of size 1.\n |          If two arguments: the last argument is the value to be set\n |          and must be a scalar, the first argument specifies a single array\n |          element location. It is either an int or a tuple.\n |      \n |      Notes\n |      -----\n |      Compared to indexing syntax, `itemset` provides some speed increase\n |      for placing a scalar into a particular location in an `ndarray`,\n |      if you must do this.  However, generally this is discouraged:\n |      among other problems, it complicates the appearance of the code.\n |      Also, when using `itemset` (and `item`) inside a loop, be sure\n |      to assign the methods to a local variable to avoid the attribute\n |      look-up at each loop iteration.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; np.random.seed(123)\n |      &gt;&gt;&gt; x = np.random.randint(9, size=(3, 3))\n |      &gt;&gt;&gt; x\n |      array([[2, 2, 6],\n |             [1, 3, 6],\n |             [1, 0, 1]])\n |      &gt;&gt;&gt; x.itemset(4, 0)\n |      &gt;&gt;&gt; x.itemset((2, 2), 9)\n |      &gt;&gt;&gt; x\n |      array([[2, 2, 6],\n |             [1, 0, 6],\n |             [1, 0, 9]])\n |  \n |  newbyteorder(...)\n |      arr.newbyteorder(new_order='S')\n |      \n |      Return the array with the same data viewed with a different byte order.\n |      \n |      Equivalent to::\n |      \n |          arr.view(arr.dtype.newbytorder(new_order))\n |      \n |      Changes are also made in all fields and sub-arrays of the array data\n |      type.\n |      \n |      \n |      \n |      Parameters\n |      ----------\n |      new_order : string, optional\n |          Byte order to force; a value from the byte order specifications\n |          below. `new_order` codes can be any of:\n |      \n |          * 'S' - swap dtype from current to opposite endian\n |          * {'&lt;', 'L'} - little endian\n |          * {'&gt;', 'B'} - big endian\n |          * {'=', 'N'} - native order\n |          * {'|', 'I'} - ignore (no change to byte order)\n |      \n |          The default value ('S') results in swapping the current\n |          byte order. The code does a case-insensitive check on the first\n |          letter of `new_order` for the alternatives above.  For example,\n |          any of 'B' or 'b' or 'biggish' are valid to specify big-endian.\n |      \n |      \n |      Returns\n |      -------\n |      new_arr : array\n |          New array object with the dtype reflecting given change to the\n |          byte order.\n |  \n |  nonzero(...)\n |      a.nonzero()\n |      \n |      Return the indices of the elements that are non-zero.\n |      \n |      Refer to `numpy.nonzero` for full documentation.\n |      \n |      See Also\n |      --------\n |      numpy.nonzero : equivalent function\n |  \n |  partition(...)\n |      a.partition(kth, axis=-1, kind='introselect', order=None)\n |      \n |      Rearranges the elements in the array in such a way that the value of the\n |      element in kth position is in the position it would be in a sorted array.\n |      All elements smaller than the kth element are moved before this element and\n |      all equal or greater are moved behind it. The ordering of the elements in\n |      the two partitions is undefined.\n |      \n |      .. versionadded:: 1.8.0\n |      \n |      Parameters\n |      ----------\n |      kth : int or sequence of ints\n |          Element index to partition by. The kth element value will be in its\n |          final sorted position and all smaller elements will be moved before it\n |          and all equal or greater elements behind it.\n |          The order of all elements in the partitions is undefined.\n |          If provided with a sequence of kth it will partition all elements\n |          indexed by kth of them into their sorted position at once.\n |      axis : int, optional\n |          Axis along which to sort. Default is -1, which means sort along the\n |          last axis.\n |      kind : {'introselect'}, optional\n |          Selection algorithm. Default is 'introselect'.\n |      order : str or list of str, optional\n |          When `a` is an array with fields defined, this argument specifies\n |          which fields to compare first, second, etc. A single field can\n |          be specified as a string, and not all fields need to be specified,\n |          but unspecified fields will still be used, in the order in which\n |          they come up in the dtype, to break ties.\n |      \n |      See Also\n |      --------\n |      numpy.partition : Return a parititioned copy of an array.\n |      argpartition : Indirect partition.\n |      sort : Full sort.\n |      \n |      Notes\n |      -----\n |      See ``np.partition`` for notes on the different algorithms.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; a = np.array([3, 4, 2, 1])\n |      &gt;&gt;&gt; a.partition(3)\n |      &gt;&gt;&gt; a\n |      array([2, 1, 3, 4])\n |      \n |      &gt;&gt;&gt; a.partition((1, 3))\n |      &gt;&gt;&gt; a\n |      array([1, 2, 3, 4])\n |  \n |  put(...)\n |      a.put(indices, values, mode='raise')\n |      \n |      Set ``a.flat[n] = values[n]`` for all `n` in indices.\n |      \n |      Refer to `numpy.put` for full documentation.\n |      \n |      See Also\n |      --------\n |      numpy.put : equivalent function\n |  \n |  repeat(...)\n |      a.repeat(repeats, axis=None)\n |      \n |      Repeat elements of an array.\n |      \n |      Refer to `numpy.repeat` for full documentation.\n |      \n |      See Also\n |      --------\n |      numpy.repeat : equivalent function\n |  \n |  reshape(...)\n |      a.reshape(shape, order='C')\n |      \n |      Returns an array containing the same data with a new shape.\n |      \n |      Refer to `numpy.reshape` for full documentation.\n |      \n |      See Also\n |      --------\n |      numpy.reshape : equivalent function\n |      \n |      Notes\n |      -----\n |      Unlike the free function `numpy.reshape`, this method on `ndarray` allows\n |      the elements of the shape parameter to be passed in as separate arguments.\n |      For example, ``a.reshape(10, 11)`` is equivalent to\n |      ``a.reshape((10, 11))``.\n |  \n |  resize(...)\n |      a.resize(new_shape, refcheck=True)\n |      \n |      Change shape and size of array in-place.\n |      \n |      Parameters\n |      ----------\n |      new_shape : tuple of ints, or `n` ints\n |          Shape of resized array.\n |      refcheck : bool, optional\n |          If False, reference count will not be checked. Default is True.\n |      \n |      Returns\n |      -------\n |      None\n |      \n |      Raises\n |      ------\n |      ValueError\n |          If `a` does not own its own data or references or views to it exist,\n |          and the data memory must be changed.\n |          PyPy only: will always raise if the data memory must be changed, since\n |          there is no reliable way to determine if references or views to it\n |          exist.\n |      \n |      SystemError\n |          If the `order` keyword argument is specified. This behaviour is a\n |          bug in NumPy.\n |      \n |      See Also\n |      --------\n |      resize : Return a new array with the specified shape.\n |      \n |      Notes\n |      -----\n |      This reallocates space for the data area if necessary.\n |      \n |      Only contiguous arrays (data elements consecutive in memory) can be\n |      resized.\n |      \n |      The purpose of the reference count check is to make sure you\n |      do not use this array as a buffer for another Python object and then\n |      reallocate the memory. However, reference counts can increase in\n |      other ways so if you are sure that you have not shared the memory\n |      for this array with another Python object, then you may safely set\n |      `refcheck` to False.\n |      \n |      Examples\n |      --------\n |      Shrinking an array: array is flattened (in the order that the data are\n |      stored in memory), resized, and reshaped:\n |      \n |      &gt;&gt;&gt; a = np.array([[0, 1], [2, 3]], order='C')\n |      &gt;&gt;&gt; a.resize((2, 1))\n |      &gt;&gt;&gt; a\n |      array([[0],\n |             [1]])\n |      \n |      &gt;&gt;&gt; a = np.array([[0, 1], [2, 3]], order='F')\n |      &gt;&gt;&gt; a.resize((2, 1))\n |      &gt;&gt;&gt; a\n |      array([[0],\n |             [2]])\n |      \n |      Enlarging an array: as above, but missing entries are filled with zeros:\n |      \n |      &gt;&gt;&gt; b = np.array([[0, 1], [2, 3]])\n |      &gt;&gt;&gt; b.resize(2, 3) # new_shape parameter doesn't have to be a tuple\n |      &gt;&gt;&gt; b\n |      array([[0, 1, 2],\n |             [3, 0, 0]])\n |      \n |      Referencing an array prevents resizing...\n |      \n |      &gt;&gt;&gt; c = a\n |      &gt;&gt;&gt; a.resize((1, 1))\n |      Traceback (most recent call last):\n |      ...\n |      ValueError: cannot resize an array that references or is referenced ...\n |      \n |      Unless `refcheck` is False:\n |      \n |      &gt;&gt;&gt; a.resize((1, 1), refcheck=False)\n |      &gt;&gt;&gt; a\n |      array([[0]])\n |      &gt;&gt;&gt; c\n |      array([[0]])\n |  \n |  round(...)\n |      a.round(decimals=0, out=None)\n |      \n |      Return `a` with each element rounded to the given number of decimals.\n |      \n |      Refer to `numpy.around` for full documentation.\n |      \n |      See Also\n |      --------\n |      numpy.around : equivalent function\n |  \n |  searchsorted(...)\n |      a.searchsorted(v, side='left', sorter=None)\n |      \n |      Find indices where elements of v should be inserted in a to maintain order.\n |      \n |      For full documentation, see `numpy.searchsorted`\n |      \n |      See Also\n |      --------\n |      numpy.searchsorted : equivalent function\n |  \n |  setfield(...)\n |      a.setfield(val, dtype, offset=0)\n |      \n |      Put a value into a specified place in a field defined by a data-type.\n |      \n |      Place `val` into `a`'s field defined by `dtype` and beginning `offset`\n |      bytes into the field.\n |      \n |      Parameters\n |      ----------\n |      val : object\n |          Value to be placed in field.\n |      dtype : dtype object\n |          Data-type of the field in which to place `val`.\n |      offset : int, optional\n |          The number of bytes into the field at which to place `val`.\n |      \n |      Returns\n |      -------\n |      None\n |      \n |      See Also\n |      --------\n |      getfield\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; x = np.eye(3)\n |      &gt;&gt;&gt; x.getfield(np.float64)\n |      array([[1.,  0.,  0.],\n |             [0.,  1.,  0.],\n |             [0.,  0.,  1.]])\n |      &gt;&gt;&gt; x.setfield(3, np.int32)\n |      &gt;&gt;&gt; x.getfield(np.int32)\n |      array([[3, 3, 3],\n |             [3, 3, 3],\n |             [3, 3, 3]], dtype=int32)\n |      &gt;&gt;&gt; x\n |      array([[1.0e+000, 1.5e-323, 1.5e-323],\n |             [1.5e-323, 1.0e+000, 1.5e-323],\n |             [1.5e-323, 1.5e-323, 1.0e+000]])\n |      &gt;&gt;&gt; x.setfield(np.eye(3), np.int32)\n |      &gt;&gt;&gt; x\n |      array([[1.,  0.,  0.],\n |             [0.,  1.,  0.],\n |             [0.,  0.,  1.]])\n |  \n |  setflags(...)\n |      a.setflags(write=None, align=None, uic=None)\n |      \n |      Set array flags WRITEABLE, ALIGNED, (WRITEBACKIFCOPY and UPDATEIFCOPY),\n |      respectively.\n |      \n |      These Boolean-valued flags affect how numpy interprets the memory\n |      area used by `a` (see Notes below). The ALIGNED flag can only\n |      be set to True if the data is actually aligned according to the type.\n |      The WRITEBACKIFCOPY and (deprecated) UPDATEIFCOPY flags can never be set\n |      to True. The flag WRITEABLE can only be set to True if the array owns its\n |      own memory, or the ultimate owner of the memory exposes a writeable buffer\n |      interface, or is a string. (The exception for string is made so that\n |      unpickling can be done without copying memory.)\n |      \n |      Parameters\n |      ----------\n |      write : bool, optional\n |          Describes whether or not `a` can be written to.\n |      align : bool, optional\n |          Describes whether or not `a` is aligned properly for its type.\n |      uic : bool, optional\n |          Describes whether or not `a` is a copy of another \"base\" array.\n |      \n |      Notes\n |      -----\n |      Array flags provide information about how the memory area used\n |      for the array is to be interpreted. There are 7 Boolean flags\n |      in use, only four of which can be changed by the user:\n |      WRITEBACKIFCOPY, UPDATEIFCOPY, WRITEABLE, and ALIGNED.\n |      \n |      WRITEABLE (W) the data area can be written to;\n |      \n |      ALIGNED (A) the data and strides are aligned appropriately for the hardware\n |      (as determined by the compiler);\n |      \n |      UPDATEIFCOPY (U) (deprecated), replaced by WRITEBACKIFCOPY;\n |      \n |      WRITEBACKIFCOPY (X) this array is a copy of some other array (referenced\n |      by .base). When the C-API function PyArray_ResolveWritebackIfCopy is\n |      called, the base array will be updated with the contents of this array.\n |      \n |      All flags can be accessed using the single (upper case) letter as well\n |      as the full name.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; y = np.array([[3, 1, 7],\n |      ...               [2, 0, 0],\n |      ...               [8, 5, 9]])\n |      &gt;&gt;&gt; y\n |      array([[3, 1, 7],\n |             [2, 0, 0],\n |             [8, 5, 9]])\n |      &gt;&gt;&gt; y.flags\n |        C_CONTIGUOUS : True\n |        F_CONTIGUOUS : False\n |        OWNDATA : True\n |        WRITEABLE : True\n |        ALIGNED : True\n |        WRITEBACKIFCOPY : False\n |        UPDATEIFCOPY : False\n |      &gt;&gt;&gt; y.setflags(write=0, align=0)\n |      &gt;&gt;&gt; y.flags\n |        C_CONTIGUOUS : True\n |        F_CONTIGUOUS : False\n |        OWNDATA : True\n |        WRITEABLE : False\n |        ALIGNED : False\n |        WRITEBACKIFCOPY : False\n |        UPDATEIFCOPY : False\n |      &gt;&gt;&gt; y.setflags(uic=1)\n |      Traceback (most recent call last):\n |        File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\n |      ValueError: cannot set WRITEBACKIFCOPY flag to True\n |  \n |  sort(...)\n |      a.sort(axis=-1, kind=None, order=None)\n |      \n |      Sort an array in-place. Refer to `numpy.sort` for full documentation.\n |      \n |      Parameters\n |      ----------\n |      axis : int, optional\n |          Axis along which to sort. Default is -1, which means sort along the\n |          last axis.\n |      kind : {'quicksort', 'mergesort', 'heapsort', 'stable'}, optional\n |          Sorting algorithm. The default is 'quicksort'. Note that both 'stable'\n |          and 'mergesort' use timsort under the covers and, in general, the\n |          actual implementation will vary with datatype. The 'mergesort' option\n |          is retained for backwards compatibility.\n |      \n |          .. versionchanged:: 1.15.0.\n |             The 'stable' option was added.\n |      \n |      order : str or list of str, optional\n |          When `a` is an array with fields defined, this argument specifies\n |          which fields to compare first, second, etc.  A single field can\n |          be specified as a string, and not all fields need be specified,\n |          but unspecified fields will still be used, in the order in which\n |          they come up in the dtype, to break ties.\n |      \n |      See Also\n |      --------\n |      numpy.sort : Return a sorted copy of an array.\n |      numpy.argsort : Indirect sort.\n |      numpy.lexsort : Indirect stable sort on multiple keys.\n |      numpy.searchsorted : Find elements in sorted array.\n |      numpy.partition: Partial sort.\n |      \n |      Notes\n |      -----\n |      See `numpy.sort` for notes on the different sorting algorithms.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; a = np.array([[1,4], [3,1]])\n |      &gt;&gt;&gt; a.sort(axis=1)\n |      &gt;&gt;&gt; a\n |      array([[1, 4],\n |             [1, 3]])\n |      &gt;&gt;&gt; a.sort(axis=0)\n |      &gt;&gt;&gt; a\n |      array([[1, 3],\n |             [1, 4]])\n |      \n |      Use the `order` keyword to specify a field to use when sorting a\n |      structured array:\n |      \n |      &gt;&gt;&gt; a = np.array([('a', 2), ('c', 1)], dtype=[('x', 'S1'), ('y', int)])\n |      &gt;&gt;&gt; a.sort(order='y')\n |      &gt;&gt;&gt; a\n |      array([(b'c', 1), (b'a', 2)],\n |            dtype=[('x', 'S1'), ('y', '&lt;i8')])\n |  \n |  swapaxes(...)\n |      a.swapaxes(axis1, axis2)\n |      \n |      Return a view of the array with `axis1` and `axis2` interchanged.\n |      \n |      Refer to `numpy.swapaxes` for full documentation.\n |      \n |      See Also\n |      --------\n |      numpy.swapaxes : equivalent function\n |  \n |  take(...)\n |      a.take(indices, axis=None, out=None, mode='raise')\n |      \n |      Return an array formed from the elements of `a` at the given indices.\n |      \n |      Refer to `numpy.take` for full documentation.\n |      \n |      See Also\n |      --------\n |      numpy.take : equivalent function\n |  \n |  tobytes(...)\n |      a.tobytes(order='C')\n |      \n |      Construct Python bytes containing the raw data bytes in the array.\n |      \n |      Constructs Python bytes showing a copy of the raw contents of\n |      data memory. The bytes object can be produced in either 'C' or 'Fortran',\n |      or 'Any' order (the default is 'C'-order). 'Any' order means C-order\n |      unless the F_CONTIGUOUS flag in the array is set, in which case it\n |      means 'Fortran' order.\n |      \n |      .. versionadded:: 1.9.0\n |      \n |      Parameters\n |      ----------\n |      order : {'C', 'F', None}, optional\n |          Order of the data for multidimensional arrays:\n |          C, Fortran, or the same as for the original array.\n |      \n |      Returns\n |      -------\n |      s : bytes\n |          Python bytes exhibiting a copy of `a`'s raw data.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; x = np.array([[0, 1], [2, 3]], dtype='&lt;u2')\n |      &gt;&gt;&gt; x.tobytes()\n |      b'\\x00\\x00\\x01\\x00\\x02\\x00\\x03\\x00'\n |      &gt;&gt;&gt; x.tobytes('C') == x.tobytes()\n |      True\n |      &gt;&gt;&gt; x.tobytes('F')\n |      b'\\x00\\x00\\x02\\x00\\x01\\x00\\x03\\x00'\n |  \n |  tofile(...)\n |      a.tofile(fid, sep=\"\", format=\"%s\")\n |      \n |      Write array to a file as text or binary (default).\n |      \n |      Data is always written in 'C' order, independent of the order of `a`.\n |      The data produced by this method can be recovered using the function\n |      fromfile().\n |      \n |      Parameters\n |      ----------\n |      fid : file or str or Path\n |          An open file object, or a string containing a filename.\n |      \n |          .. versionchanged:: 1.17.0\n |              `pathlib.Path` objects are now accepted.\n |      \n |      sep : str\n |          Separator between array items for text output.\n |          If \"\" (empty), a binary file is written, equivalent to\n |          ``file.write(a.tobytes())``.\n |      format : str\n |          Format string for text file output.\n |          Each entry in the array is formatted to text by first converting\n |          it to the closest Python type, and then using \"format\" % item.\n |      \n |      Notes\n |      -----\n |      This is a convenience function for quick storage of array data.\n |      Information on endianness and precision is lost, so this method is not a\n |      good choice for files intended to archive data or transport data between\n |      machines with different endianness. Some of these problems can be overcome\n |      by outputting the data as text files, at the expense of speed and file\n |      size.\n |      \n |      When fid is a file object, array contents are directly written to the\n |      file, bypassing the file object's ``write`` method. As a result, tofile\n |      cannot be used with files objects supporting compression (e.g., GzipFile)\n |      or file-like objects that do not support ``fileno()`` (e.g., BytesIO).\n |  \n |  tostring(...)\n |      a.tostring(order='C')\n |      \n |      Construct Python bytes containing the raw data bytes in the array.\n |      \n |      Constructs Python bytes showing a copy of the raw contents of\n |      data memory. The bytes object can be produced in either 'C' or 'Fortran',\n |      or 'Any' order (the default is 'C'-order). 'Any' order means C-order\n |      unless the F_CONTIGUOUS flag in the array is set, in which case it\n |      means 'Fortran' order.\n |      \n |      This function is a compatibility alias for tobytes. Despite its name it returns bytes not strings.\n |      \n |      Parameters\n |      ----------\n |      order : {'C', 'F', None}, optional\n |          Order of the data for multidimensional arrays:\n |          C, Fortran, or the same as for the original array.\n |      \n |      Returns\n |      -------\n |      s : bytes\n |          Python bytes exhibiting a copy of `a`'s raw data.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; x = np.array([[0, 1], [2, 3]], dtype='&lt;u2')\n |      &gt;&gt;&gt; x.tobytes()\n |      b'\\x00\\x00\\x01\\x00\\x02\\x00\\x03\\x00'\n |      &gt;&gt;&gt; x.tobytes('C') == x.tobytes()\n |      True\n |      &gt;&gt;&gt; x.tobytes('F')\n |      b'\\x00\\x00\\x02\\x00\\x01\\x00\\x03\\x00'\n |  \n |  trace(...)\n |      a.trace(offset=0, axis1=0, axis2=1, dtype=None, out=None)\n |      \n |      Return the sum along diagonals of the array.\n |      \n |      Refer to `numpy.trace` for full documentation.\n |      \n |      See Also\n |      --------\n |      numpy.trace : equivalent function\n |  \n |  transpose(...)\n |      a.transpose(*axes)\n |      \n |      Returns a view of the array with axes transposed.\n |      \n |      For a 1-D array this has no effect, as a transposed vector is simply the\n |      same vector. To convert a 1-D array into a 2D column vector, an additional\n |      dimension must be added. `np.atleast2d(a).T` achieves this, as does\n |      `a[:, np.newaxis]`.\n |      For a 2-D array, this is a standard matrix transpose.\n |      For an n-D array, if axes are given, their order indicates how the\n |      axes are permuted (see Examples). If axes are not provided and\n |      ``a.shape = (i[0], i[1], ... i[n-2], i[n-1])``, then\n |      ``a.transpose().shape = (i[n-1], i[n-2], ... i[1], i[0])``.\n |      \n |      Parameters\n |      ----------\n |      axes : None, tuple of ints, or `n` ints\n |      \n |       * None or no argument: reverses the order of the axes.\n |      \n |       * tuple of ints: `i` in the `j`-th place in the tuple means `a`'s\n |         `i`-th axis becomes `a.transpose()`'s `j`-th axis.\n |      \n |       * `n` ints: same as an n-tuple of the same ints (this form is\n |         intended simply as a \"convenience\" alternative to the tuple form)\n |      \n |      Returns\n |      -------\n |      out : ndarray\n |          View of `a`, with axes suitably permuted.\n |      \n |      See Also\n |      --------\n |      ndarray.T : Array property returning the array transposed.\n |      ndarray.reshape : Give a new shape to an array without changing its data.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; a = np.array([[1, 2], [3, 4]])\n |      &gt;&gt;&gt; a\n |      array([[1, 2],\n |             [3, 4]])\n |      &gt;&gt;&gt; a.transpose()\n |      array([[1, 3],\n |             [2, 4]])\n |      &gt;&gt;&gt; a.transpose((1, 0))\n |      array([[1, 3],\n |             [2, 4]])\n |      &gt;&gt;&gt; a.transpose(1, 0)\n |      array([[1, 3],\n |             [2, 4]])\n |  \n |  view(...)\n |      a.view(dtype=None, type=None)\n |      \n |      New view of array with the same data.\n |      \n |      Parameters\n |      ----------\n |      dtype : data-type or ndarray sub-class, optional\n |          Data-type descriptor of the returned view, e.g., float32 or int16. The\n |          default, None, results in the view having the same data-type as `a`.\n |          This argument can also be specified as an ndarray sub-class, which\n |          then specifies the type of the returned object (this is equivalent to\n |          setting the ``type`` parameter).\n |      type : Python type, optional\n |          Type of the returned view, e.g., ndarray or matrix.  Again, the\n |          default None results in type preservation.\n |      \n |      Notes\n |      -----\n |      ``a.view()`` is used two different ways:\n |      \n |      ``a.view(some_dtype)`` or ``a.view(dtype=some_dtype)`` constructs a view\n |      of the array's memory with a different data-type.  This can cause a\n |      reinterpretation of the bytes of memory.\n |      \n |      ``a.view(ndarray_subclass)`` or ``a.view(type=ndarray_subclass)`` just\n |      returns an instance of `ndarray_subclass` that looks at the same array\n |      (same shape, dtype, etc.)  This does not cause a reinterpretation of the\n |      memory.\n |      \n |      For ``a.view(some_dtype)``, if ``some_dtype`` has a different number of\n |      bytes per entry than the previous dtype (for example, converting a\n |      regular array to a structured array), then the behavior of the view\n |      cannot be predicted just from the superficial appearance of ``a`` (shown\n |      by ``print(a)``). It also depends on exactly how ``a`` is stored in\n |      memory. Therefore if ``a`` is C-ordered versus fortran-ordered, versus\n |      defined as a slice or transpose, etc., the view may give different\n |      results.\n |      \n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; x = np.array([(1, 2)], dtype=[('a', np.int8), ('b', np.int8)])\n |      \n |      Viewing array data using a different type and dtype:\n |      \n |      &gt;&gt;&gt; y = x.view(dtype=np.int16, type=np.matrix)\n |      &gt;&gt;&gt; y\n |      matrix([[513]], dtype=int16)\n |      &gt;&gt;&gt; print(type(y))\n |      &lt;class 'numpy.matrix'&gt;\n |      \n |      Creating a view on a structured array so it can be used in calculations\n |      \n |      &gt;&gt;&gt; x = np.array([(1, 2),(3,4)], dtype=[('a', np.int8), ('b', np.int8)])\n |      &gt;&gt;&gt; xv = x.view(dtype=np.int8).reshape(-1,2)\n |      &gt;&gt;&gt; xv\n |      array([[1, 2],\n |             [3, 4]], dtype=int8)\n |      &gt;&gt;&gt; xv.mean(0)\n |      array([2.,  3.])\n |      \n |      Making changes to the view changes the underlying array\n |      \n |      &gt;&gt;&gt; xv[0,1] = 20\n |      &gt;&gt;&gt; x\n |      array([(1, 20), (3,  4)], dtype=[('a', 'i1'), ('b', 'i1')])\n |      \n |      Using a view to convert an array to a recarray:\n |      \n |      &gt;&gt;&gt; z = x.view(np.recarray)\n |      &gt;&gt;&gt; z.a\n |      array([1, 3], dtype=int8)\n |      \n |      Views share data:\n |      \n |      &gt;&gt;&gt; x[0] = (9, 10)\n |      &gt;&gt;&gt; z[0]\n |      (9, 10)\n |      \n |      Views that change the dtype size (bytes per entry) should normally be\n |      avoided on arrays defined by slices, transposes, fortran-ordering, etc.:\n |      \n |      &gt;&gt;&gt; x = np.array([[1,2,3],[4,5,6]], dtype=np.int16)\n |      &gt;&gt;&gt; y = x[:, 0:2]\n |      &gt;&gt;&gt; y\n |      array([[1, 2],\n |             [4, 5]], dtype=int16)\n |      &gt;&gt;&gt; y.view(dtype=[('width', np.int16), ('length', np.int16)])\n |      Traceback (most recent call last):\n |          ...\n |      ValueError: To change to a dtype of a different size, the array must be C-contiguous\n |      &gt;&gt;&gt; z = y.copy()\n |      &gt;&gt;&gt; z.view(dtype=[('width', np.int16), ('length', np.int16)])\n |      array([[(1, 2)],\n |             [(4, 5)]], dtype=[('width', '&lt;i2'), ('length', '&lt;i2')])\n |  \n |  ----------------------------------------------------------------------\n |  Data descriptors inherited from ndarray:\n |  \n |  __array_interface__\n |      Array protocol: Python side.\n |  \n |  __array_struct__\n |      Array protocol: C-struct side.\n |  \n |  base\n |      Base object if memory is from some other object.\n |      \n |      Examples\n |      --------\n |      The base of an array that owns its memory is None:\n |      \n |      &gt;&gt;&gt; x = np.array([1,2,3,4])\n |      &gt;&gt;&gt; x.base is None\n |      True\n |      \n |      Slicing creates a view, whose memory is shared with x:\n |      \n |      &gt;&gt;&gt; y = x[2:]\n |      &gt;&gt;&gt; y.base is x\n |      True\n |  \n |  ctypes\n |      An object to simplify the interaction of the array with the ctypes\n |      module.\n |      \n |      This attribute creates an object that makes it easier to use arrays\n |      when calling shared libraries with the ctypes module. The returned\n |      object has, among others, data, shape, and strides attributes (see\n |      Notes below) which themselves return ctypes objects that can be used\n |      as arguments to a shared library.\n |      \n |      Parameters\n |      ----------\n |      None\n |      \n |      Returns\n |      -------\n |      c : Python object\n |          Possessing attributes data, shape, strides, etc.\n |      \n |      See Also\n |      --------\n |      numpy.ctypeslib\n |      \n |      Notes\n |      -----\n |      Below are the public attributes of this object which were documented\n |      in \"Guide to NumPy\" (we have omitted undocumented public attributes,\n |      as well as documented private attributes):\n |      \n |      .. autoattribute:: numpy.core._internal._ctypes.data\n |          :noindex:\n |      \n |      .. autoattribute:: numpy.core._internal._ctypes.shape\n |          :noindex:\n |      \n |      .. autoattribute:: numpy.core._internal._ctypes.strides\n |          :noindex:\n |      \n |      .. automethod:: numpy.core._internal._ctypes.data_as\n |          :noindex:\n |      \n |      .. automethod:: numpy.core._internal._ctypes.shape_as\n |          :noindex:\n |      \n |      .. automethod:: numpy.core._internal._ctypes.strides_as\n |          :noindex:\n |      \n |      If the ctypes module is not available, then the ctypes attribute\n |      of array objects still returns something useful, but ctypes objects\n |      are not returned and errors may be raised instead. In particular,\n |      the object will still have the ``as_parameter`` attribute which will\n |      return an integer equal to the data attribute.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; import ctypes\n |      &gt;&gt;&gt; x\n |      array([[0, 1],\n |             [2, 3]])\n |      &gt;&gt;&gt; x.ctypes.data\n |      30439712\n |      &gt;&gt;&gt; x.ctypes.data_as(ctypes.POINTER(ctypes.c_long))\n |      &lt;ctypes.LP_c_long object at 0x01F01300&gt;\n |      &gt;&gt;&gt; x.ctypes.data_as(ctypes.POINTER(ctypes.c_long)).contents\n |      c_long(0)\n |      &gt;&gt;&gt; x.ctypes.data_as(ctypes.POINTER(ctypes.c_longlong)).contents\n |      c_longlong(4294967296L)\n |      &gt;&gt;&gt; x.ctypes.shape\n |      &lt;numpy.core._internal.c_long_Array_2 object at 0x01FFD580&gt;\n |      &gt;&gt;&gt; x.ctypes.shape_as(ctypes.c_long)\n |      &lt;numpy.core._internal.c_long_Array_2 object at 0x01FCE620&gt;\n |      &gt;&gt;&gt; x.ctypes.strides\n |      &lt;numpy.core._internal.c_long_Array_2 object at 0x01FCE620&gt;\n |      &gt;&gt;&gt; x.ctypes.strides_as(ctypes.c_longlong)\n |      &lt;numpy.core._internal.c_longlong_Array_2 object at 0x01F01300&gt;\n |  \n |  data\n |      Python buffer object pointing to the start of the array's data.\n |  \n |  dtype\n |      Data-type of the array's elements.\n |      \n |      Parameters\n |      ----------\n |      None\n |      \n |      Returns\n |      -------\n |      d : numpy dtype object\n |      \n |      See Also\n |      --------\n |      numpy.dtype\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; x\n |      array([[0, 1],\n |             [2, 3]])\n |      &gt;&gt;&gt; x.dtype\n |      dtype('int32')\n |      &gt;&gt;&gt; type(x.dtype)\n |      &lt;type 'numpy.dtype'&gt;\n |  \n |  flags\n |      Information about the memory layout of the array.\n |      \n |      Attributes\n |      ----------\n |      C_CONTIGUOUS (C)\n |          The data is in a single, C-style contiguous segment.\n |      F_CONTIGUOUS (F)\n |          The data is in a single, Fortran-style contiguous segment.\n |      OWNDATA (O)\n |          The array owns the memory it uses or borrows it from another object.\n |      WRITEABLE (W)\n |          The data area can be written to.  Setting this to False locks\n |          the data, making it read-only.  A view (slice, etc.) inherits WRITEABLE\n |          from its base array at creation time, but a view of a writeable\n |          array may be subsequently locked while the base array remains writeable.\n |          (The opposite is not true, in that a view of a locked array may not\n |          be made writeable.  However, currently, locking a base object does not\n |          lock any views that already reference it, so under that circumstance it\n |          is possible to alter the contents of a locked array via a previously\n |          created writeable view onto it.)  Attempting to change a non-writeable\n |          array raises a RuntimeError exception.\n |      ALIGNED (A)\n |          The data and all elements are aligned appropriately for the hardware.\n |      WRITEBACKIFCOPY (X)\n |          This array is a copy of some other array. The C-API function\n |          PyArray_ResolveWritebackIfCopy must be called before deallocating\n |          to the base array will be updated with the contents of this array.\n |      UPDATEIFCOPY (U)\n |          (Deprecated, use WRITEBACKIFCOPY) This array is a copy of some other array.\n |          When this array is\n |          deallocated, the base array will be updated with the contents of\n |          this array.\n |      FNC\n |          F_CONTIGUOUS and not C_CONTIGUOUS.\n |      FORC\n |          F_CONTIGUOUS or C_CONTIGUOUS (one-segment test).\n |      BEHAVED (B)\n |          ALIGNED and WRITEABLE.\n |      CARRAY (CA)\n |          BEHAVED and C_CONTIGUOUS.\n |      FARRAY (FA)\n |          BEHAVED and F_CONTIGUOUS and not C_CONTIGUOUS.\n |      \n |      Notes\n |      -----\n |      The `flags` object can be accessed dictionary-like (as in ``a.flags['WRITEABLE']``),\n |      or by using lowercased attribute names (as in ``a.flags.writeable``). Short flag\n |      names are only supported in dictionary access.\n |      \n |      Only the WRITEBACKIFCOPY, UPDATEIFCOPY, WRITEABLE, and ALIGNED flags can be\n |      changed by the user, via direct assignment to the attribute or dictionary\n |      entry, or by calling `ndarray.setflags`.\n |      \n |      The array flags cannot be set arbitrarily:\n |      \n |      - UPDATEIFCOPY can only be set ``False``.\n |      - WRITEBACKIFCOPY can only be set ``False``.\n |      - ALIGNED can only be set ``True`` if the data is truly aligned.\n |      - WRITEABLE can only be set ``True`` if the array owns its own memory\n |        or the ultimate owner of the memory exposes a writeable buffer\n |        interface or is a string.\n |      \n |      Arrays can be both C-style and Fortran-style contiguous simultaneously.\n |      This is clear for 1-dimensional arrays, but can also be true for higher\n |      dimensional arrays.\n |      \n |      Even for contiguous arrays a stride for a given dimension\n |      ``arr.strides[dim]`` may be *arbitrary* if ``arr.shape[dim] == 1``\n |      or the array has no elements.\n |      It does *not* generally hold that ``self.strides[-1] == self.itemsize``\n |      for C-style contiguous arrays or ``self.strides[0] == self.itemsize`` for\n |      Fortran-style contiguous arrays is true.\n |  \n |  flat\n |      A 1-D iterator over the array.\n |      \n |      This is a `numpy.flatiter` instance, which acts similarly to, but is not\n |      a subclass of, Python's built-in iterator object.\n |      \n |      See Also\n |      --------\n |      flatten : Return a copy of the array collapsed into one dimension.\n |      \n |      flatiter\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; x = np.arange(1, 7).reshape(2, 3)\n |      &gt;&gt;&gt; x\n |      array([[1, 2, 3],\n |             [4, 5, 6]])\n |      &gt;&gt;&gt; x.flat[3]\n |      4\n |      &gt;&gt;&gt; x.T\n |      array([[1, 4],\n |             [2, 5],\n |             [3, 6]])\n |      &gt;&gt;&gt; x.T.flat[3]\n |      5\n |      &gt;&gt;&gt; type(x.flat)\n |      &lt;class 'numpy.flatiter'&gt;\n |      \n |      An assignment example:\n |      \n |      &gt;&gt;&gt; x.flat = 3; x\n |      array([[3, 3, 3],\n |             [3, 3, 3]])\n |      &gt;&gt;&gt; x.flat[[1,4]] = 1; x\n |      array([[3, 1, 3],\n |             [3, 1, 3]])\n |  \n |  imag\n |      The imaginary part of the array.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; x = np.sqrt([1+0j, 0+1j])\n |      &gt;&gt;&gt; x.imag\n |      array([ 0.        ,  0.70710678])\n |      &gt;&gt;&gt; x.imag.dtype\n |      dtype('float64')\n |  \n |  itemsize\n |      Length of one array element in bytes.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; x = np.array([1,2,3], dtype=np.float64)\n |      &gt;&gt;&gt; x.itemsize\n |      8\n |      &gt;&gt;&gt; x = np.array([1,2,3], dtype=np.complex128)\n |      &gt;&gt;&gt; x.itemsize\n |      16\n |  \n |  nbytes\n |      Total bytes consumed by the elements of the array.\n |      \n |      Notes\n |      -----\n |      Does not include memory consumed by non-element attributes of the\n |      array object.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; x = np.zeros((3,5,2), dtype=np.complex128)\n |      &gt;&gt;&gt; x.nbytes\n |      480\n |      &gt;&gt;&gt; np.prod(x.shape) * x.itemsize\n |      480\n |  \n |  ndim\n |      Number of array dimensions.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; x = np.array([1, 2, 3])\n |      &gt;&gt;&gt; x.ndim\n |      1\n |      &gt;&gt;&gt; y = np.zeros((2, 3, 4))\n |      &gt;&gt;&gt; y.ndim\n |      3\n |  \n |  real\n |      The real part of the array.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; x = np.sqrt([1+0j, 0+1j])\n |      &gt;&gt;&gt; x.real\n |      array([ 1.        ,  0.70710678])\n |      &gt;&gt;&gt; x.real.dtype\n |      dtype('float64')\n |      \n |      See Also\n |      --------\n |      numpy.real : equivalent function\n |  \n |  shape\n |      Tuple of array dimensions.\n |      \n |      The shape property is usually used to get the current shape of an array,\n |      but may also be used to reshape the array in-place by assigning a tuple of\n |      array dimensions to it.  As with `numpy.reshape`, one of the new shape\n |      dimensions can be -1, in which case its value is inferred from the size of\n |      the array and the remaining dimensions. Reshaping an array in-place will\n |      fail if a copy is required.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; x = np.array([1, 2, 3, 4])\n |      &gt;&gt;&gt; x.shape\n |      (4,)\n |      &gt;&gt;&gt; y = np.zeros((2, 3, 4))\n |      &gt;&gt;&gt; y.shape\n |      (2, 3, 4)\n |      &gt;&gt;&gt; y.shape = (3, 8)\n |      &gt;&gt;&gt; y\n |      array([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n |             [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n |             [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]])\n |      &gt;&gt;&gt; y.shape = (3, 6)\n |      Traceback (most recent call last):\n |        File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\n |      ValueError: total size of new array must be unchanged\n |      &gt;&gt;&gt; np.zeros((4,2))[::2].shape = (-1,)\n |      Traceback (most recent call last):\n |        File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\n |      AttributeError: incompatible shape for a non-contiguous array\n |      \n |      See Also\n |      --------\n |      numpy.reshape : similar function\n |      ndarray.reshape : similar method\n |  \n |  size\n |      Number of elements in the array.\n |      \n |      Equal to ``np.prod(a.shape)``, i.e., the product of the array's\n |      dimensions.\n |      \n |      Notes\n |      -----\n |      `a.size` returns a standard arbitrary precision Python integer. This\n |      may not be the case with other methods of obtaining the same value\n |      (like the suggested ``np.prod(a.shape)``, which returns an instance\n |      of ``np.int_``), and may be relevant if the value is used further in\n |      calculations that may overflow a fixed size integer type.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; x = np.zeros((3, 5, 2), dtype=np.complex128)\n |      &gt;&gt;&gt; x.size\n |      30\n |      &gt;&gt;&gt; np.prod(x.shape)\n |      30\n |  \n |  strides\n |      Tuple of bytes to step in each dimension when traversing an array.\n |      \n |      The byte offset of element ``(i[0], i[1], ..., i[n])`` in an array `a`\n |      is::\n |      \n |          offset = sum(np.array(i) * a.strides)\n |      \n |      A more detailed explanation of strides can be found in the\n |      \"ndarray.rst\" file in the NumPy reference guide.\n |      \n |      Notes\n |      -----\n |      Imagine an array of 32-bit integers (each 4 bytes)::\n |      \n |        x = np.array([[0, 1, 2, 3, 4],\n |                      [5, 6, 7, 8, 9]], dtype=np.int32)\n |      \n |      This array is stored in memory as 40 bytes, one after the other\n |      (known as a contiguous block of memory).  The strides of an array tell\n |      us how many bytes we have to skip in memory to move to the next position\n |      along a certain axis.  For example, we have to skip 4 bytes (1 value) to\n |      move to the next column, but 20 bytes (5 values) to get to the same\n |      position in the next row.  As such, the strides for the array `x` will be\n |      ``(20, 4)``.\n |      \n |      See Also\n |      --------\n |      numpy.lib.stride_tricks.as_strided\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; y = np.reshape(np.arange(2*3*4), (2,3,4))\n |      &gt;&gt;&gt; y\n |      array([[[ 0,  1,  2,  3],\n |              [ 4,  5,  6,  7],\n |              [ 8,  9, 10, 11]],\n |             [[12, 13, 14, 15],\n |              [16, 17, 18, 19],\n |              [20, 21, 22, 23]]])\n |      &gt;&gt;&gt; y.strides\n |      (48, 16, 4)\n |      &gt;&gt;&gt; y[1,1,1]\n |      17\n |      &gt;&gt;&gt; offset=sum(y.strides * np.array((1,1,1)))\n |      &gt;&gt;&gt; offset/y.itemsize\n |      17\n |      \n |      &gt;&gt;&gt; x = np.reshape(np.arange(5*6*7*8), (5,6,7,8)).transpose(2,3,1,0)\n |      &gt;&gt;&gt; x.strides\n |      (32, 4, 224, 1344)\n |      &gt;&gt;&gt; i = np.array([3,5,2,2])\n |      &gt;&gt;&gt; offset = sum(i * x.strides)\n |      &gt;&gt;&gt; x[3,5,2,2]\n |      813\n |      &gt;&gt;&gt; offset / x.itemsize\n |      813\n |  \n |  ----------------------------------------------------------------------\n |  Data and other attributes inherited from ndarray:\n |  \n |  __hash__ = None\n\n\n\n\n\nA.5.3 Matrix computations\n\nThe sub-module numpy.linalg implements basic linear algebra, such as solving linear systems, singular value decomposition, etc.\n\n\nA.5.3.1 Inverse\n\nnp.linalg.det(A)\n\n0.0\n\n\n\n\n\nA.5.4 Data processing and reshaping\nOften it is useful to store datasets in Numpy arrays. Numpy provides a number of functions to calculate the statistics of datasets in arrays.\n\n# column 4\nnp.mean(A[:,3]), np.std(A[:,3])\n\n(23.0, 14.142135623730951)\n\n\n\na = range(10000)\nb = np.arange(10000)\n\n\n%%timeit\nsum(a)\n\n199 µs ± 9.13 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n\n\n\n%%timeit\nnp.sum(b)\n\n7.99 µs ± 202 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n\n\nWhen functions such as min, max, etc. are applied to a multidimensional arrays, it is sometimes useful to apply the calculation to the entire array, and sometimes only on a row or column basis. Using the axis argument we can specify how these functions should behave:\n\nm = np.random.rand(3,3)\nm\n\narray([[0.31751608, 0.41545447, 0.94062331],\n       [0.17379774, 0.57561705, 0.31818086],\n       [0.40848656, 0.62145644, 0.79010869]])\n\n\n\n# global max\nm.max()\n\n0.9406233060883264\n\n\n\n\n\n\nsource: https://scipy-lectures.org/intro/numpy/operations.html\n\n\n# max in each column\nm.max(axis=0)\n\narray([0.40848656, 0.62145644, 0.94062331])\n\n\n\n# max in each row\nm.max(axis=1)\n\narray([0.94062331, 0.57561705, 0.79010869])\n\n\nMany other functions and methods in the array and matrix classes accept the same (optional) axis keyword argument.\nThe shape of a Numpy array can be modified without copying the underlying data, which makes it a fast operation even for large arrays.\n\nA\n\narray([[ 0,  1,  2,  3,  4],\n       [10, 11, 12, 13, 14],\n       [20, 21, 22, 23, 24],\n       [30, 31, 32, 33, 34],\n       [40, 41, 42, 43, 44]])\n\n\n\nn, m = A.shape\n\n\nB = A.reshape((1,n*m))\nB\n\narray([[ 0,  1,  2,  3,  4, 10, 11, 12, 13, 14, 20, 21, 22, 23, 24, 30,\n        31, 32, 33, 34, 40, 41, 42, 43, 44]])\n\n\nWith newaxis, we can insert new dimensions in an array, for example converting a vector to a column or row matrix:\n\nv = np.array([1,2,3])\n\n\nnp.shape(v)\n\n(3,)\n\n\n\n# make a column matrix of the vector v\nv[:, np.newaxis]\n\narray([[1],\n       [2],\n       [3]])\n\n\n\n# column matrix\nv[:,np.newaxis].shape\n\n(3, 1)\n\n\n\nA.5.4.1 Stacking and repeating arrays\nUsing function repeat, tile, vstack, hstack, and concatenate we can create larger vectors and matrices from smaller ones:"
  },
  {
    "objectID": "NumPy_tutorial.html#copy-and-deep-copy",
    "href": "NumPy_tutorial.html#copy-and-deep-copy",
    "title": "Appendix A — Numpy - multidimensional data arrays for python",
    "section": "A.6 Copy and “deep copy”",
    "text": "A.6 Copy and “deep copy”\n\nTo achieve high performance, assignments in Python usually do not copy the underlying objects. This is important for example when objects are passed between functions, to avoid an excessive amount of memory copying when it is not necessary (technical term: pass by reference).\nA slicing operation creates a view on the original array, which is just a way of accessing array data. Thus the original array is not copied in memory.\n\n\nA = np.array([[1, 2], [3, 4]])\nA\n\narray([[1, 2],\n       [3, 4]])\n\n\n\n# now B is referring to the same array data as A \nB = A \n\n\n# changing B affects A\nB[0,0] = 10\nprint(B)\nprint(A)\n\n[[10  2]\n [ 3  4]]\n[[10  2]\n [ 3  4]]\n\n\n\nnp.may_share_memory(A, B)\n\nTrue\n\n\nIf we want to avoid this behavior, so that when we get a new completely independent object B copied from A, then we need to do a so-called “deep copy” using the function copy:\n\nB = np.copy(A)\n\n\n# now, if we modify B, A is not affected\nB[0,0] = -5\nprint(B)\nprint(A)\n\n[[-5  2]\n [ 3  4]]\n[[10  2]\n [ 3  4]]\n\n\n\nA.6.1 Memory layout matters!\n\n\n\n\nsource: https://scipy-lectures.org/advanced/advanced_numpy/index.html\n\n\nx = np.zeros((20000,))\n\n\ny = np.zeros((20000*67,))[::67]\n\n\n%timeit x.sum()\n\n9.98 µs ± 241 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n\n\n\n%timeit y.sum()\n\n59.6 µs ± 3.38 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n\n\n\nx.strides, y.strides\n\n((8,), (536,))"
  },
  {
    "objectID": "NumPy_tutorial.html#iterating-over-array-elements",
    "href": "NumPy_tutorial.html#iterating-over-array-elements",
    "title": "Appendix A — Numpy - multidimensional data arrays for python",
    "section": "A.7 Iterating over array elements",
    "text": "A.7 Iterating over array elements\nGenerally, we want to avoid iterating over the elements of arrays whenever we can (at all costs). The reason is that in an interpreted language like Python (or MATLAB), iterations are really slow compared to vectorized operations.\n\nimport math\n\n\n%%timeit\n# itersative sum\ntotal = 0\nfor item in range(0, 10000):\n    total += item\n\n585 µs ± 24.1 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n\n\n\n%%timeit\n# vectorized sum\nnp.sum(np.arange(10000))\n\n15.2 µs ± 892 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n\n\n\n%%timeit\n# iterative  operation\n[math.exp(item) for item in range(100)]\n\n17.9 µs ± 454 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n\n\n\n%%timeit\n# vectorized operation\nnp.exp(np.arange(100)) \n\n2.86 µs ± 171 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n\n\n\nA.7.1 Create Your Own Vectorizing functions\nTo get good performance we should try to avoid looping over elements in our vectors and matrices, and instead use vectorized algorithms. The first step in converting a scalar algorithm to a vectorized algorithm is to make sure that the functions we write work with vector inputs.\n\ndef Theta(x):\n    \"\"\"\n    Scalar implemenation of the Heaviside step function.\n    \"\"\"\n    if x &gt;= 0:\n        return 1\n    else:\n        return 0\n\nTo get a vectorized version of Theta we can use the Numpy function vectorize. In many cases it can automatically vectorize a function:\n\nTheta_vec = np.vectorize(Theta)\n\n\nTheta_vec(np.array([-3,-2,-1,0,1,2,3]))\n\narray([0, 0, 0, 1, 1, 1, 1])\n\n\n\n\nA.7.2 Type casting\nSince Numpy arrays are statically typed, the type of an array does not change once created. But we can explicitly cast an array of some type to another using the astype functions (see also the similar asarray function). This always creates a new array of a new type:\n\nM.dtype\n\ndtype('float64')\n\n\n\nM2 = M.astype(float)\nM2\n\narray([[0.40570044, 0.66548144, 0.13835937, 0.83043309, 0.12319969],\n       [0.58779155, 0.06309849, 0.49710274, 0.92839462, 0.80603084],\n       [0.19839124, 0.34528354, 0.53473647, 0.97858347, 0.5030445 ],\n       [0.3474475 , 0.21278653, 0.17745402, 0.1040286 , 0.18745545],\n       [0.04031375, 0.23991727, 0.5462427 , 0.20778317, 0.99270398]])\n\n\n\nM2.dtype\n\ndtype('float64')\n\n\n\nM3 = M.astype(bool)\nM3\n\narray([[ True,  True,  True,  True,  True],\n       [ True,  True,  True,  True,  True],\n       [ True,  True,  True,  True,  True],\n       [ True,  True,  True,  True,  True],\n       [ True,  True,  True,  True,  True]])\n\n\n\n\nSee casting at https://scipy-lectures.org/intro/numpy/elaborate_arrays.html"
  },
  {
    "objectID": "NumPy_tutorial.html#file-io",
    "href": "NumPy_tutorial.html#file-io",
    "title": "Appendix A — Numpy - multidimensional data arrays for python",
    "section": "A.8 File I/O",
    "text": "A.8 File I/O\n\nNumPy has its own binary format, not portable but with efficient I/O\nUseful when storing and reading back numpy array data. Use the functions numpy.save and numpy.load\nMatlab: scipy.io.loadmat, scipy.io.savemat\n\n\nnp.save(\"random-matrix.npy\", M)\n\n\nnp.load(\"random-matrix.npy\")\n\narray([[0.40570044, 0.66548144, 0.13835937, 0.83043309, 0.12319969],\n       [0.58779155, 0.06309849, 0.49710274, 0.92839462, 0.80603084],\n       [0.19839124, 0.34528354, 0.53473647, 0.97858347, 0.5030445 ],\n       [0.3474475 , 0.21278653, 0.17745402, 0.1040286 , 0.18745545],\n       [0.04031375, 0.23991727, 0.5462427 , 0.20778317, 0.99270398]])"
  },
  {
    "objectID": "NumPy_tutorial.html#conclusion",
    "href": "NumPy_tutorial.html#conclusion",
    "title": "Appendix A — Numpy - multidimensional data arrays for python",
    "section": "A.9 Conclusion",
    "text": "A.9 Conclusion\nTo make the code faster using NumPy and - Vectorizing for loops: Find tricks to avoid for loops using numpy arrays.\n\nIn place operations: a *= 3 instead of a = 3*a\nUse views instead of copies whenever possible\nMemory arrangement is important. Keep strides small as possible for coalescing memory access\nBroadcasting: Use broadcasting to do operations on arrays as small as possible before combining them.\nUse compiled code (The following session)"
  },
  {
    "objectID": "NumPy_tutorial.html#references",
    "href": "NumPy_tutorial.html#references",
    "title": "Appendix A — Numpy - multidimensional data arrays for python",
    "section": "A.10 References",
    "text": "A.10 References\n\nhttps://scipy-lectures.org/intro/numpy/index.html - A good introduction to pydata stack\nhttps://github.com/jrjohansson/scientific-python-lectures/blob/master/Lecture-2-Numpy.ipynb - A good introduction for NumPy though a bit of outdated\nhttp://cs229.stanford.edu/section/cs229_python_tutorial/Spring_2020_Notebook.html - Another good introduction to NumPy\nhttps://www.pythonlikeyoumeanit.com/Module3_IntroducingNumpy/Broadcasting.html - A great reference for broadcasting and distance calculation\nhttps://eli.thegreenplace.net/2015/memory-layout-of-multi-dimensional-arrays- A great article for memory layout behind NumPy\nhttps://numpy.org/doc/stable/user/numpy-for-matlab-users.html - A Numpy guide for MATLAB users\nhttp://mathesaurus.sourceforge.net/r-numpy.html - A Numpy guide for R users"
  },
  {
    "objectID": "Colab_tutorial.html#setup",
    "href": "Colab_tutorial.html#setup",
    "title": "Appendix B — Introduction to Colab",
    "section": "B.1 Setup",
    "text": "B.1 Setup\nYou can lookup the resources first:\n\nimport multiprocessing\ncores = multiprocessing.cpu_count() # Count the number of cores in a computer\ncores\n\n\nfrom psutil import virtual_memory\nram_gb = virtual_memory().total / 1e9\nprint('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n\n\nimport sys\n# Is this notebook running on Colab or Kaggle?\nIS_COLAB = \"google.colab\" in sys.modules\nIS_KAGGLE = \"kaggle_secrets\" in sys.modules\n\ngpu_info = !nvidia-smi\ngpu_info = '\\n'.join(gpu_info)\nif gpu_info.find('failed') &gt;= 0:\n  if IS_COLAB:\n    print(\"Go to Runtime &gt; Change runtime and select a GPU hardware accelerator.\")\n  if IS_KAGGLE:\n    print(\"Go to Settings &gt; Accelerator and select GPU.\")\nelse:\n  from tensorflow.python.client import device_lib \n  print(device_lib.list_local_devices())\n\n\n!nvidia-smi -L\n\nV100 &gt; P100 &gt; T4 &gt; K80 (but most of the time you get K80 or T4 using the free Colab)"
  },
  {
    "objectID": "Colab_tutorial.html#cells",
    "href": "Colab_tutorial.html#cells",
    "title": "Appendix B — Introduction to Colab",
    "section": "B.2 Cells",
    "text": "B.2 Cells\nA notebook is a list of cells. Cells contain either explanatory text or executable code and its output. Click a cell to select it.\n\nB.2.1 Code cells\nBelow is a code cell. Once the toolbar button indicates CONNECTED, click in the cell to select it and execute the contents in the following ways:\n\nClick the Play icon in the left gutter of the cell;\nType Cmd/Ctrl+Enter to run the cell in place;\nType Shift+Enter to run the cell and move focus to the next cell (adding one if none exists); or\nType Alt+Enter to run the cell and insert a new code cell immediately below it.\n\nThere are additional options for running some or all cells in the Runtime menu.\n\na = 10\na\n\n\n\nB.2.2 Text cells\nThis is a text cell. You can double-click to edit this cell. Text cells use markdown syntax. To learn more, see our markdown guide.\nYou can also add math to text cells using LaTeX to be rendered by MathJax. Just place the statement within a pair of $ signs. For example $\\sqrt{3x-1}+(1+x)^2$ becomes \\(\\sqrt{3x-1}+(1+x)^2.\\)\nTable generator also works here.\n\n\nB.2.3 Adding and moving cells\nYou can add new cells by using the + CODE and + TEXT buttons that show when you hover between cells. These buttons are also in the toolbar above the notebook where they can be used to add a cell below the currently selected cell.\nYou can move a cell by selecting it and clicking Cell Up or Cell Down in the top toolbar.\nConsecutive cells can be selected by “lasso selection” by dragging from outside one cell and through the group. Non-adjacent cells can be selected concurrently by clicking one and then holding down Ctrl while clicking another. Similarly, using Shift instead of Ctrl will select all intermediate cells."
  },
  {
    "objectID": "Colab_tutorial.html#working-with-bash",
    "href": "Colab_tutorial.html#working-with-bash",
    "title": "Appendix B — Introduction to Colab",
    "section": "B.3 Working with Bash",
    "text": "B.3 Working with Bash\n\n!pip install colab-xterm -qq\n%load_ext colabxterm\n\n\n%xterm"
  },
  {
    "objectID": "Colab_tutorial.html#working-with-python",
    "href": "Colab_tutorial.html#working-with-python",
    "title": "Appendix B — Introduction to Colab",
    "section": "B.4 Working with python",
    "text": "B.4 Working with python\nColaboratory is built on top of Jupyter Notebook. Below are some examples of convenience functions provided.\nLong running python processes can be interrupted. Run the following cell and select Runtime -&gt; Interrupt execution (hotkey: Cmd/Ctrl-M I) to stop execution.\n\nimport time\nprint(\"Sleeping\")\ntime.sleep(90) # sleep for a while; interrupt me!\nprint(\"Done Sleeping\")\n\n\nB.4.1 System aliases\nJupyter includes shortcuts for common operations, such as ls:\n\n%ls /bin\n\n! calls out to a shell (in a new process), while % affects the process associated with the notebook\n\n\n!cd sample_data\n\n\n%cd sample_data\n\nThat !ls probably generated a large output. You can select the cell and clear the output by either:\n\nClicking on the clear output button (x) in the toolbar above the cell; or\nRight clicking the left gutter of the output area and selecting “Clear output” from the context menu.\n\nExecute any other process using ! with string interpolation from python variables, and note the result can be assigned to a variable:\n\nmessage = 'Colaboratory is great!'\n\n\n!echo -e '{message}\\n'\n\n\nfoo = !echo -e '{message}\\n'\nfoo\n\n\n!mkdir test\n\n\nOUT_DIR = './test'\n!rm -rf {OUT_DIR}\n\n\n!apt-get -qq install htop\n\n\n\nB.4.2 Magics\nColaboratory shares the notion of magics from Jupyter. There are shorthand annotations that change how a cell’s text is executed. To learn more, see Jupyter’s magics page.\n\n%load_ext autoreload\n%autoreload 2\n\n\n\nB.4.3 Automatic completions and exploring code\nColab provides automatic completions to explore attributes of Python objects, as well as to quickly view documentation strings. As an example, first run the following cell to import the numpy module.\n\nimport numpy as np\n\n\nfrom numpy import arccos\n\nIf you now insert your cursor after np and press Period(.), you will see the list of available completions within the np module.\n\nnp\n\nIf you type an open parenthesis after any function or class in the module, you will see a pop-up of its documentation string:\n\nnp.ndarray()\n\n\nnp.min??\n\n\nhelp(np.min)\n\nWhen hovering over the method name the Open in tab link will open the documentation in a persistent pane. The View source link will navigate to the source code for the method."
  },
  {
    "objectID": "Colab_tutorial.html#integration-with-drive",
    "href": "Colab_tutorial.html#integration-with-drive",
    "title": "Appendix B — Introduction to Colab",
    "section": "B.5 Integration with Drive",
    "text": "B.5 Integration with Drive\nColaboratory is integrated with Google Drive. It allows you to share, comment, and collaborate on the same document with multiple people:\n\nThe SHARE button (top-right of the toolbar) allows you to share the notebook and control permissions set on it.\nFile-&gt;Make a Copy creates a copy of the notebook in Drive.\nFile-&gt;Save saves the File to Drive. File-&gt;Save and checkpoint pins the version so it doesn’t get deleted from the revision history.\nFile-&gt;Revision history shows the notebook’s revision history.\n\n\nB.5.1 Uploading files from your local file system\nfiles.upload returns a dictionary of the files which were uploaded. The dictionary is keyed by the file name and values are the data which were uploaded.\n\nfrom google.colab import files\n\nuploaded = files.upload()\n\nfor fn in uploaded.keys():\n  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n      name=fn, length=len(uploaded[fn])))\n\nFiles are temporarily stored, and will be removed once you end your session.\n\n\nB.5.2 Downloading files to your local file system\nfiles.download will invoke a browser download of the file to your local computer.\n\nfrom google.colab import files\n\nwith open('example.txt', 'w') as f:\n  f.write('some content')\n\nfiles.download('example.txt')\n\n\n\nB.5.3 Mounting Google Drive locally\nThe example below shows how to mount your Google Drive on your runtime using an authorization code, and how to write and read files there. Once executed, you will be able to see the new file (foo.txt) at https://drive.google.com/.\nThis only supports reading, writing, and moving files; to programmatically modify sharing settings or other metadata, use one of the other options below.\nNote: When using the ‘Mount Drive’ button in the file browser, no authentication codes are necessary for notebooks that have only been edited by the current user.\n\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\n\n!ls /content/drive\n\n\nwith open('/content/drive/My Drive/foo.txt', 'w') as f:\n  f.write('Hello Google Drive!')\n!cat /content/drive/My\\ Drive/foo.txt\n\n\n#drive.flush_and_unmount()\n#print('All changes made in this colab session should now be visible in Drive.')\n\n\n!gdown --fuzzy https://drive.google.com/file/d/1KE8dUFWUM389SdDhGj-UJgyGtXpnsqpl/view?usp=sharing\n\n\nfrom nsysu import hello\n\n\nhello()\n\n\nimport sys\nsys.path.append('/content/drive/MyDrive/colab_test/')\nfrom nsysu_math import hello_math\n# file available at https://drive.google.com/file/d/1KAu1yxGmR_oAcCLk4aEltWWM1eMp3FJj/view?usp=sharing\n\n\nhello_math()\n\n\nRemember DO NOT store input data in your drive and load from there. The input/output is very slow (store at ./ instead). Your output data should be stored in your google drive so that it can be accessed next time.\n\n\n\nB.5.4 Loading Public Notebooks Directly from GitHub\nColab can load public github notebooks directly, with no required authorization step.\nFor example, consider the notebook at this address: https://github.com/googlecolab/colabtools/blob/master/notebooks/colab-github-demo.ipynb.\nThe direct colab link to this notebook is: https://colab.research.google.com/github/googlecolab/colabtools/blob/master/notebooks/colab-github-demo.ipynb.\nTo generate such links in one click, you can use the Open in Colab Chrome extension."
  },
  {
    "objectID": "Colab_tutorial.html#run-flask-or-other-web-app",
    "href": "Colab_tutorial.html#run-flask-or-other-web-app",
    "title": "Appendix B — Introduction to Colab",
    "section": "B.6 Run Flask or other web app",
    "text": "B.6 Run Flask or other web app\n\n!pip install flask -qq\n!pip install pyngrok -qq\n\n\nfrom pyngrok import ngrok, conf\nimport getpass\n\n\nprint(\"Enter your authtoken, which can be copied from https://dashboard.ngrok.com/auth\")\nconf.get_default().auth_token = getpass.getpass()\n\n\n# Setup a tunnel to the streamlit port 8050\npublic_url = ngrok.connect(8050)\npublic_url\n\n\nfrom flask import Flask\n\napp = Flask(__name__)\n\n@app.route('/')\ndef hello():\n    return 'Hello NSYSU!'\n\nif __name__ == '__main__':\n    app.run(port=8050)"
  },
  {
    "objectID": "Colab_tutorial.html#use-different-version-of-pythonenvironment-or-lanaguage",
    "href": "Colab_tutorial.html#use-different-version-of-pythonenvironment-or-lanaguage",
    "title": "Appendix B — Introduction to Colab",
    "section": "B.7 Use different version of python/environment or lanaguage",
    "text": "B.7 Use different version of python/environment or lanaguage\nRefer to here and condacolab for more information.\n\nimport sys\nprint(sys.version)\n\nJupyter is named after Julia, Python and R. You can change the kernel to R or Julia."
  },
  {
    "objectID": "Colab_tutorial.html#downloading-data-from-kaggle",
    "href": "Colab_tutorial.html#downloading-data-from-kaggle",
    "title": "Appendix B — Introduction to Colab",
    "section": "B.8 Downloading data from Kaggle",
    "text": "B.8 Downloading data from Kaggle\nThe Dogs vs. Cats dataset that we will use isn’t packaged with Keras. It was made available by Kaggle as part of a computer vision competition in late 2013, back when convnets weren’t mainstream. You can download the original dataset from www.kaggle.com/c/dogs-vs-cats/data.\nBut you can also use Kaggle API. First, you need to create a Kaggle API key and download it to your local machine. Just navigate to the Kaggle website in a web browser, log in, and go to the My Account page. In your account settings, you’ll find an API section. Clicking the Create New API Token button will generate a kaggle.json key file and will download it to your machine.\n\n# Upload the API’s key JSON file to your Colab\n# session by running the following code in a notebook cell:\nfrom google.colab import files\nfiles.upload()\n\nFinally, create a ~/.kaggle folder, and copy the key file to it. As a security best practice, you should also make sure that the file is only readable by the current user, yourself:\n\n!mkdir ~/.kaggle\n!cp kaggle.json ~/.kaggle/\n!chmod 600 ~/.kaggle/kaggle.json\n\n\n# You can now download the data we’re about to use:\n!kaggle competitions download -c dogs-vs-cats\n\nThe first time you try to download the data, you may get a “403 Forbidden” error. That’s because you need to accept the terms associated with the dataset before you download it—you’ll have to go to www.kaggle.com/c/dogs-vs-cats/rules (while logged into your Kaggle account) and click the I Understand and Accept button. You only need to do this once.\nMore information about Kaggle API"
  },
  {
    "objectID": "kaggle-explore.html#setup",
    "href": "kaggle-explore.html#setup",
    "title": "Appendix C — Introduction to Kaggle",
    "section": "C.1 Setup",
    "text": "C.1 Setup\nYou can lookup the resources first:\n\nimport multiprocessing\ncores = multiprocessing.cpu_count() # Count the number of cores in a computer\ncores\n\n\nfrom psutil import virtual_memory\nram_gb = virtual_memory().total / 1e9\nprint('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n\n\nimport sys\n# Is this notebook running on Colab or Kaggle?\nIS_COLAB = \"google.colab\" in sys.modules\nIS_KAGGLE = \"kaggle_secrets\" in sys.modules\n\ngpu_info = !nvidia-smi\ngpu_info = '\\n'.join(gpu_info)\nif gpu_info.find('failed') &gt;= 0:\n  if IS_COLAB:\n    print(\"Go to Runtime &gt; Change runtime and select a GPU hardware accelerator.\")\n  if IS_KAGGLE:\n    print(\"Go to Settings &gt; Accelerator and select GPU.\")\nelse:\n  from tensorflow.python.client import device_lib \n  print(device_lib.list_local_devices())\n\n\n!nvidia-smi -L\n\nV100 &gt; P100 &gt; T4 &gt; K80 (but most of the time you get K80 or T4 using the free Colab)"
  },
  {
    "objectID": "kaggle-explore.html#working-with-bash",
    "href": "kaggle-explore.html#working-with-bash",
    "title": "Appendix C — Introduction to Kaggle",
    "section": "C.2 Working with Bash",
    "text": "C.2 Working with Bash\nKaggle provides simple bash (console) below"
  },
  {
    "objectID": "kaggle-explore.html#working-with-python",
    "href": "kaggle-explore.html#working-with-python",
    "title": "Appendix C — Introduction to Kaggle",
    "section": "C.3 Working with python",
    "text": "C.3 Working with python\nKaggle kernel is built on top of Jupyter Notebook. Below are some examples of convenience functions provided.\n\nC.3.1 System aliases\nJupyter includes shortcuts for common operations, such as ls:\n\n%ls /kaggle/\n\n! calls out to a shell (in a new process), while % affects the process associated with the notebook\nExecute any other process using ! with string interpolation from python variables, and note the result can be assigned to a variable:\n\nmessage = 'Kaggle is great!'\n!echo {message}\n\nKaggle is great!\n\n\n\nfoo = !echo {message}\nfoo\n\n['Kaggle is great!']\n\n\n\n!mkdir test\n\n\nOUT_DIR = './test'\n!rm -rf {OUT_DIR}\n\n\n!apt-get  install htop\n\n\n\nC.3.2 Magics\nKaggle shares the notion of magics from Jupyter. There are shorthand annotations that change how a cell’s text is executed. To learn more, see Jupyter’s magics page.\n\n%load_ext autoreload\n%autoreload 2\n\n\n\nC.3.3 Automatic completions and exploring code\nColab provides automatic completions to explore attributes of Python objects, as well as to quickly view documentation strings. As an example, first run the following cell to import the numpy module.\n\nimport numpy as np\n\n\nfrom numpy import arccos\n\nIf you now insert your cursor after np and press Period(.), you will see the list of available completions within the np module.\n\nnp.\n\nIf you type an open parenthesis after any function or class in the module, you will see a pop-up of its documentation string:\n\n??np.ndarray\n\n\nhelp(np.ndarray)"
  },
  {
    "objectID": "kaggle-explore.html#adding-data-sources",
    "href": "kaggle-explore.html#adding-data-sources",
    "title": "Appendix C — Introduction to Kaggle",
    "section": "C.4 Adding Data Sources",
    "text": "C.4 Adding Data Sources\nOne of the advantages to using Notebooks as your data science workbench is that you can easily add data sources from thousands of publicly available Datasets or even upload your own. You can also use output files from another Notebook as a data source. You can add multiple data sources to your Notebook’s environment, allowing you to join together interesting datasets.\nNavigate to the “Data” pane in a Notebook editor and click the “Add Data” button. This will open a modal that lets you select Datasets to add to your Notebook. The input data will be stored in the /kaggle/input/ directory. The output data will be stored in the /kaggle/working/ directory. You can also use the kaggle datasets download command to download a dataset to your Notebook’s environment. For more information, see the Kaggle Datasets documentation.\nYou will notice that there is a third option in the “Add Data” modal: Notebook Output Files. Up to 20 GBs of output from a Notebook may be saved to disk in /kaggle/working. This data is saved automatically and you can then reuse that data in any future Notebook: just navigate to the “Data” pane in a Notebook editor, click on “Add Data”, click on the “Notebook Output Files” tab, find a Notebook of interest, and then click to add it to your current Notebook. By chaining Notebooks as data sources in this way, it’s possible to build pipelines and generate more and better content than you could in a single notebook alone. If you need additional temporary scratch space, consider saving to /kaggle/tmp/ where the limits are much more generous. Note that the /kaggle/tmp/ directory is not guaranteed to be persisted between Notebook runs. For more information, see the Kaggle Notebooks documentation.\nFinally, you can add your own dataset by uploading your files.\n\n!pip install --upgrade --no-cache-dir gdown -qq\n\n\n!gdown --fuzzy https://drive.google.com/file/d/1KE8dUFWUM389SdDhGj-UJgyGtXpnsqpl/view?usp=sharing\n\n\nfrom nsysu import hello\n\n\nhello()\n\n\n!ls /kaggle/input/\n\n\nC.4.1 Resoruces\nThe Resources of Kaggle GPU: - Kaggle GPU: 16G NVIDIA TESLA P100 - Limited to 30+ hrs/week depending on usage. - Limited to 12hrs/run\n\n\nC.4.2 Save version\nYou can run the code in the background with Kaggle. Firstly, make sure your code is bug-free, as any error in any code block would result in early stopping. Click the “Save Version” button. The concept of “Versions” is a collection consisting of a Notebook version, the output it generates, and the associated metadata about the environment. Two options are available:\n\nQuick Save: Skips the top-to-bottom notebook execution and just takes a snapshot of your notebook exactly as it’s displayed in the editor. This is a great option for taking a bunch of versions while you’re still actively experimenting. You can choose to reserve the output\nSave & Run All: Creates a new session with a completely clean state and runs your notebook from top to bottom. In order to save successfully, the entire notebook must execute within 12 hours (9 hours for TPU notebooks). Save & Run All is identical to the “Commit” behavior."
  }
]